{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resources:\n",
    "\n",
    "- https://medium.com/free-code-camp/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3\n",
    "- https://www.geeksforgeeks.org/text-analysis-in-python-3/\n",
    "- https://stackabuse.com/python-for-nlp-creating-tf-idf-model-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove code blocks and other weird markdown elements\n",
    "all_code = []\n",
    "\n",
    "def get_code(words, start_symbol, end_symbol, pattern):\n",
    "    code_block = ''\n",
    "#     if anti_pattern not in words:\n",
    "    if pattern in words:\n",
    "        # 'find' only finds the first instance of something, gotta find a better solution\n",
    "        code_block_start = words.find(start_symbol, 0, len(words))\n",
    "        code_block_end = words.find(end_symbol, code_block_start+1, len(words))\n",
    "        code_block = words[code_block_start+1:code_block_end]\n",
    "        all_code.append(code_block)\n",
    "    removed = words.replace(code_block, \"\")\n",
    "       # print(code_block)\n",
    "    return code_block, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the stop words\n",
    "\n",
    "otherfile = open(\"stopwords.txt\", \"rt\")\n",
    "\n",
    "stopwords = otherfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open each json file and put into a single dictionary with time_stamp as the key\n",
    "# values are a list containing user id, message type, and actual message\n",
    "\n",
    "file_shared = []\n",
    "just_words = []\n",
    "user_text =[]\n",
    "user_id_dict = {}\n",
    "ts_id_dict = {}\n",
    "\n",
    "for file in glob.iglob('yt Slack export Mar 17 2015 - Jun 23 2020/help/*.json'):\n",
    "    one_file = open(file, 'r')\n",
    "    json_file = json.load(one_file)\n",
    "    \n",
    "    for j in json_file:\n",
    "            \n",
    "        for i in j.keys():\n",
    "            try:\n",
    "                if i == 'ts':\n",
    "                    text = j['text'].lower()\n",
    "                    if j['ts'] not in ts_id_dict:\n",
    "                        ts_id_dict[j['ts']] = [j['type'], j['user'], text]\n",
    "                    if j['files'] != None:\n",
    "                        files = j['files']\n",
    "                        for f in files:\n",
    "                            file_shared.append(f['filetype'])\n",
    "                    ts_id_dict[j['ts']].append(file_shared)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if i == 'text':\n",
    "                one_word = j['text'].split(' ')\n",
    "                just_words += one_word\n",
    "                user_text.append(j['text'])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi ! i’m pretty sure it’s possible to define units equivalences from the user-end, but i can’t find back how it’s done in the cookbook... little help ?\n",
      "i don’t actually think there is a public api to do that, you may need to poke around in the code to figure this out\n",
      "in `powderday` i'm trying to write code that will automagically check what `yt` version we're on, and then deal with the octrees accordingly.   is this reasonable as a check, or is there a better way to discern (in code) if we're on 3.x or 4.x?\n",
      "\n",
      "\n",
      "```if  yt.__version__ == '4.0.dev0':\n",
      "   blah```\n",
      "that’ll break when yt-4.0 comes out\n",
      "maybe just check the first digit of the version string?\n",
      "you could also do what yt does internally for version checks\n",
      "<https://github.com/yt-project/yt/blob/master/setup.py#l311>\n",
      "and just do less than and greater than comparisons\n",
      "i’d check to make sure looseversion does the right thing with the dev0 version number\n",
      "ah. maybe i misremember something i saw as exposed when developing my frontend...\n",
      "hi everyone,\n",
      "i'm using yt to run the rockstar halo finder. is it somehow possible to output the particle data of each halo? according to the rockstar user's guide this is doable but i couldn't figure out how in yt. thanks in advance for your support!\n",
      "hi benedikt, the version of rockstar that yt uses is a bit older and may not support this, although it could be contained within the binary outputs. yt doesn’t currently have the capability to read it in, that much i know for sure.\n",
      "separately, rockstar-galaxies (the latest rockstar i’m aware of) does output particles by default. you have to run it standalone, though.\n",
      "<@u010scnqyj1> has joined the channel\n",
      "<@u010scnqyj1> hi and welcome! i'm glad to see you here!\n",
      "depending on the nature of your simulations, this might be a useful tool written by <@u050ek2tc> that can help you analyze rs output:\n",
      "\n",
      "<https://bitbucket.org/rthompson/pygadgetreader>\n",
      "<@u050ek2tc> has joined the channel\n",
      "<@u042f73r7> thank you very much\n",
      "hey folks! i'm trying to create a particle field but for a derived particle field, but it seems that i can't do that.\n",
      "to be more precise, i have a derived particle field named `gas_tracer` which derives from `io`. i'm trying to create a field called `('gas_tracer', 'mass_of_cell')` but i'd like to avoid requiring `('io', 'mass_of_cell')`\n",
      "i’m confused.  this is a derived particle field but it’s deriving its quantities from a grid-based field (i.e., `mass of cell`)?\n",
      "what does your field definition look like?\n",
      "let me be more precise. for now i have the \"raw\" particle (`io`) and a derived one (`gas_tracer`). and i'd like to do the exact operation as a particle-on-grid deposition (grid-on-particle deposition), so that i could access e.g. the temperature of the cell containing each `gas_tracer`.\n",
      "(use a gist for the code)\n",
      "i’m not sure we support that atm\n",
      "actually, huh, i think there is something that does something similar\n",
      "let me see if i can find the example\n",
      "you want to look at the `meshidentifier` particle deposit operation\n",
      "presumably we could generalize that to allow getting the value of any field instead of just the grid id\n",
      "\n",
      "<https://gist.github.com/cphyc/ea1f8217cfd7b04faa1960fd75d736cd> there you go\n",
      "although i guess looking at it, we’d need to do a substantial refactor, since there’s no way to get field data there\n",
      "i think <@u042hlt7u>  had ideas about this, i have vague memories of a conversation about something similar\n",
      "actually that was with me\n",
      "ah\n",
      "i gave a try at using `meshidentifier` but never succeeded. essentially i was a bit lost in that part of the code and couldn't ahve access to the the field data there.\n",
      "yeah, like i said, we’d need a substantial refactor to get that working\n",
      "i don’t see anything obviously wrong in your code but you’re doing something i’m pretty sure no one has tried before\n",
      "what goes wrong when you try to use this?\n",
      "it works, but let me show you the output\n",
      "```\n",
      "&gt; sp['gas_tracer', 'cell_density']\n",
      "\n",
      "derived field (io, cell_density): (units: mp/cm**3, particle field) npart=11180202 ncell=1089077\n",
      "derived field (io, cell_density): (units: mp/cm**3, particle field) npart=11180202 ncell=1089077\n",
      "derived field (io, cell_density): (units: mp/cm**3, particle field) npart=11180202 ncell=1089077\n",
      "derived field (io, cell_density): (units: mp/cm**3, particle field) npart=11180202 ncell=1089077\n",
      "\n",
      "ytarray([2.61569105e-05, 2.52232177e-05, 2.57807361e-05, ...,\n",
      "         8.77901157e-05, 8.77901157e-05, 7.21347805e-05]) mp/cm**3\n",
      "```\n",
      "ok - what am i supposed to see here?\n",
      "the fact that there are multiple prints happening?\n",
      "so the issue is only performance-wise. i only care about the ~1e5 gas tracer particles but here it's first sampling the cell on all the 1e7 io cells\n",
      "ah, i see :slightly_smiling_face:\n",
      "thanks, that clears things up\n",
      "and then i guess yt internally masks all but the gas tracers\n",
      "yeah\n",
      "`gas_tracer` is implemented as a particle filter in the ramses frontend, right?\n",
      "yup\n",
      "i’m not sure offhand why it’s not giving you data for the particle filter in your derived field definition, i’d need to poke at it with a debugger\n",
      "if you can make a runnable example i might be able to give more concrete advice\n",
      "_working on it_\n",
      "\n",
      "i posted a simple example on the channel\n",
      "note that in the example, it is pointless as all the particles are stars\n",
      "so the relevant code is here:\n",
      "<https://github.com/yt-project/yt/blob/70403c8baeb1ef8704e0f2e08a29ada8a5833aaf/yt/data_objects/data_containers.py#l1298-l1308>\n",
      "in this case `apply_fields[filter_type]` is the `io` field\n",
      "and then we do the filtering after generating the field data\n",
      "i guess in principle we don’t need to do this for derived particle fields\n",
      "so maybe you need to modify this logic to only do this for on-disk particle fields\n",
      "mmh i'm going to drop a debugger there and see what is what\n",
      "<@ucj2mn5kr> has joined the channel\n",
      "ok, i feel like i’m missing something obvious:  how do i get the star formation rate _now_? doing like `sfr = starformationrate(ds, data_source=gal_sphere)` gives me an array vs time, but the most recent `lookback_time` is, e.g., `0.18909443`, not `0.0` ?\n",
      "hey! is it possible to use a particle dataset loaded using `load_particles` with `add_smoothed_particle_field`?\n",
      "i don’t think that works in yt-3.x, it does work in yt-4.0, <@u0860sxlk> has an open pr that makes that workflow a lot more straightforward too\n",
      "<@uj03vtzkn> has joined the channel\n",
      "<@uky0ku3ml> has joined the channel\n",
      "i'm having trouble with installing yt in my home directory of a secure shell?\n",
      "<@uky0ku3ml> how so?\n",
      "hey! `bulk_velocity` needs to be set as a field parameter for a data container. in `yt/fields/vector_operations.py` , the `create_relative_field` function get called on a bunch of vector fields, including velocity.\n",
      "ah ! so if i read the tests correctly, it’s up to the user to set the bulk velocity ?\n",
      "hey i'm trying to plot mass enclosed as a function of radius from different simulations on the same panel. is this possible with yt?\n",
      "yeah, that’s right\n",
      "<@u011y3xm493> it's possible to generate the values, but probably better to grab them from the `.x` and `.y` attributes on multiple profile objects and use matplotlib directly\n",
      "thanks a lot !\n",
      "<@u012n308pch> has joined the channel\n",
      "hi! is anybody familiar with this error message?\n",
      "max_hii_density=sp.max(\"hii_density\")\n",
      " file \"/home/sslav/.conda/envs/yt-env/lib/python3.8/site-packages/yt/data_objects/data_containers.py\", line 789, in max\n",
      "  rv += (self._compute_extrema(f)[1],)\n",
      " file \"/home/sslav/.conda/envs/yt-env/lib/python3.8/site-packages/yt/data_objects/data_containers.py\", line 755, in _compute_extrema\n",
      "  mi, ma = self.quantities.extrema(field)\n",
      " file \"/home/sslav/.conda/envs/yt-env/lib/python3.8/site-packages/yt/data_objects/derived_quantities.py\", line 542, in __call__\n",
      "  rv = super(extrema, self).__call__(fields, non_zero)\n",
      " file \"/home/sslav/.conda/envs/yt-env/lib/python3.8/site-packages/yt/data_objects/derived_quantities.py\", line 69, in __call__\n",
      "  sto.result = self.process_chunk(ds, *args, **kwargs)\n",
      " file \"/home/sslav/.conda/envs/yt-env/lib/python3.8/site-packages/yt/data_objects/derived_quantities.py\", line 549, in process_chunk\n",
      "  field = data._determine_fields(field)[0]\n",
      " file \"/home/sslav/.conda/envs/yt-env/lib/python3.8/site-packages/yt/data_objects/data_containers.py\", line 1165, in _determine_fields\n",
      "  finfo = self.ds._get_field_info(\"unknown\", fname)\n",
      " file \"/home/sslav/.conda/envs/yt-env/lib/python3.8/site-packages/yt/data_objects/static_output.py\", line 798, in _get_field_info\n",
      "  raise ytfieldnotfound((ftype, fname), self)\n",
      "yt.utilities.exceptions.ytfieldnotfound: could not find field '('all', 'hii_density')' in redshift0045.\n",
      "hi shai, this means that the field `hii_density` isn't in your dataset. beware that the field names are case-sensitive. you can inspect the field names by running `ds.field_list` or `ds.derived_field_list` where ds is the dataset object.\n",
      "thank you! this has been very helpful\n",
      "<@u042fh0rb> thanks! you are a life saver!\n",
      "hi, very quick question for a complete yt-beginner here: what should the argument of `yt.sliceplot.set_antialias()` be ? all i found in the documentation is \"aa\", not really helpful (but that seems like a rare exception :))\n",
      "i think it's either set to `true` or `false`\n",
      "thanks !\n",
      "a quick patch fixing that docstring would be a great first pull request if you’re interested, totally cool if you’re not though\n",
      "sure thing, i'm cloning the repo atm\n",
      "still having a little trouble getting my vector fields recognized. i added the fields i want explicitly here(<https://github.com/yt-project/yt/blob/master/yt/fields/fluid_fields.py#l59>), and it seems like that proceeds fine but then they aren't included in the `ds.derived_field_list` later.\n",
      "seems like they're being set as invalid somewhere, but i can't seem to track down where. any ideas?\n",
      "sorry not sure, any chance you can make an example one of us could run to trigger the issue you’re running into?\n",
      "yup will do\n",
      "code: <https://pastebin.com/tmffef1g>, output: <https://pastebin.com/czbb9nwb>, code diff: <https://pastebin.com/frse2a6w>\n",
      "i feel like im just missing something simple, but i don't have intuition right now about how yt sets up and checks the fields\n",
      "takes a look\n",
      "ok, so there’s basically lots of fiddly issues that your script is hitting\n",
      "i’m able to get it to work if i modify your script to look like this: <https://gist.github.com/ngoldbaum/cccb4f84f12ed9d1345ff0c86a6e4170>\n",
      "your fields didn’t have units, that was one of the issues, the field machinery inside yt was also expecting field names like `('gas', '02_p1_velocity_x')`, so i had to change that too\n",
      "i also needed to make a number of modifications to the yt codebase:\n",
      "or really just one change i guess\n",
      "```\n",
      "diff --git a/yt/fields/field_detector.py b/yt/fields/field_detector.py\n",
      "index ad908933e..a3a73857c 100644\n",
      "--- a/yt/fields/field_detector.py\n",
      "+++ b/yt/fields/field_detector.py\n",
      "@@ -204,8 +204,12 @@ class fielddetector(defaultdict):\n",
      "             return self.field_parameters[param]\n",
      "         self.requested_parameters.append(param)\n",
      "         if param in ['center', 'normal'] or param.startswith('bulk'):\n",
      "-            return self.ds.arr(\n",
      "-                np.random.random(3) * 1e-2, self.fp_units[param])\n",
      "+            try:\n",
      "+                return self.ds.arr(\n",
      "+                    np.random.random(3) * 1e-2, self.fp_units[param])\n",
      "+            except keyerror:\n",
      "+                if param.endswith('velocity'):\n",
      "+                    return self.ds.arr(np.random.random(3) * 1e-2, 'cm/s')\n",
      "         elif param in ['surface_height']:\n",
      "             return self.ds.quan(0.0, 'code_length')\n",
      "         elif param in ['axis']:\n",
      "```\n",
      "we recently made it so all the vector fields respect the `bulk_*` field parameters\n",
      "but in so doing made things a bit more restrictive so it’s harder to set up what you’re looking for\n",
      "this should definitely be made easier and nicer, i still think we need something like `ds.define_vector_field(vx, vy, vz)` or something like that\n",
      "that handles all this mess automatically\n",
      "<@u0565nsc0> does that sound reasonable?\n",
      "i think so\n",
      "so in the part that you added, if you wanted to be able to define an arbitrary vector field you'd have to add a section that sets up a corresponding bulk parameter with the correct units?\n",
      "or we’d need to add a way for `create_vector_fields` to optionally not insist on defining a relative version of the field\n",
      "really we need something like a field parameter registry that people can register field parameters with so the field detection system knows about them\n",
      "right now the field detection system has to basically guess about field parameter names and units that people might want to use\n",
      "got it\n",
      "<@u042fh0rb> yes its 2d , i tried changing line 4 to : zgrid = np.array([0.0,0.5])\n",
      "<@udn6acn6n> did that help?\n",
      "hm. i am not sure if i understand. if \"viridis(data).shape\" is a 3d array, then how do i display this data with imshow or contourf or anything else? i have put what i thought i understood in this gist: <https://gist.github.com/anfho/29619d4eacf91504194d3855f290b3b5>. is there a way to attach my original rgb image?\n",
      "you could save it as a png and upload it to e.g. imgur\n",
      "you wouldn’t display an rgb image with contourf\n",
      "you’d use the original greyscale image\n",
      "i think imshow handles rgba arrays out of the box?\n",
      "well yes it shows it, but then i still don't know how to link a colormap to this rgb image. did a drag and drop to imgur. can you see this: <https://imgur.com/a/nm2ezia>\n",
      "sure\n",
      "sorry, i’m not sure what you mean by “link a colormap”\n",
      "sorry, i’m having issues with my python installation so it’s hard for me to reproduce what you’re seeing exactly\n",
      "one sec\n",
      "ah ok\n",
      "so in your gist, `img_ex` is *already* an rgba image array\n",
      "yes, from the start\n",
      "so it doesn’t make sense to apply a colormap to it\n",
      "do you not have the original grayscale image anymore?\n",
      "no, i do not\n",
      "ah i see\n",
      "so you want to back out the grayscale image from the rgba image\n",
      "sort of yes, but the danger is when going through the grayscale, there may be double values that are cancelled out\n",
      "why do you not have the original image anymore?\n",
      "i’m not sure what you’re doing is well-defined…\n",
      "it might be i guess if you know the colormap\n",
      "so i am not sure if there is an uncomplicated an quick way to convert the colors back into real data and then be able to manipulate and display that data\n",
      "yeah, i think ideally you’d want to have saved the original grayscale image\n",
      "i think i remember seeing a talk about doing something like this at scipy years ago\n",
      "let me see if i can find the talk…\n",
      "ok\n",
      "<https://youtu.be/ywhqiev3xxg?t=1938>\n",
      "i don’t know offhand if that code ever made it into a usable package\n",
      "<https://github.com/mrterry/yoink>\n",
      "ah, very cool. let me dig into this.\n",
      "thank you\n",
      "sounds like this will only plug into python2.7? i have already switched to 3.5 though ...\n",
      "yeah, it hasn’t been updated since 2013 it looks like\n",
      "i don’t know offhand if there’s anything like it that’s more recently updated\n",
      "is it possible to have 2.7 and 3.5 running at the same time or is this not advisable?\n",
      "i used to do that\n",
      "recently i switched to using pyenv\n",
      "you can definitely do that, although it might get confusing which python “python” refers to\n",
      "i have two anaconda setups, though the 2.7 one is just so i can use hg for a certain project\n",
      "same\n",
      "how do i install yoink?\n",
      "<http://www.yoinkery.com/> as cited in the setup.py does not exist\n",
      "yeah, i don\n",
      "i don’t think it’s been touched in five years\n",
      "you probably need to clone the repo\n",
      "btw, these aren’t really yt questions, it might be best to take this to e.g. stackoverflow instead of the yt slack\n",
      "<@u5z2bfpqr> has joined the channel\n",
      "<@udn6acn6n> has joined the channel\n",
      "hi, are .vtr files supported by yt? how can i try and use yt with .vtr files ?\n",
      "<@udn6acn6n> what's a `.vtr` file?\n",
      "visualization toolkit rectilinear grid data ?\n",
      "<@u042f73r7> yes\n",
      "fyi: <https://www.vtk.org/wp-content/uploads/2015/04/file-formats.pdf>\n",
      "i don't think yt can parse generic vtk datafiles. we only support specific flavors created by computational codes (e.g. athena)\n",
      "we don’t have support for that format, no\n",
      "this is the second time in a month we’ve had a similar request though, adding support for more “generic” cfd formats would be a great project\n",
      "thats a shame. i don't think its too difficult to do though. there is a vtk library (<https://www.kitware.com/products/books/vtktextbook.pdf>), they have reader functions. then i guess, that data just need to be fed correctly into yt. might try and hack something like this for my purpose\n",
      "an optional dependency on vtk would certainly be the easiest approach\n",
      "especially with the modularization -- it might even make sense to work more closely with vtk functions in some cases\n",
      "i’m trying to make a phase plot of `t_cool / t_freefall` vs `radius` for a sphere object.  it’s easy to do `t_cool` vs `radius` for the sphere object, but it’s tricky to extend this to include the free fall time.  the reason is that freefall time cannot be calculate on the fly for each gas cell, as it is dependent on the `mass_enclosed(r)`.  so i can make a bunch of spheres of different radii to calculate t_ff as a spherically-symmetric profile, but i’m not sure how to feed that back into the yt infrastructure to make the original desired phase plot of `t_cool / t_freefall` vs `radius`.  any ideas on how to do this crazy feat?\n",
      "<@u042j5bn6> use the profile's accumulation argument to get the enclosed mass as a function of radius\n",
      "<@u042j5bn6> if you use a profile you'll get the enclosed spherically symmetric at the same points as a similarly spaced phase\n",
      "<@u042hlt7u> that’s useful. i had forgotten about the accumulation kwarg. thanks! \n",
      "\n",
      "but is there a way to put that enclosed mass profile info back into the data for the phase plot? the issue is that there is a diversity of stuff going on in the halo and i don’t want to flatten it into a single radial profile and lose all the structure seen in a full phase plot vs radius. but maybe that isn’t possible.\n",
      "i guess i could read out the phase plot info out of the plot and manually combine it with the profile data and then out it back into a pcolormesh mpl instance. maybe? hmm. i’ll try and report back.\n",
      "i seem to be having some trouble with ytree/rockstar/consistent trees.\n",
      "\n",
      "i am not sure this is the best place to ask, but:\n",
      "\n",
      "using something like  \"  a = ytree.load(\"rockstar_halos/out_0.list\")  \" gives me nodes, but then \" a[#][\"prog\"] \"  only returns the original entry [ treenode[#] ]\n",
      "\n",
      "generating merger trees with consistent trees and loading them via   \" a = ytree.load(\".../tree_0_0_0.dat\") \" fixes this problem, and allows me to find progenitors/descendants. however, for some reason, this changes the mass, position, etc. of the original rockstar halos, and i can no longer figure out which treenodes correspond to the halos i was originally interested in.  (this is not the case if i take the approach given above.)\n",
      "\n",
      "can anyone help with either issue?\n",
      "is it possible to do volume rendering the data in a 3d numpy array? the data is either continuous or segmented (only 0 or 1) in 3d.\n",
      "<@uhwbq8vpb> has joined the channel\n",
      "hi, just wondering for a ray data, is there a simple to do the integration along the ray, ray.integrate() seems to only work along the cartesian axis.\n",
      "<@ubju11gju> good question. i believe you need to manually integrate rays. you can get the length from the field `dts`\n",
      "hey! is there a way to use yt's plotting machinery at somehow a lower level: i have a bunch of particles that i'd like to plot\n",
      "however, i want to change their position by adding some noise\n",
      "i guess you could reload the particles using load_particles? i don’t think there’s a way to modify particle fields in memory. yt plots need to be created using a dataset or a yt data object, so you’ll need to figure out how to get your data in that format\n",
      "or don’t use yt’s plotting system at all and just use matplotlib\n",
      "sometimes that’s easier than trying to get yt’s plotting system to work in a way that it wasn’t really intended\n",
      "ok!\n",
      "it might be interesting to think of modifying existing fields at some point in the future.  <@uh593l8tu> wanted to do this as well last week with the coordinate fields to affect a global coordinate transform.  could be interesting, but seems like a pretty deep change.\n",
      "in that vein of setting fields to rotate my coordinates, i'm trying to access a field i set within another field i'm defining and it's returning a value that's different from what i set. when i print it outside the function it exists.\n",
      "\n",
      "```\n",
      "ad.set_field_parameter('rotation_vectors', ad.principal_axes_vectors)\n",
      "ad.get_field_parameter(\"rotation_vectors\")\n",
      "array([[-4.64385979e-01,  2.91482979e-01,  8.36291418e-01],\n",
      "       [ 7.08714933e-01, -4.43966940e-01,  5.48285053e-01],\n",
      "       [ 5.31101502e-01,  8.47308108e-01, -4.06318212e-04]])\n",
      "```\n",
      "then within the function i'm defining\n",
      "```\n",
      "def _rotate_coordinates(field, data):\n",
      "    if data.has_field_parameter(\"rotation_vectors\"):\n",
      "        print('reading rotation vectors')\n",
      "        rv = data.get_field_parameter(\"rotation_vectors\")#.in_units(\"cm/s\")\n",
      "    else:\n",
      "        rv = data.ds.arr(np.eye(3))\n",
      "    print('the rotation vectors are: ', rv)\n",
      "    return get_coordinates_rotated(data['coordinates'], rv)\n",
      "```\n",
      "which when i call `ds.derived_field_list` prints\n",
      "```\n",
      "yt : [info     ] 2019-03-26 14:26:14,139 allocating for 2.002e+08 particles (index particle type 'all')\n",
      "yt : [info     ] 2019-03-26 14:26:59,358 identified 1.257e+07 octs\n",
      "reading rotation vectors\n",
      "the rotation vectors are:  0.0\n",
      "```\n",
      "ah, so this is because field parameters need to be scalars\n",
      "unless it’s a limited set that we have specifically enumerated in yt that can be non-scalar\n",
      "basically, during the field detection process, yt needs to have dummy values for field parameters\n",
      "but for field parameters yt doesn’t know about yet, it can’t guess values with the proper shape\n",
      "the real fix for this is to add a field parameter registry to yt\n",
      "but that would take some work and is well beyond what you want\n",
      "another way to fix this is to use a global variable instead of field parameters\n",
      "sorry this is confusing, this is something we’ve seen from other people in the past\n",
      ":see_no_evil: do you know how i could get a precomputed rotation matrix into my function `_rotate_coordinates`?\n",
      "yes, using a global variable\n",
      "ah sorry, just saw that\n",
      "also out of curiosity, why do you want to do this with a field?\n",
      "like, what’s your ultimate goal here?\n",
      "it’s possible that whatever you’re trying to do can be accomplished without rotating the data\n",
      "i'm trying to put the simulation into the same frame as the ananke synthetic gaia survey of a latte simulation which is defined by a pre computed center, center velocity, and rotation matrix, and then an lsr center and velocity\n",
      "ok, but what ultimately do you want to do that would be sensitive to how the dataset is rotated?\n",
      "oh and then reconstruct the metallicity weighted gas density on a grid of positions around that position, at the same positions i'm inferring what my model thinks the metallicity weighted gas density is\n",
      "i think you could do that with an off-axis slice?\n",
      "i want to feed a grid of positions into my model and the latte simulation and compare my prediction with the truth\n",
      "what's an off-axis slice?\n",
      "i guess we don’t have an api for non-axis-aligned covering grids, that would be worth adding at some point\n",
      "you can create yt slices at arbitrary angles to the coordinate axes\n",
      "i have the properly rotated particles in a python array, can i somehow add that to a yt dataset?\n",
      "yt.load_particles\n",
      "it has to fit in memory though\n",
      "oh, it fits in memory !\n",
      "be sure to pass a smoothing_length field to load_particles so it gets recognized as sph data\n",
      "awesome, will try this route :slightly_smiling_face:\n",
      "also i don’t think it would be terribly hard to optionally let people rotate particle data, e.g. by passing a rotation matrix to yt.load or something, i think for particle data this is a reasonable feature request\n",
      "does it make sense to consider a rotation matrix in the `particle_position_relative` fields? so it's translation + rotation\n",
      "ah yeah, that’s a good point, they already respect normal_vector\n",
      "so guess that’s what <@uh593l8tu> wants up to a rotation\n",
      "<@u0860sxlk> i think the name we standardized on was `relative_particle_position`\n",
      "ah, yeah, i was thinking of that one but forgot the name\n",
      "this is probably not too difficult to implement\n",
      "that field knows about `center` and `normal`, so it’s already implemented\n",
      "i guess it could also respect `north_vector`?\n",
      "ah, that's even easier, just need to wire them all up\n",
      "i think this would be a really good idea, and could probably be implemented without touching too many bits in the code.\n",
      "<@u9ce5d9lz> or <@u042fh0rb> what type of kernel does `ds.arbitrary_grid` use for generating the field at each position on the grid? i'm trying to understand how to convert the smoothing lengths output from fire simulations to make the calculation accurate.\n",
      "the default is a cubic spline kernel, but it’s configurable\n",
      "<https://github.com/yt-project/yt/blob/yt-4.0/yt/geometry/particle_deposit.pxd#l42>\n",
      "although i dunno offhand if the ability to use alternate kernels is exposed at the moment in the high-level interface\n",
      "<@uh593l8tu> if it works, `'quintic'` spline kernel should be used for gizmo. there's also a subtlety that when doing spatial interpolation, gizmo's normalization is different from usual sph. computationally it is similar to the gather method. since gather is already implemented by <@u9ce5d9lz>, it might be worth adding this \"gizmo\" method too.\n",
      "ah i was hoping to use the `scatter` method because it's faster, but you're suggesting i should stick to `gather` for normalization issues?\n",
      "how big is your dataset? i mean gather is certainly slower but it isn't **too** slow\n",
      "for visualization purpose, `scatter` should be fine. if you'd like to keep as close as possible to what gizmo code would do, there's a need to slightly modify `gather`. it's not in yt yet, but it won't be too difficult to implement. the heavy-lifting is already done in the `gather` code.\n",
      "what is the subtlety? is this instead of taking the kth neighbour?\n",
      "here's the equation. normal gather assumes omega(x) = 1.\n",
      "and a visual comparison.\n",
      "<https://arxiv.org/pdf/1409.7395.pdf>\n",
      "ah i see! i didn't realise the smoothing length calculation was different\n",
      "i use traditional gather when analysing arepo or sph data - i never worry too much about it. maybe i should worry more\n",
      "the addition of gather is already a great improvement. probably the gather method could be modified to optionally switch to the gizmo normalization?\n",
      "yeah, i guess it's something which we can just switch on if we detect the ds is gizmo\n",
      "<@uh593l8tu> if you have time and would like to contribute to yt, this might be worth taking a further look.\n",
      "no worries if you just want to use the code. we'd happy to implement these if you need the functionality.\n",
      "issue created: <https://github.com/yt-project/yt/issues/2221>\n",
      "how would one compute the normalization? is it simply the smooth number density of particles within the kernel? that's what i gathered (no pun intended) from the paper.\n",
      "thanks for the details <@u0860sxlk> i might punt this for the future work, <@u8nue3lbu> suggests i just use a cubic sph kernel for my application\n",
      "<@uggk0erpa> haha, yeah, i think so.\n",
      "<@uh593l8tu> no problem :slightly_smiling_face: for anyone interested to follow up, i've created an issue: <https://github.com/yt-project/yt/issues/2222>\n",
      "<@u9ce5d9lz> when i do the `ds.arbitrary_grid' example in the documentation\n",
      "```\n",
      "arbitrary_grid = ds.arbitrary_grid([0.0, 0.0, 0.0], [0.99, 0.99, 0.99], dims=[128, 128, 128])\n",
      "ag_density = arbitrary_grid[('gas', 'particle_density')]\n",
      "ag_density.shape\n",
      "```\n",
      "it prints `(135,)` which doesn't seem right, shouldn't the shape be 128^3?\n",
      "also, while i have your attention, what are the units of `left_edge` and `right_edge`? i'd like to make a grid on a 1 kpc box around the center of the simulation\n",
      "<@uc6l85lbb> has joined the channel\n",
      "hello yt community! i having some difficulties creating a parallelized volume_render in yt. essentially, the problem occurs at the end of the `for` loop when the stored objects attempt to be recombined so all the processors have the same stored data. i am simply launching the code `mpirun -np 4 python pele_make_movie_strip.py` from the shell. running in serial is no problem and works fine. the python script and data are attached below. any help would be greatly appreciated, thanks!!\n",
      "stripped version of code\n",
      "\n",
      "and the error i am getting is ```file \"../../../pelecode/pele_make_movie_strip.py\", line 30, in &lt;module&gt;\n",
      "    for fr, (sto, ds) in enumerate(ts.piter(storage=my_scenes)):\n",
      "  file \"/usr/local/lib/python2.7/site-packages/yt/data_objects/time_series.py\", line 283, in piter\n",
      "    storage=storage, dynamic=dynamic):\n",
      "  file \"/usr/local/lib/python2.7/site-packages/yt/utilities/parallel_tools/parallel_analysis_interface.py\", line 520, in parallel_objects\n",
      "    to_share, datatype = 'dict', op = 'join')\n",
      "  file \"/usr/local/lib/python2.7/site-packages/yt/utilities/parallel_tools/parallel_analysis_interface.py\", line 284, in passage\n",
      "    return func(self, *args, **kwargs)\n",
      "  file \"/usr/local/lib/python2.7/site-packages/yt/utilities/parallel_tools/parallel_analysis_interface.py\", line 725, in par_combine_object\n",
      "    self.comm.send(data, dest=0, tag=0)\n",
      "  file \"mpi4py/mpi/comm.pyx\", line 1156, in mpi4py.mpi.comm.send\n",
      "  file \"mpi4py/mpi/msgpickle.pxi\", line 174, in mpi4py.mpi.pympi_send\n",
      "  file \"mpi4py/mpi/msgpickle.pxi\", line 104, in mpi4py.mpi.pickle.dump\n",
      "  file \"mpi4py/mpi/msgpickle.pxi\", line 91, in mpi4py.mpi.pickle.cdumps\n",
      "  file \"/usr/local/lib/python2.7/site-packages/yt/data_objects/data_containers.py\", line 1040, in __reduce__\n",
      "    [getattr(self, n) for n in self._con_args] +```\n",
      "can you create an issue on github please?\n",
      "that way we won’t lose track :)\n",
      "sure thing!\n",
      "i’ll try to take a look at this tomorrow\n",
      "thanks! i just reported the problem on github.\n",
      "<@u042f73r7> has joined the channel\n",
      "<@u042fh0rb> has joined the channel\n",
      "i want to visualize amr data, produced by amrex. i don't like amrex's standard format because it creates too many files to be efficient. can you recommend another generic amr file format that yt supports? for example, i can also produce silo data.\n",
      "yt doesn’t support silo data\n",
      "does it support some other, similar format that's efficient?\n",
      "i'm looking for block-structured amr with vertex/cell/face/edge-centred grids.\n",
      "i don’t know offhand\n",
      "<http://yt-project.org/docs/dev/examining/index.html|http://yt-project.org/docs/dev/examining/index.html> this page lists all the data formats yt supports\n",
      "have you tested working with the normal amrex format with yt? or is the inefficiency on the application side not on the analysis side?\n",
      "it's inefficient when writing, because there are so many files. i have runs with &gt;10 million output files. my file system quota stopped the run.\n",
      "it's not a yt problem at all.\n",
      "fair enough\n",
      "fwiw writing a silo frontend has come up before\n",
      "i don’t think there’s anything blocking it besides an interested person to write the code\n",
      "it's much easier to write a write. hence my question – is there a different format that yt understands?\n",
      "i think the answer is no, particularly if you need support for all those different kinds of centering schemes, but someone might be able to correct me\n",
      "at the moment i am using `snapshot_028_z000p000`  from <https://yt-project.org/data/>\n",
      "i agree the results *should* be closer. i really think changing to eq. 9 as is common in other codes doing similar things will be a step in the right direction. i plan to implement this tomorrow, locally, and assess the differences\n",
      "alright, i look forward to seeing if that improves things :slightly_smiling_face:\n",
      "hi there! i just issued a pull request (<https://github.com/yt-project/yt/pull/2604|#2604>) which is a one line change from a collaborator’s fork. is it preferable to do this from a branch within the yt repo?\n",
      "nope!!! you did the right thing!\n",
      "thank you for the pr! if you’re interested in contributing more check out the development channel too. :slightly_smiling_face:\n",
      "i'm having some difficulty extracting unit data from a dataset\n",
      "it seems to be stored in a dict, but i can't seem to get it out\n",
      "anyone know how to pull what kind of unit code_length is from ds without having to use ds.unit_systems.base_units?\n",
      "and pulling them out by hand\n",
      "like, is there a way to just get(code_length) and have it return \"cm\"\n",
      "<@ulqt4pxbp> has joined the channel\n",
      "i cannot change the background color in a slice plot. i have an exodus file to visualize where the geometry is in an x-y plane in 3d. so i just created a slice plot at z = 0.\n",
      "```\n",
      "ds = yt.load('out.e')\n",
      "slc = yt.sliceplot(ds, 'z', 'c_o2', origin='native',  width=(20, 'nm'))\n",
      "slc.set_background_color('c_o2', color='black') \n",
      "slc.save('black_background')\n",
      "```\n",
      "however, the background color is still the minimum value color of the colorbar (rouge in my case). how could i change the background correctly?\n",
      "are you certain that you're looking at the background and not an area that is contained within the dataset?\n",
      "i think so. the geometry shown in paraview is in the first attached picture, and what i got with the background setting is the second one.\n",
      "that is weird\n",
      "have you tried plotting it using matplotlib or some such?\n",
      "i haven't done that with matplotlib. i am not sure if i can do that because i have an unstructured mesh.\n",
      "<@ulqt4pxbp> could you check if it works if you set the first argument in `set_background_color` to `'all'` ? my hunch is that it expects a tuple, cause otherwise it sets it for a wrong field\n",
      "<@u042f73r7> i got `ytfieldnotfound: could not find field '('io', 'all')' in out.e.`\n",
      "interesting, looks like our docs are out of date\n",
      "<@ulqt4pxbp> i'm not sure what the proper first component should be\n",
      "if you could do:\n",
      "```\n",
      "ds = yt.load('out.e')\n",
      "print(ds.field_list)\n",
      "```\n",
      "and see if there's ('something, 'c_o2') field in there\n",
      "try to use that in set_background_color\n",
      "<@u042f73r7> ok\n",
      "actually, while we're both online at the same time, is there any chance i could convince you to send me the data your working with? i'm sorely lacking in unstructured mesh datasets\n",
      "just for testing sake\n",
      "<@ulqt4pxbp> i just checked that i can reproduce that with a different dataset, so it looks like a bug\n",
      "<@ujyb2mz5x> there's a plenty of examples here: <https://yt-project.org/data>\n",
      "ah, perfect, thanks\n",
      "<@u042f73r7> you wouldn't happen to know off the top of your head how to get unit data would you?\n",
      "<@ujyb2mz5x> unit data?\n",
      "<@u042f73r7> just fyi, i tried `slc.set_background_color(('all', 'c_o2'), color='black')` and `slc.set_background_color(('connected1, 'c_o2'), color='black')`, but they didn't work. i have two blocks in my mesh.\n",
      "if you load up a dataset, and fetch a field the array it returns usually contains the units\n",
      "but in the form \"code_length\" or something like that\n",
      "is there a way to fetch the unit it's actually using for \"code_length\"\n",
      "meaning \"cm\" or \"ft\" or whatever\n",
      "likewise, the same questions exists for things like density, temperature and angle\n",
      "<@ujyb2mz5x> i don't know if there's a helper for that, but you can do something like that:\n",
      "```\n",
      "ds = yt.load(...)\n",
      "length = ds.quan(1, 'code_length')\n",
      "length.in_cgs()\n",
      "```\n",
      "it will print out 1 code_length in cms\n",
      "or `length.in_units('ft')`\n",
      "that's not really what i need thought. it seems that the unit and the field are matched together in a dictionary type object\n",
      "[temp: \"k\", angle: \"rads\"] style\n",
      "and i can't seem to actually pull from the dictionary\n",
      "not sure i understand what you want to achieve\n",
      "let me take a screenshot, that may help me explain it better, i'm awful at explaining stuff\n",
      "see how every field (length, time, mass ect) has a unit that goes with it? and this information seems to be stored in the \"ds\" object?\n",
      "do you know a way to say \"get me length\" and have it return \"cm\"\n",
      "yt seems to be able to automatically find the appropriate units when plotting, but i'm not having such luck\n",
      "wait\n",
      "i think i got it\n",
      "ds.unit_system['length'] ?\n",
      "nevermind\n",
      "yeah, that does it, earlier i was looking at the dictionary internally, which seems harder to pull from\n",
      "ds.unit_system['length'] seems to work\n",
      "thanks for you help\n",
      "i'm not sure that's the thing you're looking for\n",
      "i'd need to check it on a dataset that doesn't use cgs\n",
      "<@ulqt4pxbp> could you file an issue at <https://github.com/yt-project/yt/issues/new> related to the background problem you're seeing?\n",
      "my assumption is that the units would change if you use a different unit system\n",
      "<@u042f73r7> i will\n",
      "thank you!\n",
      "<@ujyb2mz5x> yup, you're right. i'm not really familiar with our unit system so i had to double check\n",
      "tanfastic, that makes everything easier\n",
      "<https://github.com/yt-project/yt/blob/master/yt/units/tests/test_unit_systems.py> shows how it works\n",
      "<@u042f73r7> i posted a new issues here: <https://github.com/yt-project/yt/issues/2305>\n",
      "i am wondering if the function is collectedly implemented, the background will be in that color all over the place, or just outside the smallest rectangle which encloses the geometry.\n",
      "quick followup to this, <@u042j7xjp> - apparently this line:\n",
      "\n",
      "```\n",
      "sp.set_field_parameter(\"bulk_velocity\", [vx, vy, vz])\n",
      "```\n",
      "\n",
      "causes problems downstream (in, e.g., projections) unless you feed in a ytarray instead of a list for [vx,vy,vz]\n",
      "i had to add a couple lines to make everything work correctly:\n",
      "\n",
      "```\n",
      "from yt import ytarray\n",
      "velocities = ytarray([vx,vy,vz])\n",
      "sp.set_field_parameter(\"bulk_velocity\", velocities)\n",
      "```\n",
      "ah, good point <@u1dlem6kw> \n",
      "thanks\n",
      "<@utk5ejyfj> has joined the channel\n",
      "<@uc6l85lbb> it works for me if i explicitly load as an amrexdataset:\n",
      "\n",
      "got it! that is much easier than i expected. thank you!\n",
      "<@u043bna00> i guess we need a fancier _is_valid somewhere?\n",
      "<@u042fh0rb> yeah\n",
      "<@u0hhgt2v9> has joined the channel\n",
      "<@ubju11gju> i've updated my pr, this should do the work now\n",
      "hi everyone.  i’m installing  yt on a new system using the `install_script.sh`, and it just failed for a numpy versioning thing:\n",
      "\n",
      "```\n",
      "installing the miniconda python environment.\n",
      "installing the necessary packages for yt.\n",
      "this may take a while, but don't worry.  yt loves you.\n",
      "installing python\n",
      "installing setuptools\n",
      "installing numpy\n",
      "installing jupyter\n",
      "installing ipython\n",
      "installing sphinx\n",
      "installing git\n",
      "installing gitpython\n",
      "installing h5py\n",
      "installing matplotlib\n",
      "installing cython\n",
      "installing nose\n",
      "installing scipy\n",
      "installing astropy\n",
      "installing conda-build\n",
      "installing sympy\n",
      "installing netcdf4\n",
      "building yt from source\n",
      "setting yt_dir=/home/chummels/src/yt-conda/src/yt-git\n",
      "********************************************\n",
      "        failure report:\n",
      "********************************************\n",
      "\n",
      "    self._add_defaults_ext()\n",
      "  file \"/home/chummels/src/yt-conda/lib/python3.6/site-packages/setuptools/command/py36compat.py\", line 119, in _add_defaults_ext\n",
      "    build_ext = self.get_finalized_command('build_ext')\n",
      "  file \"/home/chummels/src/yt-conda/lib/python3.6/distutils/cmd.py\", line 299, in get_finalized_command\n",
      "    cmd_obj.ensure_finalized()\n",
      "  file \"/home/chummels/src/yt-conda/lib/python3.6/distutils/cmd.py\", line 107, in ensure_finalized\n",
      "    self.finalize_options()\n",
      "  file \"setup.py\", line 317, in finalize_options\n",
      "    if looseversion(numpy.__version__) &lt; looseversion('1.10.4'):\n",
      "attributeerror: module 'numpy' has no attribute '__version__'\n",
      "```\n",
      "interesting\n",
      "yeah, i’ve never seen this.\n",
      "someone else hit the exact same thing today\n",
      "maybe it’s an issue with conda-forge\n",
      "or wait\n",
      "did numpy remove that or something?\n",
      "possibly.\n",
      "tries running the install script\n",
      "so you set inst_yt_source=1?\n",
      "make any other modifications?\n",
      "yup.\n",
      "and astropy and scipy=1\n",
      "that’s it\n",
      "weird.  \n",
      "```\n",
      "&gt;&gt;&gt; import numpy\n",
      "&gt;&gt;&gt; print(numpy.__version__)\n",
      "1.13.3\n",
      "```\n",
      "are you running this on a supercomputer with a module system by any chance?\n",
      "yes.\n",
      "caltech’s local cluster.\n",
      "what’s `module list`?\n",
      "i module loaded a python module.\n",
      "ah, can you try running with that unloaded?\n",
      "but i figured we’d override that.\n",
      "i don’t think we can….\n",
      "with this local conda install\n",
      "module systems do funky things with injecting stuff into various environment variables\n",
      "sure, i can unload that python install\n",
      "and retry.\n",
      "there might be a way to do that but i’m not sure what it is\n",
      "one sec.  trying.\n",
      "thanks for the suggestion.\n",
      "hmm..  same error even using the default python install on the system, py27.\n",
      "not using the system-wide conda python install.\n",
      "sorry, what did you do?\n",
      "error:  \n",
      "```\n",
      "awesome!  here we go.\n",
      "\n",
      "using curl\n",
      "\n",
      "downloading <http://repo.continuum.io/miniconda/miniconda3-latest-linux-x86_64.sh>\n",
      "\n",
      "installing the miniconda python environment.\n",
      "installing the necessary packages for yt.\n",
      "this may take a while, but don't worry.  yt loves you.\n",
      "installing python\n",
      "installing setuptools\n",
      "installing numpy\n",
      "installing jupyter\n",
      "installing ipython\n",
      "installing sphinx\n",
      "installing git\n",
      "installing gitpython\n",
      "installing h5py\n",
      "installing matplotlib\n",
      "installing cython\n",
      "installing nose\n",
      "installing scipy\n",
      "installing astropy\n",
      "installing conda-build\n",
      "installing sympy\n",
      "installing netcdf4\n",
      "building yt from source\n",
      "setting yt_dir=/home/chummels/src/yt-conda/src/yt-git\n",
      "********************************************\n",
      "        failure report:\n",
      "********************************************\n",
      "\n",
      "    self._add_defaults_ext()\n",
      "  file \"/home/chummels/src/yt-conda/lib/python3.6/site-packages/setuptools/command/py36compat.py\", line 119, in _add_defaults_ext\n",
      "    build_ext = self.get_finalized_command('build_ext')\n",
      "  file \"/home/chummels/src/yt-conda/lib/python3.6/distutils/cmd.py\", line 299, in get_finalized_command\n",
      "    cmd_obj.ensure_finalized()\n",
      "  file \"/home/chummels/src/yt-conda/lib/python3.6/distutils/cmd.py\", line 107, in ensure_finalized\n",
      "    self.finalize_options()\n",
      "  file \"setup.py\", line 317, in finalize_options\n",
      "    if looseversion(numpy.__version__) &lt; looseversion('1.10.4'):\n",
      "attributeerror: module 'numpy' has no attribute '__version__'\n",
      "\n",
      "********************************************\n",
      "```\n",
      "so you used the install script with the default python?\n",
      "what?\n",
      "what's your `numpy.__path__`?\n",
      "so, i unloaded the python module (`module unload python/anaconda2-4.1.1`)\n",
      "and then my python is just the default python installation on the system, which is 2.7\n",
      "and then reran the installer\n",
      "and got the error above.\n",
      "can i see the output of `module list`?\n",
      "```\n",
      "&gt;&gt;&gt; import numpy\n",
      "&gt;&gt;&gt; print(numpy.__path__)\n",
      "['/usr/lib64/python2.7/site-packages/numpy']\n",
      "```\n",
      "```\n",
      "[chummels@wheeler ~/src]$ module list\n",
      "currently loaded modulefiles:\n",
      "  1) hdf5/1.8.17     2) gcc/5.3.0       3) gsl/2.1         4) openmpi/2.0.1\n",
      "```\n",
      "i’m running the install script over here, let’s see if i trigger this\n",
      "ok cool.\n",
      "just on my mac\n",
      "yeah, fair enough.\n",
      "i haven’t installed it locally for a while, so i wasn’t sure if this was a new global bug, or if it was just some problems on this machine.\n",
      "someone else reported this exact issue today\n",
      "it's possible that they modified the numpy version\n",
      "i haven’t seen it before today\n",
      "but `numpy.__version__` definitely exists on the latest release of numpy\n",
      "where was the other person finding this error?  what sort of system?  cluster?\n",
      "it was a cluster yeah\n",
      "look at <#c046hvb59|particles>\n",
      "ok\n",
      "can you show me the output of `env`?\n",
      "sure!\n",
      "\n",
      "these are all just defaults.  i haven’t messed with anything yet.\n",
      "i don’t see anything particularly suspicious\n",
      "seems to have not crashed on my machine\n",
      "yeah, seemed reasonable to me.\n",
      "did it get to actually installing yt from source?\n",
      "yes, it’s building yt\n",
      "my error occurred as soon as it started on that step.\n",
      "so i guess it must be a bug specific to clusters or something.\n",
      "weirdness.\n",
      "you could try doing what the install script does, but just manually\n",
      "e.g. install miniconda3 in your home directory\n",
      "edit your path\n",
      "and install yt’s dependencies into that new environment, then build yt\n",
      "if it dies that’s something you can take to your sysadmin\n",
      "i can give that a shot.\n",
      "i’ll try and report back.\n",
      "thanks for the suggestions!\n",
      "can you link me to this script? i've never heard of it before\n",
      "<https://raw.githubusercontent.com/yt-project/yt/master/doc/install_script.sh>\n",
      "```\n",
      "$ wget <https://raw.githubusercontent.com/yt-project/yt/master/doc/install_script.sh>\n",
      "```\n",
      "<http://yt-project.org/docs/dev/installing.html#all-in-one-installation-script>\n",
      "<@u91855pa9> it’s an artifact of a bygone era when it was way harder to get a python env setup\n",
      "it’s still listed as the first option in the installation directions in the docs.\n",
      "we keep it around because even installing anaconda manually is too much for complete beginners\n",
      "oh\n",
      "yeah, install script ran fine on my mac\n",
      "ok, thanks for the update.\n",
      "i think i’ve got it working.\n",
      "installing manually seems to be fine.\n",
      "i don’t know what was going wrong with the installer script, though.\n",
      "thanks for the help, everyone!\n",
      "perhaps i should update the trident/yt-4.0 installer instructions to just forego using the install_script.\n",
      "up to you\n",
      "if you want to spend some time debugging why it’s breaking that would be helpful\n",
      "unfortunately i don’t have a way to reproduce so can’t help much...\n",
      "it would be nice to figure out what `numpy.__file__` is when it fails\n",
      "if i want to run the command-line version of yt (e.g., `yt notebook` or `yt upload_image`, where is the python script that i can put in my path to do this?\n",
      "since the manual install of yt doesn’t include the command-line functionality.\n",
      "yes it does\n",
      "i guess by just `git clone yt` and then `pip install -e .`, it doesn’t for me.\n",
      "it’ll be in the bin folder associated with your miniconda install\n",
      "hmm.\n",
      "got it.  my mistake.  path weirdness here.\n",
      "(those scripts get installed by yt’s setup.py, so they’ll always be there no matter how you invoke it)\n",
      ":+1:\n",
      "ok, that’s great.\n",
      "<@u042j5bn6> could you try running `find /home/chummels/src/yt-conda/ -type f -name \"numpy.py\"` and see if anything pops up ?\n",
      "on that cluster where you saw the failure\n",
      "ok, one sec.\n",
      "well, i wiped out the yt-conda directory after the failed install with the install_script, and then reinstalled conda from scratch in that directory.\n",
      "when i run that command now, i see no such files.\n",
      "i’m currently trying to pull an `frb` out of the image plane of a `phaseplot` in the same way i do for `plotwindow` objects, but it seems like this isn’t an option.  is there another way to pull the image data out of the `phaseplot` to dump to a matplotlib `pcolormesh()` call or something similar?\n",
      "<@u042j5bn6> it's not an frb, but you can access it via a dict on the `.profile` attribute of the `phaseplot`\n",
      "gotcha--just found that.\n",
      "thanks!\n",
      "<@uhvu9ca3s> has joined the channel\n",
      "<@uddtt4wsz> has joined the channel\n",
      "<@ulqt4pxbp> i've issued <https://github.com/yt-project/yt/pull/2318> which i think addresses one of the issues you mentioned\n",
      "<@uhkuhfybf> has joined the channel\n",
      "is there any way to easily make cartesian x/y/z slices from a spherical dataset? i’m seeing stuff about make slices along the spherical axes (<http://yt-project.org/docs/dev/examining/spherical_data.html>), but haven’t found anything about being able to slice along axes besides how the dataset is saved\n",
      "no, that’s not how the support for spherical data works right now\n",
      "<@u042hlt7u> would be the person to talk with about this, he has ideas related to making this easier\n",
      "and has a better idea of how things work right now\n",
      "cool, thanks!\n",
      "<@ulbk1pvsr> some of us are away at scipy but will try to reply to you when we are back\n",
      "@matt i found that using the tuple (\"boxlib\", \"phi\") as field name is working...\n",
      "hi i have a problem about adding a derived field. if i just use yt.add_field((“enzo”,“ure”), function=_unit_rpsi,force_override=true)\n",
      "the field will not show up in ds.derived_field_list\n",
      "after i call ad[(‘enzo’,‘ure’)], it is finally added to the ds.derived_field_list\n",
      "is there any way that the field will be added immediately after i do yt_add_field?\n",
      "<@ubju11gju> my guess is that you need to add that field before index is created for enzo dataset, e.g. before you do yt.load(\"your_datasets\"), or use `add_field` on dataset directly: `ds.add_field(...)`\n",
      "also it's not a bad thing it's not initially there, it should be created when it's gonna be required\n",
      "<@u042f73r7> thanks, it’s great! i just copy the code from the documentation.\n",
      "which part?\n",
      "<@u042f73r7> just change yt.add_field to ds.add_field\n",
      "checkout the following code:\n",
      "```import yt\n",
      "from yt.units import dimensions\n",
      "\n",
      "def _pressure(field, data):\n",
      "    return (data.ds.gamma - 1.0) * \\\n",
      "          data[\"density\"] * data[\"thermal_energy\"]\n",
      "\n",
      "yt.add_field((\"gas\",\"pressure1\"), function=_pressure, units=\"auto\",\n",
      "             dimensions=dimensions.pressure)\n",
      "\n",
      "ds = yt.load(\"isolatedgalaxy/galaxy0030/galaxy0030\")\n",
      "print((\"gas\", \"pressure1\") in ds.derived_field_list)\n",
      "\n",
      "yt.add_field((\"gas\",\"pressure2\"), function=_pressure, units=\"auto\",\n",
      "             dimensions=dimensions.pressure)\n",
      "print((\"gas\", \"pressure2\") in ds.derived_field_list)\n",
      "ds.add_field((\"gas\",\"pressure3\"), function=_pressure, units=\"auto\",\n",
      "             dimensions=dimensions.pressure)\n",
      "print((\"gas\", \"pressure3\") in ds.derived_field_list)```\n",
      "if you run it (you may need to change `yt.load` to your dataset), you'll notice that `pressure2` won't be on the list, but `pressure1` and `pressure3` will\n",
      "how can i find the maximum of a field?\n",
      "ah, i see it's output by default when the field is read. thanks!\n",
      "<@ulddr26el> has joined the channel\n",
      "is there an equivalent of `yt.off_axis_projection` for particle plots? i want access to the bitmap that's produced so i can fiddle with it\n",
      "\n",
      "found how to do the particular thing i needed but it would be nice to know if the image data is accessible\n",
      "if i load up a time series via `yt.dataseries`, is there an easy way to plot time-average fields from that?\n",
      "or maybe there is another way to do this i am not thinking of?\n",
      "i'm trying to select particles in a box with a side length of 150 kpc, so i think my left and right edges are 75 kpc from the center. but when i run this code, i see a bunch of distance components that are outside [-75, 75]. is this a bug or am i misunderstanding the `ds.box` selector?\n",
      "```\n",
      "ds = yt.load(datadir + 'snapshot_172.0.hdf5')\n",
      "region = ds.box(center - ds.quan(75.0, 'kpc'),\n",
      "                center + ds.quan(75.0, 'kpc'))\n",
      "output = region['parttype0', 'coordinates'].to('kpc')\n",
      "distance_components = output - center\n",
      "print(np.sort(distance_components.flatten()))\n",
      "```\n",
      "is this an isolated simulation? were the particles cut out? did you check that the origin is not at 32.5, 32.5, 32.5 instead of 0, 0, 0?\n",
      "the way selection works for sph data, particles will be selected if their \"sphere of influence\" (e.g. the sphere centered on the particle with a radius given by a constant factor times the smoothing length [the factor is 2 for gadget flavored sph data iirc]) intersects with the selection box\n",
      "i bet the particles that are formally outside the box have smoothing regions that overlap\n",
      "the idea was that if you wanted to reconstruct an sph field inside a selection region you'd need not only the particles inside the box but also the particles whose sphere of influence overlap with the region\n",
      "at least with scatter-style smoothing, it doesn't really make as much sense to do it that way for gather style smoothing\n",
      "ah that makes a lot of sense, thanks.\n",
      "<@utjkvdb98> has joined the channel\n",
      "hi! i've been playing around using yt project to import simulations from openfoam. is it possible to construct datasetseries in memory?\n",
      "the examples i've found all seem to construct it from a glob of filenames.\n",
      "you can create a `datasetseries` from a list of datasets iirc\n",
      "i will look for this functionality. thank you.\n",
      "<@ugs4tdhqf> has joined the channel\n",
      "can yt export to .obj files?\n",
      "yup, let me find the relevant place in the docs\n",
      "<http://yt-project.org/doc/visualizing/sketchfab.html>\n",
      "in particular surface.export_obj()\n",
      "<http://yt-project.org/doc/reference/api/yt.data_objects.construction_data_containers.html#yt.data_objects.construction_data_containers.ytsurface.export_obj>\n",
      "<@u042fh0rb> can you export volumetric rendering to .obj?\n",
      "<@ugs4tdhqf> not really, no -- we only support mesh obj files.  i didn't realize obj had support for volumetric fields.\n",
      "so the meshes can be extracted into isocontours and put into obj\n",
      "<@u042hlt7u> i don't know if they do :wink:\n",
      "okay cool\n",
      "or rather, volumetric data -&gt; isocontours -&gt; obj\n",
      "trying to create a fake dataset with\n",
      "```from yt.testing import fake_particle_ds\n",
      "my_data = fake_particle_ds()\n",
      "my_data.all_data()```\n",
      "\n",
      "but i’m getting a `typeerror: super(type, obj): obj must be an instance or subtype of type` on the last line-- am i doing something wrong here?\n",
      "wait… could be an autoreload error…\n",
      "yep\n",
      "nevermind\n",
      "reloading? :scream:\n",
      "hi all. i’m trying to extract a fixed resolution 2d array from an off-axis projection plot and save it to disk. the following script works for the grid-aligned projection:\n",
      "```plt = yt.projectionplot( ds, 'z', my_field )\n",
      "plt.data_source.save_as_dataset()```\n",
      "but it doesn’t work for the *off-axis* projection:\n",
      "```plt = yt.offaxisprojectionplot( ds, normal=[+1,+1,+1], my_field )\n",
      "plt.data_source.save_as_dataset()\n",
      "\n",
      "attributeerror                            traceback (most recent call last)\n",
      "&lt;ipython-input-36-c002fa3b88aa&gt; in &lt;module&gt;()\n",
      "----&gt; 1 plt.data_source.save_as_dataset()\n",
      "\n",
      "attributeerror: 'offaxisprojectiondummydatasource' object has no attribute 'save_as_dataset'```\n",
      "should i use the image returned by the `off_axis_projection` method? i also notice there is an `offaxisprojectionfixedresolutionbuffer`  function. any suggestion is appreciated!\n",
      "yeah, you should just save the image buffer\n",
      "`offaxisprojectionfixedresolutionbuffer` is just a wrapper around `off_axis_projection` so it doesn't really matter which one you use\n",
      "it does make sense to save the projection data object as a dataset since there's an intermediate state for the data - a 2d projected version of the amr structure of your data\n",
      "for off-axis projections we use the volume rendering infrastructure in yt to generate the image so there's no intermediate state to save, we go directly to the image buffer from the amr data\n",
      "<@u0hhgt2v9> hope that makes sense\n",
      "if all you want is an image in the end for both cases then it may make sense to just always save an image, the you won't notice this difference\n",
      "<@u042fh0rb> thanks for the elaboration! i only need a fixed resolution array so storing an image buffer should be sufficient :)\n",
      "i'm working through this yt gadget tutorial: <http://yt-project.org/doc/cookbook/gadget_notebook.html>. in cell 3, yt.projectionplot is called to plot the 'density' of 'gas'. is it possible to do an analogous thing for the dark matter particles only? i was hoping something like 'parttype1' would work but i can't figure it out.\n",
      "(‘deposit’, ‘parttype1_density’)\n",
      "it’s not quite the same, that’s nn interpolation, not sph smoothing\n",
      "there’s also (‘deposit’, ‘parttype1_cic’) for cic interpolation.\n",
      "see <http://yt-project.org/doc/analyzing/fields.html#deposited-particle-fields>\n",
      "hope that helps!\n",
      "you may also want to try yt.particleprojectionplot for the dark matter particles\n",
      "<http://yt-project.org/doc/visualizing/plots.html#particle-plots>\n",
      "<@u042fh0rb> <@u0860sxlk> thanks, i got it working with projectionplot, which is sufficient for my needs right now!\n",
      "<@u011zhnewj1> has joined the channel\n",
      "<@u011y3xm493> morning! i don't know how to do it better than what i pasted above\n",
      "thank u so much!!\n",
      "<@u042hlt7u>\n",
      "would u have a look please?\n",
      "<@u011y3xm493> i don't know what to suggest.  can you fill out a concise bug report, where i can iterate with you?  that might be more productive right now.\n",
      "yes +1 to what matt said. since this might be a bigger problem having a bug report would help make sure your issue doesn’t get lost!\n",
      "<@uklbr3yq7> has joined the channel\n",
      "<@uc6l85lbb> it's ok to spam this channel, people can mute it\n",
      "just try not to spam general :slightly_smiling_face:\n",
      "sounds good!\n",
      "(i think i asked this question here before but it is not in the archive anymore)\n",
      "how do i “rotate” a plot or “flip” the axis? if  i do `yt.sliceplot(ds,fields='density',origin='native', normal=[0,1,0])` the z-axis is on the x-axis and the x-axis on the y-axis of the plot. i would like to flip those (i.e., have the z-axis on the y-axis). any simple way of doing this?\n",
      "i tried to play around  with `north_vector` and `ds.coordinates.x_axis` but no success, unfortunately [also the search feature  of the website seems to be broken at the moment…? ]\n",
      "<@u8fuk8kcl> see <https://gist.github.com/anonymous/2937442f997bb16f5505fa470edb2480>\n",
      "hey, regarding vadlamani question on the mailing list\n",
      "it happens that their ramses output is invalid (the output is broken)\n",
      "however, old version of yt used to be able to read them but the new one can't\n",
      "yet the old version was reading invalid files while the new one fails on them\n",
      "hey  all - when  downloading the `arepo` tnghalo from:\n",
      "\n",
      "<http://yt-project.org/data/>\n",
      "\n",
      "it reports that  the `ds.dataset_type` is gadget -- is this a common feature for all `arepo` simulations?\n",
      "\n",
      "```\n",
      "in [5]: ds = yt.load('halo_59.hdf5')\n",
      "yt : [info     ] 2019-05-03 15:08:42,038 calculating time from 1.000e+00 to be 4.356e+17 seconds\n",
      "yt : [info     ] 2019-05-03 15:08:42,039 assuming length units are in kpc/h (comoving)\n",
      "yt : [info     ] 2019-05-03 15:08:42,059 parameters: current_time              = 4.355810528213309e+17 s\n",
      "yt : [info     ] 2019-05-03 15:08:42,059 parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [info     ] 2019-05-03 15:08:42,060 parameters: domain_left_edge          = [0. 0. 0.]\n",
      "yt : [info     ] 2019-05-03 15:08:42,060 parameters: domain_right_edge         = [205000. 205000. 205000.]\n",
      "yt : [info     ] 2019-05-03 15:08:42,061 parameters: cosmological_simulation   = 1\n",
      "yt : [info     ] 2019-05-03 15:08:42,061 parameters: current_redshift          = 0.0\n",
      "yt : [info     ] 2019-05-03 15:08:42,061 parameters: omega_lambda              = 0.6911\n",
      "yt : [info     ] 2019-05-03 15:08:42,061 parameters: omega_matter              = 0.3089\n",
      "yt : [info     ] 2019-05-03 15:08:42,061 parameters: hubble_constant           = 0.6774\n",
      "\n",
      "in [6]: ds.dataset_type\n",
      "out[6]: 'gadget_hdf5'\n",
      "```\n",
      "are you on the yt-4.0 branch and have you pulled recently?\n",
      "if yes then <@u042j7xjp> is the guy to talk to\n",
      "hmm i haven't - let me do so and report back!\n",
      "aha <@u042fh0rb> an update does it:\n",
      "\n",
      "```\n",
      "in [4]: ds.dataset_type\n",
      "out[4]: 'arepo_hdf5'\n",
      "```\n",
      "that pr was merged very recently, let us know if anything seems odd\n",
      "will do -\n",
      "hi <@u046k2qnk>--yes, let me know if something odd happens\n",
      "<@u042j7xjp> am i  right that the `arepo` front end is  only in yt-4.0?\n",
      "(i'm making a final push this summer to get `powderday` ready for release (cc <@u042hlt7u>) and eventually am going to transition it fully to yt4.0 - whether this happens sooner or later probably will depend on the answer to the above question :)\n",
      "<@u046k2qnk> that’s right, only 4.0\n",
      "okay  thanks - that's very helpful!\n",
      "what’s a good way to quickly visualise a gradient field for a 2d dataset ?\n",
      "i tried the obvious\n",
      "```ds.add_gradient_fields((\"gas\", \"density\"))\n",
      "yt.plot_2d(ds, \"density_gradient_x\")```\n",
      "but it raises\n",
      "```runtimeerror: error: yt attempted to read outside the boundaries of a non-periodic domain along dimension 0.```\n",
      "as it should, but then i do not understand how i can select a smaller region to perform the plot.\n",
      "\n",
      "i think what _should_ work is something like this\n",
      "```grad_defined_region = ds.r[1:9, 1:9, 1:9]  # here i assume the full domain extends from 0 to 10 (code_units) in all directions\n",
      "ds.add_gradient_fields((\"gas\", \"density\"))\n",
      "yt.plot_2d(ds, \"density_gradient_x\", data_source=grad_defined_region)```\n",
      "however, the slicing mechanism in `dataset.r` seems somewhat impossible to satisfy with my 2d dataset. the example above raises\n",
      "```ytdimensionalityerror: dimensionality specified was 3 but we need 2```\n",
      "meanwhile, if i attempt to perform a “2d” region selection, i get\n",
      "```grad_defined_region = ds.r[1:9, 1:9]\n",
      "&gt;&gt;&gt; ytdimensionalityerror: dimensionality specified was 2 but we need 3```\n",
      "i’m hesitant to report this as an issue since i’m not certain i’m doing things correctly here.\n",
      "`ds.r[1:9, 1:9,:]` works? i remember someone had a question similar to this one in the chat before… i think you have to overrite the boundary condition in the 3rd dimension to be periodic (is it not already?)\n",
      "within the last couple days, i’ve been getting memoryerror exceptions running things on blue waters that i would have thought were just fine\n",
      "someone just contacted me about getting similar errors using ytree on that machine as well\n",
      "perhaps run with tracemalloc?\n",
      "or unless you’re saying it’s an issue with blue waters itself?\n",
      "<@u042s6y2g> if we can track it down to just the broad outline of which section of code it happens in that might help. (we have a memory profiler, i think.)\n",
      "<@urn1ykckz> has joined the channel\n",
      "submitted: <https://github.com/yt-project/yt/issues/2465>\n",
      "i see :slightly_smiling_face:\n",
      "a little annoying that cantera doesn't publish packages on pypi\n",
      "so i suspect if you just call that function directly instead of using `frompyfunc` it'll work, yt's unit system doesn't know about `frompyfunc` at all\n",
      "or cast to ndarray before calling a `frompyfunc` ufunc\n",
      "i'm installing cantera so i can confirm\n",
      "i've tried that but `gas.tpx` doesn't accept the yt data. i get:\n",
      "```  file \"interfaces/cython/cantera/thermo.pyx\", line 1000, in cantera._cantera.thermophase.tpx.__set__\n",
      "typeerror: only size-1 arrays can be converted to python scalars```\n",
      "ok, still building cantera\n",
      "i'll try some things once i can actually run the test script :slightly_smiling_face:\n",
      "thanks!\n",
      "i guess supporting `frompyfunc` in general would be tricky for any unit system, you don't know what kind of math is happening inside of the function and thus what kind of unit transformations need to happen\n",
      "could just convert the arguments to ndarray and print a warning\n",
      "<@ujeeuv7lh> what are the units of `gas.viscosity`?\n",
      "hmm, not sure how to fix `attributeerror: module 'cantera' has no attribute 'solution'`\n",
      "maybe i didn't build cantera in a way that gives me that?\n",
      "anyway, it looks like casting to ndarray at least gets you past the error you're seeing\n",
      "```def _nu(field, data):\n",
      "    vfunc = np.frompyfunc(set_tpx, 2, 1)\n",
      "    dyn_visc = vfunc(data[\"temp\"].d, data[\"pressure\"].d)\n",
      "    dyn_visc = yt.ytarray(dyn_visc, \"pa*s\")\n",
      "    return dyn_visc / data[\"density\"]```\n",
      "the `.d` attribute returns an ndarray view, it's equivalent to doing `myarray.view(numpy.ndarray)`\n",
      "i'm gonna open an separate issue about supporting vectorize and frompyfunc\n",
      "`gas.viscosity` is dynamic viscosity, so should be mass/(length*time)\n",
      "your workaround (with `.d`) seems to be working! thanks\n",
      "<@uunapurps> has joined the channel\n",
      "hi there. i’m a grad student just starting with yt. this might be a simple question (and i’m not sure if this is the right place to ask this, but here we are): when going through different fields with artio data and seeing what units are associated with them, i’m noticing that pressure does not have units associated with it. is there maybe some unit conversion thing happening in there or am i missing something more fundamental?\n",
      "\n",
      "additional details: i’m using yt 3.6.dev0. to access the units, i use:\n",
      "```import yt as yt\n",
      "\n",
      "ds = yt.load(\"~/sizmbhloz-clref04snth-rs9_a0.9011/sizmbhloz-clref04snth-rs9_a0.9011.art\")\n",
      "\n",
      "ds.index\n",
      "\n",
      "ds.field_info['gas','temperature']\n",
      "ds.field_info['gas','density']\n",
      "ds.field_info['gas','pressure']```\n",
      "temperature and density are included to show that i do get correct units displaying for these.\n",
      "\n",
      "if i make pressure using my own derived field, i can use my own units, but i just want to see if i’m missing something or if this is a bug.\n",
      "this would need to be fixed by patching yt\n",
      "fields that are known to yt get metadata attached to them\n",
      "e.g. all these fields:\n",
      "<https://github.com/yt-project/yt/blob/master/yt/frontends/artio/fields.py#l34>\n",
      "in `known_other_fields`\n",
      "it looks like `hvar_pressure` is in there though\n",
      "oh wait, but the units are blank aren't they?\n",
      "so yeah, you'd need to replace the empty string in the tuple here:\n",
      "\n",
      "`(\"hvar_pressure\", (\"\", [\"pressure\"], none)), # unused`\n",
      "with whatever the units should be\n",
      "i'm not sure what that `# unused` comment is about\n",
      "okay cool. so follow up question, should these units be included automatically since it does whenever i make my own derived field for pressure? or perhaps there’s some internal unit conversion with constants that i’m not aware of?\n",
      "\n",
      "thanks for your speedy response btw\n",
      "so if you make a derived field from density and temperature, the units will happen automatically because of yt's unit system\n",
      "e.g. the units will come out of the arithmetic\n",
      "but if the pressure is getting read from disk\n",
      "then yt uses that `known_other_fields` tuple to infer what the units should be\n",
      "right now that tuple is saying that `hvar_pressure` is dimensionless, so that's why you're getting those units back\n",
      "does that make sense?\n",
      "yes absolutely. thank you! :slightly_smiling_face:\n",
      "if you'd like assistance with making a pull request to fix this i'm happy to help out\n",
      "it sounds like you're already running from a git clone of the repository so you're like 90% of the way there :slightly_smiling_face:\n",
      "i would love some help with that if you have the time. i don’t want to encroach if you have other things to get done\n",
      "i would start with the instructions here: <https://yt-project.org/docs/dev/developing/developing.html#making-and-sharing-changes>\n",
      "alright cool. thanks again. i appreciate it\n",
      "<@u042fh0rb> (or anyone else who happens to know), i'm trying to create a `sliceplot` of the new viscosity field and it's giving the following error stack:\n",
      "```sx = yt.sliceplot(ds,'z','nu', origin=\"native\", center=[23.24, 6.35001, 3.8100001], width=((33.28,'cm'),(12.7,'cm')))\n",
      "...\n",
      "  file \"/p/home/whitmans/yt-conda/src/yt-git/yt/geometry/coordinates/cartesian_coordinates.py\", line 240, in _ortho_pixelize\n",
      "    period, int(periodic))\n",
      "  file \"yt/utilities/lib/pixelization_routines.pyx\", line 64, in yt.utilities.lib.pixelization_routines.pixelize_cartesian```\n",
      "any idea what could be causing this? do i need to enforce `np.float64_t[:]` in some way?\n",
      "probably that cython code shouldn’t be using float64 explicitly i guess\n",
      "i don’t see the full stack trace\n",
      "what does your field definition look like? and what is the dtype of the nu field, like if you get the data from a data object\n",
      "it’s getting to be late on friday so i may not be able to help today, opening an issue for things like stack traces from normal usage is totally ok\n",
      "i have the following definitions:\n",
      "```def get_mu(temp,pres):\n",
      "        gas = ct.solution(\"air.cti\")\n",
      "        gas.tpx = temp, pres, 'o2:1.0, n2:3.76'\n",
      "        return gas.viscosity\n",
      "\n",
      "def _nu(field,data):\n",
      "        vfunc = np.frompyfunc(get_mu, 2, 1)\n",
      "        dyn_visc = vfunc(data[\"temp\"].d, data[\"pressure\"].d)\n",
      "        dyn_visc = yt.ytarray(dyn_visc, \"g/(cm*s)\")\n",
      "        return dyn_visc / data[\"density\"]```\n",
      "and am adding the field with\n",
      "```ds.add_field((\"gas\",\"nu\"), function=_nu, units=ds.unit_system[\"length\"]**2/ds.unit_system[\"time\"], sampling_type=\"cell\")```\n",
      "and then having issues with sliceplot:\n",
      "```sx = yt.sliceplot(ds,'z','nu', origin=\"native\", center=[23.24, 6.35001, 3.8100001], width=((33.28,'cm'),(12.7,'cm')))```\n",
      "i can open an issue later tonight with the full stack if need be\n",
      "it may help to replace this line:\n",
      "\n",
      "```dyn_visc = yt.ytarray(dyn_visc, \"g/(cm*s)\")```\n",
      "with:\n",
      "\n",
      "```dyn_visc = yt.ytarray(dyn_visc.astype(np.float64), \"g/(cm*s)\")```\n",
      "\n",
      "just a guess though\n",
      "yeah it's working once i enforce the float64 type. thanks!\n",
      "<@u042j5bn6> thanks for answering!!! :grinning:\n",
      "using a projection plot on a changa dataset:\n",
      "\n",
      "```prj = yt.projectionplot(ds,'z',('gas','density'),center='m',width=(50,'kpc'))```\n",
      "is the center in this example calculated based on the gas center of mass? baryon center of mass? or total (including dm) center of mass?\n",
      "(i'm trying to calculate the coordinates that this projection plot is centered on)\n",
      "[using yt4.x]\n",
      "<@u046k2qnk> my guess is the xyz of the particle with highest value for \"density\" whether computed or on disk\n",
      "thanks!\n",
      "hi <@udju40mfy>, i’m at a conference in japan for the week, so i may be a little slow at responding, but i have some more ideas that may help. i’ve been thinking for some time about a way to speed up the `select_halos` function and what i’d like to do is implement a ytree frontend for `yt`. this would allow one to access all nodes at once and do the type of things you want. it only just occurred to me that this is exactly what you’re looking for. anyway, i think i can do this sometime this week.\n",
      "\n",
      "in the mean time, this might be a little bit faster than using the `select_halos` function:\n",
      "```\n",
      "    tsize = np.array([t['tree'].size for t in a])\n",
      "    all_nodes = np.empty(tsize.sum(), dtype=object)\n",
      "    offset = 0\n",
      "    for i, ts in enumerate(tsize):\n",
      "        all_nodes[offset:offset+ts] = a[i]['tree']\n",
      "        offset += ts\n",
      "```\n",
      "<@u7ku54sg5> no, i didn't.  can we set up a call to talk through this?\n",
      "hi, is there a way to plot the negative of a quantity in `sliceplot`<https://yt-project.org/doc/visualizing/plots.html#slice-plots>\n",
      "<@ufagql7u0> you'll have to make a derived field like so:\n",
      "\n",
      "```python\n",
      "@yt.derived_field(name=\"negative_of_whatever\")\n",
      "def neg_field(field, data):\n",
      "    return -data[\"whatever\"]\n",
      "```\n",
      "there are some other options you can specify to get a pretty-printed name and stuff\n",
      "thanks, and how do i specify that in the sliceplot command?\n",
      "<@ufagql7u0> whatever you name the field is the new field you should plot.  note that if you're using a script, you should do this field definition *before* you load the dataset, and if not, you have to call `ds.add_field`\n",
      "that worked, thanks. i used `ds.add_field`\n",
      "hooray!\n",
      "sure, if you’re available. i’m pretty much free all day tomorrow (time zone is us pst)\n",
      "<@u042hlt7u> \n",
      "hmm, i thought `annotate_line` worked with phase plots, but maybe not\n",
      "@matt i was thinking of adding annotate_mean_profile or something like that\n",
      "ah, neat\n",
      "two `annotate_particles` questions: i’m looking at <http://yt-project.org/doc/visualizing/callbacks.html> and trying to only draw the star particles within a sphere. `p.annotate_particles(stride=10,width=(0.5, 'mpc')` works, but `p.annotate_particles(stride=10,width=(0.5, 'mpc'), data_source=sph)` gives `typeerror: __init__() got an unexpected keyword argument 'data_source'`. likewise, i’d like to only mark the stars, which is `particle_type` 2, but setting `ptype=2` gives `ytplotcallbackerror: annotate_particles callback failed with the following error: cannot identify field (2, 'particle_position_y')` so maybe  i’m passing that in wrong?\n",
      "for the second question, these are two slightly different uses of the term particle_type. in the enzo frontend, `particle_type` is a field associated with each particle, whereas in particle-based datasets the particle_type is equivalent to the field_type (e.g., ‘gas’). to do what you want, you’ll have to create a particle filter, like this:\n",
      "<https://yt-project.org/docs/dev/analyzing/filtering.html#filtering-particle-fields>\n",
      "then, you would be able to do `ptype=\"stars\"` for example.\n",
      "for the first question, it doesn’t look like there is a particularly simple way to do what you want, but i think you could do it by making a particle filter in the manner discussed above. or, it might be possible to implement a `data_source` keyword.\n",
      "hmm, ok, partially i was trying to test if my “stars” filter was working :upside_down_face:\n",
      "(update: it doesn’t crash with the `\"stars\"` option, but it also doesn’t plot any points, which i think answers my question. thanks!)\n",
      "i'm having problems with certain `tipsy` datasets but not all of them. after initializing the coarse index it breaks. i've uploaded two datasets at <http://use.yt/upload/99e20269>, one works (g2.63e10) and one doesn't (g8.26e11) and i can't see why, they look substantially the same. i'm using yt-4.0 from source and this is an sph code.\n",
      "my script is:\n",
      "```\n",
      "import yt\n",
      "dsname = \"~/testnihaoinput/g2.63e10/g2.63e10.00320\"\n",
      "ds = yt.load(dsname)\n",
      "print ds.field_list\n",
      "```\n",
      "the error i'm getting is in `frontends/tipsy/io.py` here:\n",
      "```\n",
      "...\n",
      "/users/claytonstrawn/yt/yt/frontends/tipsy/io.pyc in _yield_coordinates(self, data_file, needed_ptype)\n",
      "    310                 mas = np.empty(3, dtype=\"float64\")\n",
      "    311                 for axi, ax in enumerate('xyz'):\n",
      "--&gt; 312                     mi = pp[\"coordinates\"][ax].min()\n",
      "    313                     ma = pp[\"coordinates\"][ax].max()\n",
      "    314                     mylog.debug(\n",
      "...\n",
      "valueerror: zero-size array to reduction operation minimum which has no identity\n",
      "```\n",
      "can others reproduce this?\n",
      "edit: doing this again on a different simulation runs into the same problem...i think it breaks when the redshift gets too low? z&gt;2 is ok, z&lt;=2 doesn't work\n",
      "hi! i’m generating a angular moment vector from a sphere. i’m expecting the result to be an unitless array, like (0, 0, 1), showing the direction of the angular momentum but the code actually gives me some huge numbers with units of cm**2/s, which seem to be the unit of r*m*v with m being a unit mass (not even sure if this interpretation is correct). how should i get the vector array that shows the direction of the angular momentum instead? thanks!\n",
      "here is what i did.\n",
      "yes, specific angular momentum has units of cm^2/s\n",
      "you could divide l_vector by its norm\n",
      "norm would be np.sqrt(l[0]**2 + l[1]**2 + l[2]**2)?\n",
      "yup\n",
      "sounds good, thanks!\n",
      "another question… i used yt about 4 years ago, at that time there was an allsky projection function, called yt.visualization.volume_rendering.camera.allsky_projection, but it is not there anymore. i wonder are there other similar functions in the new yt?\n",
      "so that had to be removed when we relicensed from gpl to bsd, since it used healpix which is a gpl library\n",
      "since then the astropy community created a bsd-licensed healpix implementation that we could use\n",
      "someone would need to port the old code to use the new healpix library\n",
      "<https://github.com/astropy/astropy-healpix>\n",
      "i see, so that isn’t any module existing for this at the moment.\n",
      "nope, unfortunately no\n",
      "that was the one piece of functionality we had to stub out, unfortunately\n",
      "we’d like to bring it back, the last i heard about it <@u0860sxlk> wanted to do that\n",
      "yeah, too bad there was a license issue. it was such a nice function.\n",
      "thanks a lot for the info! i’ll shop around to see what else i can use.\n",
      "is the function still in the old code? and where i can find the old version of yt?\n",
      "so there’s this wip pull request: <https://github.com/yt-project/yt/pull/1667>\n",
      "the old interface was removed, but the old version of the code is still in the repo, on the yt-2.x branch\n",
      "<https://github.com/yt-project/yt/blob/yt-2.x/yt/visualization/volume_rendering/camera.py#l2008>\n",
      ":+1: thanks!\n",
      "<@ujn82he20> has joined the channel\n",
      "does anybody know how i can load gridded _cosmological_ data? `yt.load_uniform_grid` does not allow for the parameters `cosmological` or `cosmological_parameters`.\n",
      "so far i tried manually setting e.g. ` ds.cosmological_simulation = 1.` and the other cosmological params. but then the cosmological units (e.g., `mpccm`) do not get registered…\n",
      "hey <@u8fuk8kcl>, i’ve done stuff like this for running halo finding on generic particle data. long story short, you have to set a lot of things manually, but here is a script that i put together that you can probably alter for your purposes.\n",
      "<http://paste.yt-project.org/show/119/>\n",
      "maybe we can pr some of this functionality at some point.\n",
      "great, thanks! i’ll check it out.\n",
      "worked like a charm! thanks :slightly_smiling_face:\n",
      "excellent!\n",
      "<@ujeeuv7lh> hi sam, to address the deeper issue, do you think that i could get you to store to an hdf5 file the contents of all the `ds.index.grid_*` arrays?  this is the full amr hierarchy, and would help me replicate it here to see if the issue is something i can re-create *without* the full dataset.\n",
      "i created a new issue for this problem:\n",
      "<https://github.com/yt-project/yt/issues/2311>\n",
      "in a slice plot, is it possible to align the numbers of the color bar? in the picture, the number in the bottom has a minus sign, and the digits are shifted to the right. it would be great if i can add a white space or plus sign to the positive numbers to align the digits.\n",
      "yeah, good call, that will be a bit faster.  i think i can speed up my code even further with a technique like this.  it’ll still much slower than i think it could be though — ultimately, i’m spending a lot of time just accessing the properties of each node, so if there properties of all the nodes were exposed in a way like i’m describing, most of my script would be reduced to re-indexing those arrays.  i’ll give this a shot and let you know what i find, and let me know if you want to chat more about the type of interface i’m hoping for.\n",
      "is it possible to create a derived field of an averaged field along one axis only? something like `np.average(data[\"density\"], axis=1)`. i can do this by extracting fixed resolution data, but i can not see how to do this more easily by defining a derived field. thanks!\n",
      "sure, you’ll need to use a `validatespatial(1)` validator\n",
      "and make your field definition do a finite difference along the axis you want to average along\n",
      "`validatespatial` ensures that your field function is passed a “spatial” (3d) data chunk\n",
      "passing 1 to it tells yt to generate 1 layer of ghost zones for the chunk before calling the field function\n",
      "<https://github.com/yt-project/yt/blob/master/yt/fields/vector_operations.py#l405>\n",
      "thank you! i’ll check it out!\n",
      "<@uejkuj2db> has joined the channel\n",
      "i think the yt frontend will help quite a bit since you’ll be able to access the merger tree data like a conventional particle dataset. hopefully, i can get that done soon.\n",
      "<@uejh8gf2n> has joined the channel\n",
      "hi <@ucybq5kpa>, i was able to do this altering the `mass_defiinition` in `config.template.h` as you had tried\n",
      "i noticed that when i did `make clean` in the rockstar directory that it did not actually delete `librockstar.so`, so you might want to that\n",
      "ah, we should probably patch the makefile to do that\n",
      "yeah\n",
      "hi all. i am trying to plot a figure using a .dat file created by amrvac. i use spherical coordinates and want plot a slice at a certain distance r from the origin. i would like this plot to be (theta, phi) or (phi, theta) but plotted as if it where a plane, not a sphere. what i tried to far (a slice at r), produces a full sky 2d image (like is usual in astronomy- see attached). however i would like to create an image similar to the second attachment. i can’t seem to figure out how to do this?\n",
      "hi, i co-authored the amrvac frontend ! as of now, there is no yt-wise solution for this but the good news is that there is a workaround for amrvac data. you can load your dataset as if it was written in cartesian coordinates using `yt.load(\"your file.dat\", geometry_override=\"cartesian\")`, then perform an « x-slice» which should correspond to the r axis.\n",
      "this method should produce the plot you are trying to make, however it will require a little more work on your part because you’ll need to manually update the axes labels.\n",
      "hope this helps. please let me know if you need additional info <@usj3smag2>  :)\n",
      "perfect! exactly what i need.\n",
      "happy to help !\n",
      "fyi , i think i tested this with different non cartesian coordinate systems, though only used it with polar datasets. let me know if you’re having any issues :)\n",
      "<@uc5mp3mj8> has joined the channel\n",
      "not sure where to put this, so i figured this is a good place to start. i fixed several problems with the art frontend, and put them on hte yt-dev mailer, and was wondering if i should put/talk to anyone here about them?\n",
      "here is fine, there’s also <#cbe6579cz|development>\n",
      "did you see i replied to you?\n",
      "on yt-dev\n",
      "i did not\n",
      "<https://mail.python.org/mm3/archives/list/yt-dev@python.org/message/2rwxastft2w2657kmka4fkwsvrqzaxsn/>\n",
      "not sure why you didn’t get that\n",
      "also not sure why you’re double-mailing\n",
      "yeah just saw it and made a request\n",
      ":+1:\n",
      "looks non-controversial to me but let’s see what the tests say\n",
      "i pinged kenza over e-mail\n",
      "thanks for the contribution :slightly_smiling_face:\n",
      "no problem, i am also looking into trying to fix a few other things that may or may not be broken, will let kenza know\n",
      "yup, i haven’t heard from her in a few years so she may or may not engage\n",
      "good to know\n",
      "hi all, quick question here: is it possible to show 3d streamlines (velocity or magnetic field) when making a volume rendering? and while i’m at it, what about annotations of the physical scale on the domain box as well?\n",
      "if you mean “invert the x and y axes in cartesian geometry”, there’s a trick for that\n",
      "otherwise, i don’t know but i would be very interested in the answer as well\n",
      "here’s the trick for the narrow case i know how to deal with :\n",
      "```# flipping the axes\n",
      "ds.coordinates.x_axis[2] = 1\n",
      "ds.coordinates.x_axis['z'] = 1\n",
      "ds.coordinates.y_axis[2] = 0\n",
      "ds.coordinates.y_axis['z'] = 0\n",
      "\n",
      "# plotting\n",
      "yt.sliceplot(ds, \"density\", \"z\")```\n",
      "no to the former at the moment, yes to the latter, see the volume rendering docs\n",
      "oh wait, just the domain box for the latter\n",
      "oh yes i have the domain box, but i was wondering if i could label the axes as well\n",
      "there’s save_annotated but that doesn’t include the physical scale\n",
      "could be extended to probably\n",
      "alright thanks, maybe i’ll see if i can extend it at some point and submit a pr for that\n",
      "<@uvb4rhzng> has joined the channel\n",
      "hey! is there the equivalent of `relative_particle_position` but for the grid?\n",
      "looks like no\n",
      "`('index', 'x')` currently doesn’t have any logic to deal with field parameters\n",
      "ok\n",
      "and how do you create a vector field in yt?\n",
      "i think right now they need to be set up in a hard-coded manner inside the yt field system\n",
      "i might be wrong about that\n",
      "might be easier to just work with the components of the vector\n",
      "yeah, the sticking point is field detection, as soon as we make that smarter it gets easier to handle this, but right now if you add a new vector field you’d need to add a line here so that field detection generates data with the correct shape:\n",
      "<https://github.com/yt-project/yt/blob/master/yt/fields/field_detector.py#l126>\n",
      "also it looks like it has to be a particle field, so vector mesh fields will likely require more work to get working besides adding it to that line\n",
      "oh wait <@u37dtbl6n> i’m totally wrong, there is a relative_x field, excuse me\n",
      "is there?\n",
      "i see it referenced inside the yt source code, let me make an example…\n",
      "or i’m misreading things…\n",
      "ah wait no\n",
      "there’s a `relative_velocity_x` but no `relative_x`\n",
      "sorry! yeah i guess we need relative position fields, we don’t have\n",
      "ftr we have a globus endpoint for large files\n",
      "if you search for dxl it should pop up\n",
      "hi all, i have been having a problem with `yt.enable_plugins()` that i recently posted about on the forum, but i wanted to reach out to people here to see if they can reproduce my problem or have seen it before, or know how to fix it.\n",
      "\n",
      "i recently updated to yt v3.5.0, and now there are some fields in my plugins file which are not being loaded in properly. i have a sample data set (`dd0041`), a problem script (`test_yt.py`), the error i receive from `test_yt.py` (`error.txt`), a script which works (`test_yt_works.py`), and my plugins file (`my_plugins.py`) within this dropbox link (<https://www.dropbox.com/sh/iqrqxl0f4m3hqka/aadyihlcqxtho137giqtqcqja?dl=0>). please note that the dataset is about 275 mb.  there are a few fields in my plugins file that are not working properly, one specifically is my definition for `j21_lw` (see attached code snippet). it would be great if someone could run my short scripts with yt v3.5.0, and see if they get a similar error. thanks in advance for taking the time to look at this. please let me know if you have any questions!\n",
      "\n",
      "<@ufypfd5cl> thanks for all the info, i’m going to try to look at this today :slightly_smiling_face:\n",
      "awesome, thank you!\n",
      "do you have a github account i can ping when i open an issue about this?\n",
      "yes! my username is drenniks.\n",
      ":+1:\n",
      "<@u91855pa9> i’ve staged the file at <http://yt-project.org/data/simbaexample.tar.gz|yt-project.org/data/simbaexample.tar.gz>, it weighs in at 14 gb compressed (we use the compressed size for the datafiles.json file)\n",
      "good stuff\n",
      "do you know why, although a simple `yt.sliceplot(ds, 'z', 'density')` gives me correct units now without any tweak,  `plot = yt.profileplot(ds, \"z\", [\"density\"])` displays my x axis in cm? moreover, depending on values, it can display a tick label but no unit, or unit but no tick label? hereafter related figures\n",
      "\n",
      "\n",
      "for the second plot it looks like the label is getting cut off, that's a bug, it shouldn't be happening\n",
      "you can customize the units of the x axis iirc\n",
      "yeah, `plot.set_unit(\"z\", \"kpc\")`\n",
      "for the first plot, not sure offhand why it's not displaying a tick label, it could be there's not enough dynamic range to do so and switching to linear scaling for the y axis will fix that\n",
      "`plot.set_unit(\"z\", \"kpc\")` works, i will start with that, cheers!\n",
      "<@urd7nhdj9> has joined the channel\n",
      "hi all, i’m using trident to generate spectra from lagrangian simulations, and i’ve found that *slow loading of data in yt can lead to very long times to generate rays*. is anyone familiar with this or related issues? here’s some more explicit information:\n",
      "• the loading times mean it can take ~10 min to generate a ray, depending on simulation resolution.\n",
      "• 626 of the 627 seconds spent generating a ray are spent calling `yt/utilities/io_handler.py:158(_read_particle_selection)`.\n",
      "• it’s not clear why it takes so long to read the data. using h5py to open the same data directly and perform simple calculations takes <5 seconds.\n",
      "• i’m using updated demeshened-compatible versions of trident and yt; version 1.3.dev1, yt version 4.0.dev0 changeset 947ebdc22c48\n",
      "for interested parties i’ve attached a notebook capable of reproducing these issues. thanks!\n",
      "can you file an issue against trident on github please?\n",
      "<@ue8nzm52l> wow -- i'm not sure this is a trident issue; it may very well be a yt one.  it spending its tiem in that function suggests it's yt taking way too long.  can you say more about the *data source* you're using?  is it `all_data` or something like it, or is it a sphere, or something more complex?  and, is the data spread between multiple files?\n",
      "i've had issues using masks on open h5py objects when h5py attempts to use \"fancy indexing\" with hdf5 hyperslabs. you think you're saving time by opening only a subset of the data but in reality it takes &gt;&gt;10x as long\n",
      "very true!  i think we *mostly* avoid that in yt, but i should double check...\n",
      "when zach and i were talking about this between us my suspicion was that yt was trying to save memory/ time opening a subset of the data that is fancy indexed to the ray\n",
      "yes, i chatted with <@u042s6y2g> about this on the trident slack, and we came to the conclusion it’s a yt one, and i should continue the discussion here. i appreciate trying to direct the discussion to the appropriate channel though.\n",
      "regarding the data format, the raw data is a lagrangian cosmological zoom-in simulation with data separated into multiple files. total file size ~8gb.\n",
      "the yt data object is produced by `ds.ray()`  after loading with `yt.load`\n",
      "here’s a relevant link:\n",
      "<https://github.com/h5py/h5py/issues/293>\n",
      "i'm going to start a thread here, even though i usually forget to.  :wink:  i have a few followups before i try to diagnose.  does this happen *regardless* of the location you pass the ray through?  if you send it through the edge of the box, does it go considerably faster?\n",
      "and, any chance you could run it with cprofile and send me the resultant output?\n",
      "happens regardless of location. haven’t tested with edge of box, since that’s way outside the zoom-in region usually.\n",
      "the notebook above contains the results of cprofile.\n",
      "oh!  i didn't read the notebook; i will check it out.  i have to handle some teaching responsibilities for a bit, but i will return to this.  i'm optimistic there's something we can do, or at least, figure out a bit more about.\n",
      "\n",
      "one last question -- what's the \"depth\" of the lagrangian zoom-in?\n",
      "i.e., the dynamic range\n",
      "particle mass resolution ~7000 msun, for a total gas mass of a few times 1e12 msun (i.e. out to a few times the virial radius of a l* galaxy). high dynamic range, in other words.\n",
      "also, i realized i did send a sightline through the edge of the box, and it took about as long.\n",
      "ok, that may be *very* useful information -- and may simply expose something we have not dealt with in the bitmap indexing.  i will have further diagnostics to request, but if you could, would you be able to send the `.ewah` files?\n",
      "at this point i have to close down slack -- could you file an issue with some of this (on yt) and i will assign myself, so i do not lose track?\n",
      "will do. thanks!\n",
      "ok great, maybe file a yt issue then? sounds like matt is working with you though\n",
      "<@u5scnpebf> has joined the channel\n",
      "hello, everyone. \n",
      "not entirely sure if this is the right place to ask, but i have encountered a field definition in yt that i cannot wrap my head around. what does the \"create_averaged_field\" method actually do? there are no comments in the source and the docs do not tell much.\n",
      "i mean sure, it computes weighted means. but with respect to what? a coordinate axis? this method is associated e.g. with the field (gas, averaged_density\". maybe you've heard of it.\n",
      "hi simon, i just looked through the source code.  it looks like it takes a full 3d grid and creates an averaged field that is weighted by cell mass by default, using 3x3x3 cubes of cells.  it does this by throwing away the outer layer of cells, so if you have an (nx, ny, nz) grid, the weighted grid will be (nx-2, ny-2, nz-2)\n",
      "the default weighting field is cell_mass, but it seems that you can use whatever you want\n",
      "this line right here is important: <https://github.com/yt-project/yt/blob/484cb11b3fa848a586ccc8eae3d5eb438dcb6c9f/yt/fields/vector_operations.py#l336>\n",
      "if you use a validatespatial field validator with a field definition, you will be working with 3d structured grid data\n",
      "hi everyone, is there a quick way to add a particle field/deal with particles on an individual basis in yt? i'm looking to modify the following:\n",
      "\n",
      "<http://yt-project.org/doc/_modules/yt/fields/xray_emission_fields.html#add_xray_emissivity_field>\n",
      "\n",
      "so that it calculates the x-ray emissivity on a per-particle basis in my simulations rather than on the yt-mesh, because some of my analysis tools rely on the per-particle information. thanks!\n",
      "from first glance it seems that i could simply copy everything with ('gas', etc..) and replace it with, for example, ('parttype0', etc...) but i want to be sure this won't mess anything up\n",
      "hi <@uggk0erpa> is it gizmo? are you using 3.5 or 4.0 version? i'm not familiar with this specific field. but in general, in yt 4.0, 'gas' and 'parttype0' are almost the same. they are both per-particle.\n",
      "i am using gizmo yes, and i'm using 3.5.1 currently. so if i counted the number of 'gas' objects and 'parttype0' i would get the same number out?\n",
      "okay, so in 3.5.1 they are different. you are right 'gas' is the mesh field, and you should add field to 'parttype0' for the per-particle field.\n",
      "here is an example to add a particle field: <https://github.com/yt-project/yt/blob/14f330d5200c9e1e9036030433b3b677aef5c97c/yt/frontends/gizmo/fields.py#l96>\n",
      "fantastic! thank you :smile:\n",
      "besides changing ptype, note that also to specify `sampling_type=\"particle\"`\n",
      "thanks so much\n",
      "is there a way to silence all the logging that comes out of `yt.load`? i can `import logging` and `logging.disable(<http://logging.info|logging.info>)`, but that seems a bit overbroad because it's a global setting.\n",
      "you can do `yt.funcs.mylog.setlevel(some_high_number_here)`\n",
      "<@u011y3xm493> has joined the channel\n",
      "hello everyone. i want to plot the cell mass at every number density, but with decreasing number density. i've only been able to do this with profileplot\n",
      "any help is appreciated\n",
      "<@u011y3xm493> hi! do you want to just reverse x axis?\n",
      "```import yt\n",
      "ds = yt.load(\"enzo_tiny_cosmology/dd0046/dd0046\")\n",
      "ad = ds.all_data()\n",
      "plot = yt.profileplot(ad, \"density\", [\"temperature\", \"velocity_x\"],\n",
      "                      weight_field=\"cell_mass\",\n",
      "                      plot_spec=dict(color='red', linestyle=\"--\"))\n",
      "#### relevant part\n",
      "# reverse x axis\n",
      "for p in plot.plots.values():\n",
      "    p.axes.set_xlim(p.axes.get_xlim()[::-1])\n",
      "####\n",
      "\n",
      "plot.save()```\n",
      "something like this ^^ should work i think\n",
      "thank you for the prompt reply!\n",
      "i'll try it\n",
      "there might be some magic option directly in matplotlib, which can be passed as `mpl_kwargs` in `plot.save()` but i'm not that familiar with it to know off-hand\n",
      "so i basically edited like this:\n",
      "import yt\n",
      "from yt.units import msun\n",
      "ds = yt.load(\"~/scratch/smallnh2/dd0002/dd0002\")\n",
      "ad = ds.all_data()\n",
      "plot = yt.profileplot(ad, \"number_density\", \"cell_mass\", weight_field= none, accumulation= true)\n",
      "#### relevant part\n",
      "# reverse x axis\n",
      "for p in plot.plots.values():\n",
      "    p.axes.set_xlim(p.axes.get_xlim()[::-1])\n",
      "####\n",
      "plot.set_units(\"cell_mass\",\"msun\")\n",
      "plot.save()\n",
      "\n",
      "\n",
      "and it gave me a weird plot, is there something wrong with it? also if i want the number density to go from 1e8 to 1e7 how do i write that?  p.axes.set_xlim(p.axes.get_xlim(1e8,1e7)[::-1]) ??\n",
      "also if i do this\n",
      "import yt\n",
      "from yt.units import msun\n",
      "ds = yt.load(\"~/scratch/smallnh2/dd0002/dd0002\")\n",
      "ad = ds.all_data()\n",
      "plot = yt.profileplot(ad, \"number_density\", \"cell_mass\")\n",
      "plot.set_unit(\"cell_mass\",\"msun\")\n",
      "#### relevant part\n",
      "# reverse x axis\n",
      "for p in plot.plots.values():\n",
      "    p.axes.set_xlim(p.axes.get_xlim()[::-1])\n",
      "####plot.set_unit(\"cell_mass\",\"msun\")\n",
      "plot.save()\n",
      "\n",
      "the x axis is increasing again\n",
      "if you know your limits, you can do `plot.set_xlim(1e8, 1e7)`\n",
      "tried that but didnt work\n",
      "gave me empty plot\n",
      "hmm, give me a sec, i'll try to reproduce locally\n",
      "<@u011y3xm493> try this:\n",
      "```import yt\n",
      "from yt.units import msun\n",
      "ds = yt.load(\"~/scratch/smallnh2/dd0002/dd0002\")\n",
      "ad = ds.all_data()\n",
      "plot = yt.profileplot(ad, \"number_density\", \"cell_mass\", weight_field= none, accumulation= true)\n",
      "plot.set_units(\"cell_mass\",\"msun\")\n",
      "# any yt specific modifications have to happen before this line\n",
      "plot._setup_plots()\n",
      "\n",
      "# reverse x axis\n",
      "for p in plot.plots.values():\n",
      "    p.axes.set_xlim(p.axes.get_xlim()[::-1])\n",
      "\n",
      "plot.save()```\n",
      "it's unfortunately prone to the order of operations\n",
      "i'm sorry it's so brittle, we should definitely fix `plot.set_xlim()` to make it more intuitive and easy\n",
      "sorry to bother u so much but i got no one else to help. this gave same plot as before.\n",
      "is there any way i can specify the limits and make it decrease\n",
      "ok wait i just realized something, when i do set_xlim(1e8,1e7) the axis is actually decreasing but nothing there.\n",
      "without accumulation the graph makes sense:\n",
      "this one with accumulation true, which makes no sense\n",
      "<@u011y3xm493> i think it's because the accumulation only goes left to right -- and your number density here goes high to low.\n",
      "<@u011y3xm493> not sure i understand, the axis is decreasing now on all plots\n",
      "@matt what i suggested above should work around it\n",
      "we're literally doing everything and then setting xlim on the axis\n",
      "<@u011y3xm493> on the first plot looks like you ylim is wrong\n",
      "it's just below the curve\n",
      "that's why it's empty\n",
      "oh, it's grams nvm\n",
      "<@u042f73r7> oh sorry\n",
      "but 10^1 g doesn't look like a lot\n",
      "<@u042f73r7> the last 2 plots are ur script\n",
      "ok, why it's wrong?\n",
      "<@u042f73r7> the first empty plot with xlim(1e8,1e7)\n",
      "<@u042f73r7> without the accumulation it's correct, but now i need to sum them up the curve should be increasing\n",
      "the first empty plot has y range between 1 and 10g, does those value make sense for your data?\n",
      "i didn't specify the ylim in that plot\n",
      "u think if i do it would be correct?\n",
      "but a priori i cant know the limits on y\n",
      "what matt said is making my wonder if i understood you correctly. you don't want to just reverse the axis\n",
      "you also want the accumulation to go from maximum value and increase by adding the decreasing values?\n",
      "you also want the accumulation to go from maximum value and increase by adding the decreasing values? (edited) yes basically start with the mass at the highest density and increase from there\n",
      "sorry if i was ambiguous\n",
      "ok, i think it's gonna be easier to achieve outside of profileplot, give me a sec\n",
      "the idea is simple yet i couldnt formulate it well with the append quantities\n",
      "i'm trying to compute the data you want and create a mock profile plot out of it (by saving it as a dataset) but unfortunately i'm failing...\n",
      "this is what i've got so far in terms of computing the values you want\n",
      "```import yt\n",
      "from yt.data_objects.profiles import create_profile \n",
      "from yt.visualization.profile_plotter import profileplot\n",
      "\n",
      "ds = yt.load(\"enzo_tiny_cosmology/dd0046/dd0046\")\n",
      "ad = ds.all_data()\n",
      "prof = create_profile(\n",
      "    ad, \n",
      "    \"number_density\",\n",
      "    n_bins=[64],\n",
      "    fields=[\"cell_mass\"],\n",
      "    weight_field=\"cell_mass\",\n",
      "    accumulation=false,\n",
      "    logs=none\n",
      ")\n",
      "\n",
      "# do accumulation in reverse manually\n",
      "data = prof[(\"gas\", \"cell_mass\")][::-1].cumsum(axis=0)[::-1]```\n",
      "wait that's probably wrong if you have weight\n",
      "<@u042hlt7u> can you help?\n",
      "<@u042f73r7> <@u011y3xm493> yes, am about to start teaching\n",
      "thank you so much for ur time, any time will be fine thank u again\n",
      "this is what i ended up with:\n",
      "```import numpy as np\n",
      "import yt\n",
      "from yt.data_objects.profiles import create_profile \n",
      "\n",
      "ds = yt.load(\"enzo_tiny_cosmology/dd0046/dd0046\")\n",
      "ad = ds.all_data()\n",
      "prof = create_profile(\n",
      "    ad, \n",
      "    \"number_density\",\n",
      "    n_bins=[64],\n",
      "    fields=[\"cell_mass\"],\n",
      "    weight_field=\"cell_mass\",\n",
      "    accumulation=false,\n",
      "    logs=none\n",
      ")\n",
      "\n",
      "\n",
      "# do accumulation in reverse manually\n",
      "temp = np.flipud(prof.field_data[(\"gas\", \"cell_mass\")])\n",
      "weight = np.flipud(prof.weight)\n",
      "data = (temp  * weight).cumsum(axis=0) / weight.cumsum(axis=0)\n",
      "# overwrite profile data\n",
      "prof.field_data[(\"gas\", \"cell_mass\")] = np.flipud(data)\n",
      "# save as temporary dataset\n",
      "prof.save_as_dataset(\"my_profile.h5\")\n",
      "\n",
      "prof_ds = yt.load(\"my_profile.h5\")\n",
      "# create a plot with modified profile\n",
      "plot = yt.profileplot(prof_ds.data, none, none)\n",
      "plot.set_unit(\"cell_mass\",\"msun\")\n",
      "plot._setup_plots()\n",
      "# reverse x axis\n",
      "for p in plot.plots.values():\n",
      "    p.axes.set_xlim(p.axes.get_xlim()[::-1])\n",
      "plot.save()```\n",
      "the results are very confusing to me, cause even though it's an \"accumulation\" the curve is actually decreasing, but i looked at `cumsum / cumsum` long enough that i think i know why it's happening...\n",
      "do you think it can be fixed? do i wait for your reply?\n",
      "<@u042hlt7u> assigning the octree to the `ds.parameters` dict worked as needed. thanks again!\n",
      "hooray\n",
      "in yt3.x, does setting `over_refine_factor &gt; 1` not impact the actual saved octree? (for example the following):\n",
      "\n",
      "```\n",
      "import yt\n",
      "\n",
      "#load with no over refining\n",
      "\n",
      "ds = yt.load('snapshot_094.hdf5',over_refine_factor=0)\n",
      "ad = ds.all_data()\n",
      "saved =\tds.index.oct_handler.save_octree()\n",
      "refined = saved['octree']\n",
      "print(len(refined))\n",
      "\n",
      "#now load with over refining\n",
      "\n",
      "ds = yt.load('snapshot_094.hdf5',over_refine_factor=2)\n",
      "ad = ds.all_data()\n",
      "saved =ds.index.oct_handler.save_octree()\n",
      "refined = saved['octree']\n",
      "print(len(refined))\n",
      "```\n",
      "\n",
      "results in:\n",
      "```\n",
      "179225\n",
      "179225\n",
      "```\n",
      "or, is there a way to access a `refined` array that reflects the over_refined cells?\n",
      "related to this tangentially:\n",
      "\n",
      "in yt 4.x, is there a way to access the same `refined` array? (i.e. after creating the octree with something like):\n",
      "\n",
      "```\n",
      " octree = ds.octree(left, right, n_ref=64)\n",
      "```\n",
      "\n",
      "though here it seems like the `octree` doesn't have a key (at least thats obvious to me) for the `refined` array\n",
      "hi all, i'm working with some semi-structured grid data and trying to make use of `cut_region` data objects, but the `cut_region`s don't seem to work properly when passed to the `data_source` argument of either `sliceplot` or when creating another data object. creating the `cut_region` works as it should, but passing it to `data_source` has no effect. i'm working from my own fork of `master`, forked at commit `2407793`. my changes were just bug fixes to the exodus ii frontend and `hexahedral_connectivity`.\n",
      "<@uatgd4b6f> are you using exodus ii?\n",
      "not anymore, i used yt to convert it to hdf5 and now i'm just loading it in as semi-structured following the docs\n",
      "<@uatgd4b6f> ah, ok -- it may be that `cut_region` is not working properly with the hexahedral stuff.  can you see if a cut region with the sample dataset does the same behavior, and file a bug?\n",
      "sample dataset as in the one used in the docs?\n",
      "yup, or one off the website\n",
      "hi, i recently updated yt to 3.5.1. when i try to load my ramses data using just  ds = yt.load(“output_00001/info_00001.txt”), the error is that: ‘ramsesdataset’ object has no attribute ‘bbox’. can anyone kindly tell me how to set the bbox and load ramses data in yt 3.5.1?\n",
      "this sounds like a bug to me\n",
      "is there any chance you can share a dataset that triggers the issue? you can upload files smaller than 5 gb using “yt upload some_file.tar.gz”\n",
      "that will print out a url you can share in here\n",
      "let me try\n",
      "hi, here is the url <http://use.yt/upload/2d98cead>\n",
      "thank you, let me see if i can reproduce your issue\n",
      "thank you\n",
      "i can reproduce :slightly_smiling_face:\n",
      "let me see what’s going on…\n",
      "looks like it’s a one-line fix, going to make a pull request\n",
      "thank you very much for the report\n",
      "thank you, how can i get the fixed code? i used pip to install yt on the cluster\n",
      "so a really hacky way to apply the fix would be to patch the installed version of yt on the cluster\n",
      "a better way would be to clone the yt git repository and pull from github to get the fix\n",
      "and build yt from source\n",
      "for the first case you can get the path to the installed copy of yt by printing `yt.__file__`\n",
      "for the latter you can look at <http://yt-project.org/doc/installing.html#installing-yt-from-source>\n",
      "do you have a github username?\n",
      "yep, it’s suniverse\n",
      "<https://github.com/yt-project/yt/pull/2200>\n",
      "thanks for bringing this to our attention :slightly_smiling_face:\n",
      "thank you\n",
      "<@ulbk1pvsr> you can also do .max(field) on a data object\n",
      "that's great, thank you all again !\n",
      "<@u91855pa9> you can get the image from the `frb` attribute of the plot, like this: `plot.frb[my_field]`\n",
      "that will be the image buffer you ant\n",
      "<@uc6l85lbb> what do you mean by “plot time-average fields”?\n",
      "like for a slice or something?\n",
      "if you mean that, no, there isn’t a straightforward way to tell yt to calculate that at the moment, although you could iterate over the datasets, make a series of fixed resolution buffers, then do the averaging yourself\n",
      "yeah exactly. that’s what i am currently doing at the moment but i wanted to see if there was some capability that i was missing out on. thank you!\n",
      "it would be cool to add more functionality like that\n",
      "if you’d be interested in spitballing a design or even writing a little code, this is something we could chat about in <#cbe6579cz|development> or on the yt-dev mailing list\n",
      "that sounds like a really neat feature\n",
      "better support for more advanced time series analysis has definitely come up in the past\n",
      "i actually would be interested. we (me and my advisor) are potentially looking to do some advanced linear algebra on amr fields; writing some code to make time average fields would be an excellent starting point.\n",
      "i will send a shout to <#cbe6579cz|development> whenever i get started on that work.\n",
      "awesome! looking forward to hearing more\n",
      "<@uc6l85lbb> that sounds so cool\n",
      "hey all (and probably particularly <@u042s6y2g>), i'm trying to use ytree to stitch together the out.lists from rockstar.  everything works fine so long as i stick within the ytree environment, but i'd like to convert the data to another format.\n",
      "\n",
      "most of this is working fine -- i can do `a[0]['tree'] ` gives me the halos in that tree, and i can use `a[0]['prog']` to get the main branch, which is what i need.  the problem is that i can't figure out how to go from a node to that node's location in, e.g., `a['mass']` -- even the `uid` doesn't seem to match.  using the test data on yt hub:\n",
      "```\n",
      "a[0]['tree', 'uid'][1]\n",
      "&gt;&gt; 342\n",
      "342 in a['uid'].astype('int')\n",
      "&gt;&gt;&gt; false\n",
      "```\n",
      "is there any property of a given node that gives me a way (without searching ideally) for that halos location in the root fields?\n",
      "hmm, i thought i had this figured out via a derived field:\n",
      "```\n",
      "def tree_index(field, data):\n",
      "    import yt\n",
      "    \n",
      "    return yt.units.yt_array.ytarray(\n",
      "        np.arange(data.arbor.size, dtype=float))\n",
      "a.add_derived_field(\"treeindex\", tree_index, units='')\n",
      "```\n",
      "but the problem is that it's recalculated for each tree, so i just get `np.arange(a[ii].size)` if i do `a[0]['tree', 'tree.index']`.  i also tried using an analysis field, but it wouldn't let me directly assign except on a node-by-node basis, as far as i can tell, which isn't useful for my purposes (i.e., i can't do `a['tree.index'] = np.arange(a.size)`).\n",
      "hi all,\n",
      "\n",
      "hope all is well; i was hoping you might give some advice on my analysis of some `athena++` simulations with `yt`:\n",
      "\n",
      "my mesh is a cartesian, uniform grid; but i want to analyze average quantities vs radius in a yt disk object:\n",
      "\n",
      "```# ytdisk\n",
      "my_center = ds.find_max(\"density\")[1]\n",
      "my_disk = ds.disk(my_center, [0.0, 0.0, 1.0], (8, 'r_earth'), (5, 'r_earth'))```\n",
      "\n",
      "and then i believe the radius i should choose here is the `cylindrical_radius`:\n",
      "\n",
      "```profile = yt.create_profile(my_disk, \"cylindrical_radius\", \"cell_mass\")```\n",
      "\n",
      "but the bounds of my profile are not from 0, 8 but rather:\n",
      "\n",
      "```\n",
      "# range of cylindrical_radius\n",
      "profile.x.in_cgs() / yt.units.r_earth.in_cgs()\n",
      "```\n",
      "\n",
      "```\n",
      "ytarray([14.27000956, 14.3696463 , 14.46997873, 14.57101171, 14.67275012,\n",
      "         14.7751989 , 14.87836299, 14.98224741, 15.08685717, 15.19219734,\n",
      "         15.29827303, 15.40508936, 15.51265151, 15.62096468, 15.73003413,\n",
      "         15.83986512, 15.95046298, 16.06183306, 16.17398076, 16.2869115 ,\n",
      "         16.40063075, 16.51514402, 16.63045684, 16.74657481, 16.86350354,\n",
      "         16.9812487 , 17.09981598, 17.21921113, 17.33943993, 17.46050819,\n",
      "         17.58242179, 17.70518661, 17.82880861, 17.95329376, 18.07864811,\n",
      "         18.20487771, 18.33198867, 18.45998715, 18.58887936, 18.71867152,\n",
      "         18.84936992, 18.98098088, 19.11351079, 19.24696606, 19.38135314,\n",
      "         19.51667855, 19.65294883, 19.79017059, 19.92835046, 20.06749514,\n",
      "         20.20761136, 20.34870591, 20.49078561, 20.63385735, 20.77792805,\n",
      "         20.92300469, 21.06909429, 21.21620393, 21.36434072, 21.51351184,\n",
      "         21.6637245 , 21.81498599, 21.96730363, 22.12068478]) (dimensionless) ```\n",
      "\n",
      "if instead of `cylindrical_radius` i use `radius`, then the range of `profile.x.in_cgs() / yt.units.r_earth.in_cgs()` is not between [0, 8]:\n",
      "\n",
      "```\n",
      "# range of \"radius\"\n",
      "profile.x.in_cgs() / yt.units.r_earth.in_cgs()\n",
      "```\n",
      "\n",
      "```\n",
      "ytarray([0.06382166, 0.06904446, 0.07469467, 0.08080727, 0.08742008,\n",
      "         0.09457405, 0.10231346, 0.11068622, 0.11974416, 0.12954336,\n",
      "         0.14014446, 0.1516131 , 0.16402026, 0.17744276, 0.19196368,\n",
      "         0.20767292, 0.2246677 , 0.24305325, 0.26294337, 0.28446118,\n",
      "         0.30773989, 0.3329236 , 0.3601682 , 0.38964235, 0.4215285 ,\n",
      "         0.45602403, 0.49334248, 0.53371486, 0.5773911 , 0.62464155,\n",
      "         0.67575872, 0.73105903, 0.7908848 , 0.85560638, 0.92562442,\n",
      "         1.00137233, 1.08331904, 1.1719718 , 1.2678794 , 1.37163555,\n",
      "         1.48388252, 1.60531516, 1.73668516, 1.87880574, 2.03255668,\n",
      "         2.19888973, 2.37883455, 2.57350504, 2.78410628, 3.01194194,\n",
      "         3.2584224 , 3.52507344, 3.8135457 , 4.12562492, 4.46324295,\n",
      "         4.82848976, 5.22362631, 5.65109863, 6.11355288, 6.61385181,\n",
      "         7.15509239, 7.74062507, 8.37407447, 9.05936181]) (dimensionless) ```\n",
      "\n",
      "which makes me think that `yt` is actually using the spherical radius.  am i misunderstanding something here? thanks so much in advance for any/all help.\n",
      "yes, “radius” is the spherical radius\n",
      "i don’t know offhand if cylindrical_radius respects the “center” field parameter, that might be the issue?\n",
      "that’s what my first guess is. although i am not sure if this is a bug, limitation, or user error.\n",
      "see if the values of `my_disk['cylindrical_radius']` change after you do `my_disk.set_field_parameter('center', some_position)`\n",
      "also instead of going through the profile machinery it might be easier to reason about what’s going on if you look at the field values in `my_disk` explicitly\n",
      "finally if you can make a runnable example (e.g. with imports, loading a dataset or constructing one in memory) one of us could take a look as well\n",
      "i believe there was at some point a conscious decision -- i cannot recall much of the context -- to *not* feed the \"center\" attribute of an object into the field parameters by default.\n",
      "that’s not true though?\n",
      "i think?\n",
      "i can confirm that `my_disk.set_field_parameter('center', some_position)` does not change `my_disk['cylindrical_radius']`\n",
      "i will make a runnable example (with a smaller data file than i am presently using) and put it here\n",
      "```\n",
      "in [7]: sp = ds.sphere(ds.domain_center + 100*ds.units.kpc, (10, 'kpc'))\n",
      "\n",
      "in [8]: sp['radius'].to('kpc').min()\n",
      "out[8]: unyt_quantity(0.65855503, 'kpc')\n",
      "```\n",
      "perhaps it was just for some objects like rectangular prisms, or maybe i am completely off-base\n",
      "although it seems `cylinrical_radius` works too:\n",
      "```\n",
      "in [9]: sp['cylindrical_radius'].to('kpc').min()\n",
      "out[9]: unyt_quantity(0.53770793, 'kpc')\n",
      "```\n",
      "<@uhkuhfybf> did you start from a fresh interpreter? you might have been getting cached data\n",
      "but yeah, if you can make a runnable example that will make things less ambiguous\n",
      "\n",
      "\n",
      "<@u042fh0rb> i did start from a fresh interpreter.\n",
      "thanks so much for all the help everyone!\n",
      "ah you know what, i bet this is because of a periodic correction not doing the right thing…\n",
      "yeah, if i set `ds.periodicity = (false, false, false)` in your notebook i get an answer i’d expect\n",
      "ah and you know what i think i see where the bug is\n",
      "yup, pr incoming, thank you for the report\n",
      "<@uhkuhfybf> do you have a github handle?\n",
      "ah thanks so much for the quick fix!\n",
      "`@pdmullen` is also my github username :smile:\n",
      "cool, let me see if i can come up with a test using fake data and i’ll put in a pr\n",
      "hey <@uhkuhfybf> so this is a bit trickier than i thought, i’m going to file an issue because i don’t have a real fix, for your specific issue you can get a quick fix by setting `ds.periodicity = (false, false, false)` after you load your data\n",
      "this quick fix is perfectly fine for my purposes!\n",
      "i'm getting an error with export_sketchfab saying `attributeerror: 'ytsurface' object has no attribute 'vertex_data'` even though i have copied the example here: <http://yt-project.org/doc/reference/api/yt.data_objects.construction_data_containers.html?highlight=export_sketchfab#yt.data_objects.construction_data_containers.ytsurface.export_sketchfab>\n",
      "further traceback here\n",
      "```file \"xyz.py\", line 18, in &lt;module&gt;\n",
      "    color_field=\"radius\")\n",
      "  file \"/usr/local/lib/python3.7/site-packages/yt/data_objects/construction_data_containers.py\", line 1915, in export_sketchfab\n",
      "    sample_type = \"vertex\", no_ghost = no_ghost)\n",
      "  file \"/usr/local/lib/python3.7/site-packages/yt/data_objects/construction_data_containers.py\", line 1762, in export_ply\n",
      "    color_field not in self.vertex_data:```\n",
      "hi <@ucnrc0276>, you’re hitting a bug that was fixed back in may: <https://github.com/yt-project/yt/pull/1792>\n",
      "i’m working on getting a new release out\n",
      "for now if you update to the development version of yt you can get the fix\n",
      "basically, “pip uninstall yt”, clone the repo, then do “pip install -e .” in the repo, more instructions here: <http://yt-project.org/doc/installing.html#installing-yt-from-source>\n",
      "happy to help out if you run into issues updating\n",
      "sorry for the trouble! hopefully we’ll have a new release out soon.\n",
      "hi <@u042fh0rb>, i wasn't completely sure what to do with your message about the cell_mass coming up as zero in the phaseplot.  should i be able to pull the change #1850 using git somehow so i can make it phaseplot work?\n",
      "i don’t know, i haven’t checked yet\n",
      "i’m working on fixing the travis tests before i look at that\n",
      "it’s possible that applying #1850 to your local copy will fix the issue\n",
      "i just haven’t actually checked that yet\n",
      "okay, i will see if i can figure it out and let you know!\n",
      "hi all! did anyone use yt to determine the extent of a protostar like in greif 2012?\n",
      "or if i want to overplot the contour of a certain density  ie if i just want to see the radius at which the density is 1e10gcm-3 only\n",
      "10e-10*\n",
      "hi, i am not familiar with the work of greif. you may be interested in the \"overplotting contours\" functionality <https://yt-project.org/doc/visualizing/callbacks.html#overplot-contours>\n",
      "hey thanks for the reply. i used\n",
      "```p.annotate_sphere([0.5, 0.5, 0.5], radius=(2, 'kpc'),\n",
      "                  circle_args={'color':'black'})```\n",
      "how do i set the center to be the maximum density?center='max' did not work\n",
      "you can first locate the maximum density using e.g. `ad.argmax('density')` , where `ad` is your data source (e.g. `ad = ds.all_data()`, or `ad = ds.sphere([...])`\n",
      "then you can use this as input to the `annotate_sphere` callback\n",
      "there’s also `ds.find_max()ˋ fyi \n",
      "<@u0159np22bv> has joined the channel\n",
      "<@uky0ku3ml> i totally recommend using the quickstart notebooks like ben kimock said! they’re located here: <https://yt-project.org/doc/quickstart/index.html> . if you need any help getting them up and running let us know!\n",
      "\n",
      "i also think the cookbook is has a lot of good examples of how to use some of the functionality in yt: <https://yt-project.org/doc/cookbook/index.html>\n",
      "\n",
      "if you think there’s anything confusing or that could be improved in the documentation, please let us know!\n",
      "\n",
      "i think that the particular dataset you downloaded isn’t used in the cookbook or the quickstart notebooks, but there are several other datasets that are. however, there’s a section in the documentation that goes over loading generic particle data here that might be helpful: <https://yt-project.org/doc/examining/generic_particle_data.html#generic-particle-data>\n",
      "also, my dataset wasn't gizmo it was gadget. i think we might have a multifile gadget snapshot in the yt data? \n",
      "we could probably reproduce the issue with one of those, but unfortunately they are quite large i think  \n",
      "we don’t have any in the hdf5 format, but maybe it doesn’t matter?\n",
      "i suspect it doesn't\n",
      "but not sure\n",
      "what about `snapshot_033`?\n",
      "oh you’re right\n",
      "thats also not super big\n",
      "i can check locally if that has the issue\n",
      "yes\n",
      "that reproduced it\n",
      "issue pls :slightly_smiling_face:\n",
      "in my case, to reproduce the error i needed to have a new python instance, but only tried a couple of times\n",
      "on it\n",
      "<@u91855pa9> <https://github.com/yt-project/yt/issues/2059> thanks for the report, i’ll try to take a look soon!\n",
      "and thanks for digging in ash :slightly_smiling_face:\n",
      "general question regarding the memory consumption of yt… (has somewhat to do with issue 2154 or issue 1909): how much memory usage should i expect for a dataset size s analysed on n nodes with n processors per node? i.e., ~s/n on each node would be great but my experience it is more ~s*n… but likely i am doing something far from optimal. i know there was a discussion using dask at some stage but this is not the current status, is it?\n",
      "<@u010zdngdpa> has joined the channel\n",
      "hi, i've tried to install my own yt fork in a separate conda environment. i can do `yt --help`  and `yt version`  and suchlike from the command line without a problem, but whenever i try to import yt from within a python script i get this error: `modulenotfounderror: no module named 'yt'` . any idea what i can do to fix it? thanks!\n",
      "sounds like your fork was not installed correctly. try `which yt` from within your fork env ? i suspect this will point to your base env’s yt installation\n",
      "that returns `/c/users/windows/.conda/envs/yt development/scripts/yt` , which is the fork environment\n",
      "in that case, how do you run your python script ?\n",
      "i've tried running it from the command line and in a jupyter notebook (using anaconda navigator). it returns the same error in both cases\n",
      "when you say command line, you mean within ipython i’m guessing ?\n",
      "if so, did you make sure to install ipython (and jupyter) in your fork env ?\n",
      "there's an option to open environment in terminal from the anaconda navigator, i'm not really sure beyond that. but jupyter is definitely installed\n",
      "jupyter is based off ipython so i would definetly run `which ipython`\n",
      "(and `which jupyter` to double check)\n",
      "they returned `/c/users/windows/.conda/envs/yt development/scripts/ipython`  and `/c/users/windows/.conda/envs/yt development/scripts/jupyter`\n",
      "hm. and how did you install yt there ?\n",
      "i opened the environment in terminal (via the anaconda navigator) and then i followed the instructions for _building yt from source for conda-based installs (_<https://yt-project.org/doc/installing.html#installing-yt-using-anaconda>), but cloning my fork rather than the main repository\n",
      "damn, then i have no idea what could have gone wrong. i have zero experience with the anaconda navigator so i don’t understand if/how that could be a source of problem.\n",
      "thanks for the help anyway! i might just try again with a new environment and see how that goes. it's the first time i've tried something like this so i could easily have made a mistake\n",
      "if it still doesn’t work, maybe open an issue on github. documenting how you did it, your python/conda version might also give some clues. i for one wouldn’t be able to reproduce your setup since i don’t have a windows machine, but maybe someone else could. sorry i couldn’t figure it out\n",
      "<@u91855pa9> it happens in _setup_plots, <https://github.com/yt-project/yt/blob/master/yt/visualization/plot_window.py#l911>\n",
      "<@u042fh0rb> should i ask for help/feedback on adding a yt feature for plotting arbitrary images in the issue or in <#cbe6579cz|development>\n",
      "probably <#cbe6579cz|development>\n",
      "but during the week :)\n",
      "yt-dev mailing list is there too\n",
      "i'm trying to use ahf (amiga's halo finder)\n",
      "they need data in the gadget binary format, which i do not have; instead i have some hdf5 snapshots. they provide a conversion script, but to use it with our data would require some tenuous hackery.\n",
      "can yt help me here? is there a way to save a dataset as the gadget binary format?\n",
      "failing that, does anyone know of documentation for that format? i'm having trouble with my google-fu\n",
      "sorry, yt doesn't do that\n",
      "maybe one day it will but it's a seriously hard thing to do in general\n",
      "<@u0860sxlk> might be able to help, i know he's used ahf\n",
      "<@u0860sxlk> has joined the channel\n",
      "<@u042fh0rb> hey, related to that question i had.  i'm adding tests now and realizing i don't know -- in the tests -- how to add the default unit objects to a given `unitregistry()`.  should i be instantiating a `unitsystem` or maybe accessing an extant one and using that as the base for the tests?\n",
      "sorry, i don’t quite understand what you’re doing\n",
      "<@u042fh0rb> i want a `unitregistry` with lots of unit object entries.  i don't see a test harness that does this.\n",
      "the default `lut` has stuff, but not in unit object format\n",
      "yeah i don’t think that exists yet\n",
      "ok\n",
      "you could see how the `default_unit_registry` singleton gets set up\n",
      "i think it’s just unitregistry()\n",
      "and then you could add stuff to the default one\n",
      "yeah, that's what i was hoping to avoid, but i'll go for it anyway.\n",
      "ok, maybe i’ll have a better suggestion when i look at your test?\n",
      "it looks like the unitregistry class isn’t really tested on its own\n",
      "hi <@u91855pa9>, i know that people usually write their own conversion script, me included. i could share my python script with you if that helps. one thing is that you do not need to convert all fields to the binary format for ahf to work. that might make the conversion easier. i’ve also tried to make a more general gadget format conversion tool, but it’s not stable yet. you could try if it works out of the box: <https://github.com/qobilidop/gadgeteer>\n",
      "<@u042fh0rb> followup: i would have expected everything that is in the lut to be instantiated and added to `unit_objs`, but seems that's not the case.  i assume this is by design, but i wondered if you could say why?\n",
      "enh, n/m, i'll work around it.\n",
      "i think `unit_objs` is just there as a cache, it’s not necessarily fully populated at any given time. the lut should be the source of truth.\n",
      "ah, ok\n",
      "for the last question, the documentation is <https://wwwmpa.mpa-garching.mpg.de/gadget/users-guide.pdf> section 6. note that there’s a small difference between format 1 (6.1) and format 2 (6.2). for ahf, both formats are accepted.\n",
      "<@u042hlt7u> is there stuff in `unit_objs` that’s not in the `lut`?\n",
      "i don't know, to be honest.\n",
      "<@u0860sxlk> i managed to get something hooked up to ahf, but i'm running with the openmp configuration and it's launched 32 threads, but is only running on one. any suggestions?\n",
      "yikes\n",
      "```\n",
      "in [11]: set(list(reg.lut.keys()) + list(reg.unit_objs.keys())) == set(list(reg.lut.keys()))\n",
      "out[11]: false\n",
      "```\n",
      "```\n",
      "in [12]: set(list(reg.lut.keys()) + list(reg.unit_objs.keys())) - set(list(reg.lut.keys()))\n",
      "out[12]:\n",
      "{'c**2/n/m**2',\n",
      " 'j/kg',\n",
      " 'n/a**2',\n",
      " 'cm**2',\n",
      " 'cm**3/g/s**2',\n",
      " 'cm/s',\n",
      " 'cm/s**2',\n",
      " 'dyne/cm**2',\n",
      " 'erg*s',\n",
      " 'erg/k',\n",
      " 'erg/cm**2/s**1/k**4',\n",
      " 'erg/g',\n",
      " 'ft*lbf',\n",
      " 'lbf/ft**2'}\n",
      "```\n",
      "ah, i guess that makes sense\n",
      "these are all compound units\n",
      "the lut will only have atoms\n",
      "nm carry on :slightly_smiling_face:\n",
      "i haven’t encountered this issue myself. just to clarify, does the .log file says `number of threads in use = 1`?\n",
      "```\n",
      "╰ ➤ cat test_.log | grep threads\n",
      "number of threads in use       = 32\n",
      "```\n",
      "except that the other threads aren't actually engaged in any work. if i `ps -t -p` with the pid of the ahf process, all the other threads have 0 cpu time\n",
      "ah, i have never done extra checking but to trust those numbers reported in the log file.\n",
      "sorry i don’t have enough knowledge to have useful comments on this. probably you could ask it in the google group.\n",
      ":smile:\n",
      "<@u014shc71a4> has joined the channel\n",
      "hey, i've been trying to access some files like this one\n",
      "<http://paste.yt-project.org/show/215/>\n",
      "\n",
      "but the links just time out. i've confirmed that it's not just me experiencing this.\n",
      "hey cj! there’s an issue on our end. we’re fixing it now.\n",
      "<@u014shc71a4> it's back up\n",
      "awesome, thanks so much!\n",
      "so it turns out that the critical density 3h²/8pig is in physical units, not comoving. that will be a factor (1+z)³ in overdensity and thus originally the mean densities were only off by a few percent, depending on redshift, after all:sweat_smile:\n",
      "<@ulrlwtjjc> has joined the channel\n",
      "hi, i am yuxuan. recently i don't know how to re-scale the simulation snapshot data to another size and density scale. this requires me to change the field value by a factor. is there some simple codes that could do this?\n",
      "is it possible to get a value range of variable? i want to match the value range for several plots. i can do this with `set_zlim` but i have to start with a kind of random guess for min and max.\n",
      "<@ulqt4pxbp> on a given data object, you can call `.min(field)` and `.max(field)`.  when it does one, it caches the results for both.\n",
      "<@u042hlt7u> so if i have a slice object `yt.visualization.plot_window.axisalignedsliceplot` as `slc`, i can get the dataset by `slc.ds` but `slc.ds.min('my_var')` causes `attributeerror: 'exodusiidataset' object has no attribute 'min'`\n",
      "<@ulqt4pxbp> from a slice object you can do `slc.data_source` to get the actual slice.  a data object is distinct from a data set in that the data object is a slice, a rectangular prism, a sphere, etc.\n",
      "`psi_m` is my variable name. `slc.data_source.max('psi_m')` ‘returns -1e+90 dimensionless’ whereas the data structure with which the slice object was created has the maximum value of 0.15. i used `ds.r['psi_m'].max()`\n",
      "<@ulqt4pxbp> grabbing `ds.r` is a good way to get it for the full source; i am not really sure why it's giving that value for the max on the slice!  that's quite strange to me, but probably specific to the unstructured mesh implementation.\n",
      "<@u042hlt7u> thank you. my original mesh is unstructured in 3d but all the elements are triangles. (the dimensionality of the yt dataset is 2) i checked with a 3d unstructured mesh and the min and max values are reasonable. the projection of triangles to slice could be doing something.\n",
      "<@u042hlt7u> i just got max or min values with `float(ds.r['psi_m'].max().d)` or `float(ds.r['psi_m'].min().d)`. the data on the slice is the same as original 2d data, so this just worked.\n",
      "<@ulqt4pxbp> oh, good.\n",
      "<@u042hlt7u> thank you for your help!\n",
      "when i have these two lines for a slice plot:\n",
      "```\n",
      "slc.save('plot_with_suffix', suffix='jpg')\n",
      "slc.save('plot_with_mpl_kwargs', mpl_kwargs={'format' : 'jpg'})\n",
      "```\n",
      ", both generate a png file instead of jpeg file. i believe matplotlib supports the jpeg format. how could i select the format correctly?\n",
      "so `unyt.latex_repr` is pretty awesome, but it only works on units. is there any interest in having such a thing for quantities? i'm imagining something that would make it really easy to write a python script that emits latex that i can just paste into a paper.\n",
      "<@ud4ecst8f> has joined the channel\n",
      "i am not having this issue with the fake data.\n",
      "i found that when i use `s1.set_log('velocity_1', false)` this issue arises.\n",
      "(also in the test case)\n",
      "<@uc7cqjwma> has joined the channel\n",
      "hi yt friends! i’m wondering if particleprojectionplot is supposed to consider periodic boundary conditions? and if there’s anything that needs to be configured to achieve that? i’m doing a naive experiment but failed:\n",
      "```\n",
      "import yt\n",
      "\n",
      "ds = yt.load('gizmo_64/output/snap_n64l16_093.hdf5')\n",
      "center = ds.domain_width * 0.618\n",
      "yt.particleprojectionplot(ds, 'z', 'particle_mass', center=center).save()\n",
      "```\n",
      "\n",
      "i don’t think it does\n",
      "i also don’t think it would be terribly hard to fix that\n",
      "<@u042fh0rb> thanks for the information. i’ll submit an enhancement proposal on yt then.\n",
      "the code to look at is the `particleimagebuffer` class in `yt/visualization/fixed_resolution.py`\n",
      "no need to submit a ytep, just open a pr\n",
      "sure\n",
      "i’d like to work on it but maybe two weeks later. gotta focus on some other things before that.\n",
      "no worries, any time :slightly_smiling_face:\n",
      "sorry it didn’t work for your use case out of the box :confused:\n",
      "hi, when i try to load the rasmes data file using ds = yt.load(“data”,fields=my_field) to give the variables correct names, yt interpret many variables as dimensionless quantities, it there a way for me to manually give then the right dimension while loading the data?\n",
      "no, not easily unfortunately, this is something that we’re planning to fix for yt 4.0 along with a significant reworking of the field system in general\n",
      "usually what i suggest as a workaround is to define alias fields, like this:\n",
      "```\n",
      "def density_alias(field, data):\n",
      "    return data.ds.arr(data['ramses', 'on_disk_field'], 'g/cm**3')\n",
      "\n",
      "ds.add_field(('gas', 'density_alias'), function=density_alias, units='g/cm**3',\n",
      "             sampling_type='cell')\n",
      "```\n",
      "you can put those in your plugin file so you don’t need to add them to every script:\n",
      "<http://yt-project.org/doc/reference/configuration.html#the-plugin-file>\n",
      "you can attach units to these\n",
      "i.e ([15, 15, 15], \"kpc\") but i presume they default to code_length\n",
      "i think you want (‘gas’, ’density), not ‘particle_density’\n",
      "is it ‘particle_density’ in the documentation?\n",
      "ah i think i interpreted the  ds.load_particle docmuentation poorly, i thought it suggests i should put 'particle_' before each field ```to hook up with yt's internal field system, the dictionary keys must be 'particle_position_x', 'particle_position_y', 'particle_position_z', and 'particle_mass', as well as any other particle field provided by one of the particle frontends.```\n",
      "ah i guess those docs need to be updated to have a discussion about sph data\n",
      "i think bili has an example you can look at in a pr he’s working on:\n",
      "<https://github.com/yt-project/yt/pull/2186>\n",
      "i think you need particle_mass, particle_position, and smoothing_length, and yt should be able to generate a density field\n",
      "bili’s pr might be exactly what you’re looking for if you don’t have smoothing lengths already\n",
      "anyway, sorry this is a little confusing, this is what happens when lots of people start using beta level software :confused:\n",
      "reports and fixes are greatly appreciated, we’re still not really ready to do an official yt 4.0 release and beta testing is very useful, but only if we get feedback about what’s broken\n",
      "<@u9ce5d9lz> how do i include units? when i try `arbitrary_grid = ds.arbitrary_grid(([0.0, 0.0, 0.0], 'kpc'), ([0.99, 0.99, 0.99], 'kpc') , dims=[128, 128, 128])` i get `runtimeerror: length of edges must match the dimensionality of the dataset`\n",
      "hm, i thought that would work. maybe <@u042fh0rb>  knows!\n",
      "no worries on the confusion, just really appreciate this slack channel exists\n",
      "i think you need to do this:\n",
      "you can do `ds.arbitrary_grid(ds.arr([0.0, 0.0, 0.0], 'kpc'), ds.arr([0.99, 0.99, 0.99], 'kpc') , dims=[128, 128, 128])`\n",
      "```\n",
      "from yt.units import kpc\n",
      "left_edge = [0, 0, 0]*kpc\n",
      "right_edge = [.99, .99, .99]*kpc\n",
      "ag = ds.arbitrary_grid(left_edge, right_edge, dims=[128]*3)\n",
      "```\n",
      "not everything needs to be a one-liner :slightly_smiling_face:\n",
      "i’m making a pr to add ds.units too, along with a bunch of other goodies\n",
      "<https://github.com/yt-project/yt/pull/2219>\n",
      "the shape of the density grid is still confusing. following <@u042fh0rb>'s example for generating the arbitrary grid, then `ag_density = ag[(field, 'density')]` and printing the shapes of each\n",
      "```arbitrary grid shape:  (128, 128, 128)\n",
      "density grid shape:  (135,) ```\n",
      "what is the `field`?\n",
      "changing the `field` from `gas` to `io` made it so yt could calculated the `density`\n",
      "i see, what frontend is this?\n",
      "following the bili's example notebook\n",
      "can you share your code?\n",
      "what do you mean by frontend?\n",
      "they’re using load_particles\n",
      "if you make a short runnable example one of us could run, that would help\n",
      "that way one of us can run it locally and tell you what exactly is going wrong\n",
      "you can make rake random data using e.g. np.random\n",
      "i think what matters here is the field names you’re using\n",
      "you need to use a very specific set of field names right now to trigger what you want to happen in yt’s field system\n",
      "<https://github.com/andersdot/dust/blob/master/domainsimulation_test2.ipynb>\n",
      "so this is a really huge example\n",
      "can you make it more minimal?\n",
      "please don't judge :innocent:\n",
      "and only use yt?\n",
      "oh sure, give me a bit, i'm a bit slow\n",
      "let me take a look at your full code and see if i can understand, but in general it’s much better to share what stackoverflow calls an mcve, <https://stackoverflow.com/help/mcve>\n",
      "often in the process of generating an mcve you figure out what’s going wrong\n",
      "i think you might be hitting a bug though? i don’t see anything obviously wrong with your code\n",
      "if you can make an mcve and file an issue on github one of us can take a look, that will also allow us to escape this slack thread\n",
      "i’m going to bike into work\n",
      "looks like its an issue with the cluster not loading the correct version of yt :woman-facepalming:\n",
      "ah, that behavior would make sense on an old yt version, glad to hear it got sorted :slightly_smiling_face:\n",
      "for some codes [here i'm looking at a gadget 3 code] the unit \"unitary\" doesn't seem to be doing what i expect it to do. (`start, end`) are points in r3 which are the start and end of a trident sightline.\n",
      "```print start\n",
      "print end\n",
      "print start.in_units(\"kpc\")\n",
      "print end.in_units(\"kpc\")\n",
      "print start.in_units(\"unitary\")\n",
      "print end.in_units(\"unitary\")\n",
      "print start.in_units(\"code_length\")\n",
      "print end.in_units(\"code_length\")\n",
      "\n",
      "[  29.95018959   30.56563377 <tel:4719.1440201|4719.1440201> ] kpccm/h\n",
      "[ -396.61849768  -703.58215356 -4620.71636703] kpccm/h\n",
      "[  3.83224448   3.91099298 603.83302643] kpc\n",
      "[ -50.74889573  -90.02610205 -591.23882134] kpc\n",
      "[ 0.49916983  0.50942723 78.65240033] unitary\n",
      "[ -6.61030829 -11.72636923 -77.01193945] unitary\n",
      "[  29.95018959   30.56563377 <tel:4719.1440201|4719.1440201> ] code_length\n",
      "[ -396.61849768  -703.58215356 -4620.71636703] code_length```\n",
      "\n",
      "then when it goes into trident, it has a `runtimeerror: need to fix case of ray extending beyond edge of domain,` which i think basically comes from `unitary` not being in between 0 and 1 as expected. is there a way to change that in place while running (redefine `unitary`) or where can i go to fix it in the source code?\n",
      "\n",
      "thanks!\n",
      "“unitary” gets defined here: <https://github.com/yt-project/yt/blob/master/yt/data_objects/static_output.py#l1084>\n",
      "it should be equivalent to `ds.domain_width.max()`\n",
      "i.e. 1 in unitary units is the size of the longest edge of the domain\n",
      "for a dataset where this is happening, what is `ds.quan(1, 'unitary').to('kpc')`?\n",
      "and what’s `<http://ds.domain_width.to|ds.domain_width.to>('kpc')`?\n",
      "```print ds.quan(1,'unitary').to('kpc')\n",
      "print <http://ds.domain_width.to|ds.domain_width.to>('kpc')\n",
      "\n",
      "7.677235835829029 kpc\n",
      "[7.67723584 7.67723584 7.67723584] kpc```\n",
      "do you think it has something to do with being comoving?\n",
      "i mean, is that the domain width you’d expect?\n",
      "i don’t know anything about this data\n",
      "no that seems way too small for this\n",
      "is this dataset from a really high redshift?\n",
      "this is at z=10\n",
      "ah then i think that makes more sense, particularly for a zoom-in\n",
      "where are `start` and `end` coming from?\n",
      "because it seems like you’re using a `start` and `end` that are outside your domain?\n",
      "which trident is complaining about\n",
      "that is possible, though i feel like the box itself should be big enough to handle it. is yt just using the farthest sph particles to determine the boundary of the simulation? i assumed it was using the whole box\n",
      "it’s coming from the header for gadget cosmology simulations\n",
      "the boxsize parameter\n",
      "<https://github.com/yt-project/yt/blob/master/yt/frontends/gadget/data_structures.py#l310>\n",
      "ok, i'll look into that and where it defines unitary. maybe i'll need to keep my sightlines smaller until we get into more \"galaxy\" level redshifts. thanks nathan!\n",
      "<@u7ku54sg5> you could always create your own bounding box for the data when you read it in, instead of relying on yt to figure it out.  that way you could arbitrarily specify a really large bounding box and not run into this issue of having start/endpoints for the trident ray outside of the domain.\n",
      "or choose a `start` and `end` that’s always inside your box?\n",
      "e.g. specify it in comoving coordinates?\n",
      "<http://yt-project.org/docs/dev/examining/loading_data.html#units-and-bounding-boxes>\n",
      "also if yt is generating a bounding box that doesn’t enclose all your data that sounds like a bug to me\n",
      "on either the yt or gadget side\n",
      "well, it could just be that <@u7ku54sg5> is specifying some arbitrary points that are outside of the domain? i dunno.\n",
      "yeah, i don’t think they ever said\n",
      "i asked a couple times where start and end were coming from\n",
      "yeah they were just randomly generated. it does look like it works if you restrict it sufficiently. this is what it looks like. i think clearly a trident sightline outside this box would be kind of a waste anyways, so i'll probably try harder to determine a realistic impact parameter instead of just redefining the box. that's a good suggestion though <@u042j5bn6>\n",
      "what is the goal, clayton?  is this for agora?  is it just to get a bunch of random sightlines at different impact parameters from the center of the galaxy?\n",
      "<@usmvd0udq> has joined the channel\n",
      "<@ut20cs337> has joined the channel\n",
      "hi, i wanted to add a field to yt to calculate a vector cross product, but yt does not recognize the vector as an array with shape of (3, ), instead, it reads it as a float value, how can i fix it? here is the code with a yt test dataset\n",
      "\n",
      "\n",
      "when yt does a projection in say the z direction and the code coordinates go from 0 to 1, does the projection go from 0 in the front to 1 in the back or 1 in the front and 0 in the back?\n",
      "i think it does it in i/o order, so neither\n",
      "in what context would it matter?\n",
      "<@uggk0erpa> has joined the channel\n",
      "i'm having a bit of trouble trying to get the halo catalog to work with my data, for some bizarre reason. is this the right place to ask about this?\n",
      "sure, ask away\n",
      "great, thanks! i ran ahf on my data and have been excited to use yt's built in containers for halos, since i use yt for my gizmo runs. i wanted to test everything out so i did exactly what is shown here in the only cell under that heading: <http://yt-project.org/doc/examining/loading_data.html#amiga-halo-finder>\n",
      "\n",
      "i can successfully print the particle_mass and virial_radius of the halos. however, when i try to do more complex analysis, such as combining the dataset and halos dataset, to create a halo catalog, such as here: <http://yt-project.org/doc/analyzing/analysis_modules/halo_catalogs.html#extra-halo-analysis>\n",
      "\n",
      "i get the first problem, with a warning when i call halocatalog(data_ds = data_ds, halos_ds = halos_ds):\n",
      "\n",
      "yt : [warning  ] 2019-02-26 09:30:10,474 halo dataset test-v010.snapshot_296.hdf5.parameter has no field ('halos', 'particle_identifier').\n",
      "\n",
      "this happens for many of the fields, i think all of them. however, as i said above, i can print out halos_ad[('halos', 'particle_identifier')] and see it fine (where halos_ad = halos_ds.all_data()).\n",
      "\n",
      "does ahf work with the halocatalog module?\n",
      "i have sample data, i could upload it to dropbox if necessary for reproducibility\n",
      "i should also add that if i call hc.create(), it throws an exception that those fields do not exist, and does not continue.\n",
      "so i don’t know much about the halo catalog machinery (<@u042s6y2g> may know more) but the issue seems to be that the catalog generated by ahf doesn’t have ids? or yt isn’t reading them?\n",
      "it's actually all of the fields, here is the full warning:\n",
      "\n",
      "yt : [warning  ] 2019-02-26 09:30:10,474 halo dataset test-v010.snapshot_296.hdf5.parameter has no field ('halos', 'particle_identifier').\n",
      "yt : [warning  ] 2019-02-26 09:30:10,494 halo dataset test-v010.snapshot_296.hdf5.parameter has no field ('halos', 'particle_mass').\n",
      "yt : [warning  ] 2019-02-26 09:30:10,494 halo dataset test-v010.snapshot_296.hdf5.parameter has no field ('halos', 'particle_position_x').\n",
      "yt : [warning  ] 2019-02-26 09:30:10,494 halo dataset test-v010.snapshot_296.hdf5.parameter has no field ('halos', 'particle_position_y').\n",
      "yt : [warning  ] 2019-02-26 09:30:10,494 halo dataset test-v010.snapshot_296.hdf5.parameter has no field ('halos', 'particle_position_z').\n",
      "yt : [warning  ] 2019-02-26 09:30:10,495 halo dataset test-v010.snapshot_296.hdf5.parameter has no field ('halos', 'virial_radius')\n",
      "\n",
      "but as i said, if i directly access the halos_ds.all_data(), i can output them.\n",
      "also <@u0860sxlk> added the ahf frontend so he might know more as well\n",
      "maybe you could pastebin the full output of what you’re seeing\n",
      "along with the script you’re running\n",
      "sure, i can do that\n",
      "<https://pastebin.com/mwuanym3>\n",
      "\n",
      "i should note i'm using python 3.7.1 and yt 3.5.0, maybe 3.5 is the issue and i should revert back to 3.4.1.\n",
      "i also tried python 3.7.1 with yt 3.5.1 and the yt-astrophysical analysis package (1.1-dev), and got the exact same errors as above.\n",
      "can you also pastebin the output of `print(halos_ds.field_list)`?\n",
      "britton and bili are on the west coast so they might not respond for a little while\n",
      "<https://pastebin.com/uxyuvjpn>\n",
      "so indeed, it is missing those fields\n",
      "or i guess it might be in derived_field_list\n",
      "yes it is in there\n",
      "yeah, i dunno offhand if this is an issue with the frontend or is particular to the dataset you’re looking at\n",
      "it looks like we do have an ahf dataset on <http://yt-project.org/data|yt-project.org/data>\n",
      "does this work for that dataset?\n",
      "<http://yt-project.org/data/ahf_halos.tar.gz> which was generated using <http://yt-project.org/data/gizmo_64.tar.gz>\n",
      "thanks, i was wondering about the sim data\n",
      "i'll try it out now\n",
      "hi <@uggk0erpa>, i haven’t had time to read through this yet and i’ve got a pretty busy day ahead of me here, but i’ll try to have a look at this at some point soon.\n",
      "thank you very much!\n",
      "the sample data for ahf halos+gizmo dataset gives the exact same error!\n",
      "ah great\n",
      "can you share your script using those datasets?\n",
      "just want to make sure i’m doing exactly what you’re doing\n",
      "<https://pastebin.com/byjqh75y>\n",
      "i used the astro_analysis package\n",
      "but the same thing happens with with the other package that is now deprecated (or future-deprecated?)\n",
      "yt.analysis_modules is deprecated, yes\n",
      "thanks for the report, i’m trying to reproduce now\n",
      "i believe the appropriate portmanteau is fuprecated\n",
      "ok, here’s the test script:\n",
      "```\n",
      "import yt\n",
      "from yt.extensions.astro_analysis.halo_analysis.api import halocatalog\n",
      "halos_ds = yt.load('ahf_halos/snap_n64l16_135.parameter', hubble_constant=0.7)\n",
      "data_ds = yt.load('gizmo_64/output/snap_n64l16_135.hdf5')\n",
      "\n",
      "hc = halocatalog(data_ds=data_ds, halos_ds=halos_ds)\n",
      "\n",
      "hc.create()\n",
      "```\n",
      "and i can reproduce\n",
      "thanks for looking into this\n",
      "lol\n",
      "this is an easy fix\n",
      "pr incoming\n",
      "is it not looking in derived_field_list?\n",
      "nope\n",
      "i’ll add a test too\n",
      "great!\n",
      "hmm actually no test, this is hard to test from the yt_astro_analysis side with real data\n",
      "<https://github.com/yt-project/yt_astro_analysis/pull/43>\n",
      "easiest way to get that is probably to install yt_astro_analysis from source and make the change yourself\n",
      "e.g. clone the git repo and do “pip install -e .” inside\n",
      "you could probably also edit the version of yt_astro_analysis you have installed too but that’s way hackier :slightly_smiling_face:\n",
      "yeah i was going to do the dirty trick\n",
      "go right into site-packages and then forget about it in the future\n",
      "seems to work great, got a halo mass function pretty quickly\n",
      "i get the following error when running the above code, i guess this is another bug report:\n",
      "\n",
      "<https://pastebin.com/rqkehdps>\n",
      "i think you might have gotten the paths backwards?\n",
      "oops, i copied the code incorrectly, i used the correct paths in my code :flushed:\n",
      "i pasted my paths incorrectly into the example on the documentation, but if you swap them you will get the error\n",
      "ah, this is a yt bug for sure\n",
      "<@uggk0erpa> <https://github.com/yt-project/yt/pull/2159>\n",
      "great, thanks! i'll integrate this into mine :slightly_smiling_face:\n",
      "i'm probably going to go through most of these examples for the next hour and see if everything works, is it okay if i keep pushing bug reports in here?\n",
      "sure\n",
      "<@ua5gp3nbh> has joined the channel\n",
      "how do i create a single float with units kpccm?\n",
      "`ds.quan(3.5, 'kpccm')`\n",
      "i want to add `ds.units` so you can do `3.5*ds.units.kpccm`, or something like that :slightly_smiling_face:\n",
      "hm; why does it require that i have a dataset loaded?\n",
      "because it depends on the redshift\n",
      "there's no constructor for a `kpccm` or `yt.units` type that takes a redshift that i can call?\n",
      "the requirement for a dataset is all fine and good for what i'm doing _at the moment_ but i'm curious\n",
      "right, it all gets set up inside the `dataset` class itself, let me point you at the relevant bit of code\n",
      "<https://github.com/yt-project/yt/blob/master/yt/data_objects/static_output.py#l979-l988>\n",
      "this is also how e.g. `code_length` works\n",
      "hi all, i would like to modify the axis center and the scaling of the axis in the follwing example using an frb: <http://yt-project.org/doc/visualizing/manual_plotting.html>. what are the commands that can do that? thanks!\n",
      "also, is it possible to add an 'integrate' obtion to\n",
      "proj = ds.proj('density', 0)\n",
      "sorry, can you explain a little bit more what you mean by “axis center” and “scaling of the axis”?\n",
      "yes, add `method='integrate'` to the call to `ds.proj`\n",
      "i think i found the answer to my first question here: <https://stackoverflow.com/questions/18696122/change-values-on-matplotlib-imshow-graph-axis>, let me try this and i will get back. thanks\n",
      "<@u042fh0rb> i think the integrate method is default, no?\n",
      "yup, it is\n",
      "the \"extent\" argument does what i want with regards to adjusting the axis limits and adjusting the axis orgin to the right numbers and location without messing with the aspect ratio\n",
      "so that's good\n",
      ":+1:\n",
      "since extend only modifies xmin and xmax, ymin and ymax and does some autoscaling, what would be the correct way to modify the x and y ticks as well as the x and y labels? thnks\n",
      "something like this: <https://stackoverflow.com/questions/9382664/python-matplotlib-imshow-custom-tickmarks> ?\n",
      "or <https://stackoverflow.com/questions/36437584/how-to-set-xticks-and-yticks-with-my-imshow-plot>\n",
      "i used a linspace previously to define the ticks - it did not work -but a list works\n",
      "arange works also, but i have bad experience with arange\n",
      "do you know why linspace does not work?\n",
      "not off-hand, no\n",
      "i’d need to see an example triggering the issue and poke at it locally probably\n",
      "ok, makes sense. understanding this issue is not my highest priority at the moment ... since this works, i need to move on to other things and come back to this later. thank you though.\n",
      "hey all - i'm trying to make a proj plot in yt 3.x and change the label on the colorbar:\n",
      "\n",
      "```\n",
      "plot = px.plots([('deposit','parttype0_smoothed_density')])\n",
      "plot.cax.yaxis.set_label_text(\"n$_h$ g cm$^{-2}$\")\n",
      "px.save('proj.x.png')\n",
      "```\n",
      "\n",
      "and get\n",
      "\n",
      "```\n",
      "traceback (most recent call last):\n",
      "  file \"projection_octree.py\", line 20, in &lt;module&gt;\n",
      "    plot = px.plots([('deposit','parttype0_smoothed_density')])\n",
      "```\n",
      "\n",
      "\n",
      "i looked around the docs and didn't see how to do this -- but maybe i missed it?\n",
      "do you get more to that error?\n",
      "px.plots is a dict but you’re accessing it like a function?\n",
      "is that the problem?\n",
      "the entire traceback is:\n",
      "\n",
      "```\n",
      "[desika.narayanan@login4 attenuation_laws]$ more proj.e\n",
      "/home/desika.narayanan/miniconda3/envs/mypy2env/lib/python2.7/site-packages/h5py/__init__.py:36: futurewarning\n",
      ": conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. in future, it w\n",
      "ill be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "yt : [info     ] 2018-09-11 08:55:22,342 calculating time from 9.756e-01 to be 4.263e+17 seconds\n",
      "yt : [info     ] 2018-09-11 08:55:22,343 assuming length units are in kpc/h (comoving)\n",
      "yt : [info     ] 2018-09-11 08:55:22,358 parameters: current_time              = 4.263438511011124e+17 s\n",
      "yt : [info     ] 2018-09-11 08:55:22,358 parameters: domain_dimensions         = [4 4 4]\n",
      "yt : [info     ] 2018-09-11 08:55:22,359 parameters: domain_left_edge          = [0. 0. 0.]\n",
      "yt : [info     ] 2018-09-11 08:55:22,359 parameters: domain_right_edge         = [50000. 50000. 50000.]\n",
      "yt : [info     ] 2018-09-11 08:55:22,359 parameters: cosmological_simulation   = 1\n",
      "yt : [info     ] 2018-09-11 08:55:22,359 parameters: current_redshift          = 0.02499995903287333\n",
      "yt : [info     ] 2018-09-11 08:55:22,359 parameters: omega_lambda              = 0.7\n",
      "yt : [info     ] 2018-09-11 08:55:22,359 parameters: omega_matter              = 0.3\n",
      "yt : [info     ] 2018-09-11 08:55:22,359 parameters: hubble_constant           = 0.68\n",
      "yt : [info     ] 2018-09-11 08:55:22,362 allocating for 1.551e+08 particles (index particle type 'all')\n",
      "yt : [info     ] 2018-09-11 08:55:45,393 identified 2.181e+07 octs\n",
      "yt : [info     ] 2018-09-11 13:03:57,411 max value is 5.20419e-20 at 28073.1499195098876953 22261.768579483032\n",
      "2266 28530.7705402374267578\n",
      "yt : [info     ] 2018-09-11 17:14:35,282 projection completed\n",
      "yt : [info     ] 2018-09-11 17:14:35,299 xlim = 22181.340925 22320.740919\n",
      "yt : [info     ] 2018-09-11 17:14:35,299 ylim = 28461.739948 28601.139942\n",
      "yt : [info     ] 2018-09-11 17:14:35,300 xlim = 22181.340925 22320.740919\n",
      "yt : [info     ] 2018-09-11 17:14:35,300 ylim = 28461.739948 28601.139942\n",
      "yt : [info     ] 2018-09-11 17:14:35,307 making a fixed resolution buffer of (('deposit', 'parttype0_smoothed_\n",
      "density')) 800 by 800\n",
      "traceback (most recent call last):\n",
      "  file \"projection_octree.py\", line 20, in &lt;module&gt;\n",
      "    plot = px.plots([('deposit','parttype0_smoothed_density')])\n",
      "typeerror: 'plotdictionary' object is not callable\n",
      "```\n",
      "ah sorry - i didnt' realize i hadn't pasted the full traceback before (posted before i had my coffee)\n",
      "ah so should it be:\n",
      "\n",
      "```\n",
      "px.plots[('deposit','parttype0_smoothed_density')]\n",
      "```\n",
      "(removing one set of ```( ) ```\n",
      "yup!\n",
      "(that’s what the error message at the bottom says)\n",
      "is there a way to control the min/max of the color scale in a projectionplot? i have a series of plots and i want the colors to always correspond to the same values instead of shifting around slightly between plots\n",
      "the `set_zlim` callback does the job. here’s an example: <https://yt-project.org/docs/dev/visualizing/plots.html#colormaps> (need to scroll down to the last one in this section)\n",
      "hey! i'm trying to make phase plots using a yt covering grid, but it fails with error `ytdataselectornotimplemented: data selector 'covering_grid' not implemented.`. i assume i have to do it by hand?\n",
      "is this with yt-4.0?\n",
      "no, that's 3.5+amr dataset\n",
      "as the error message says, that’s not implemented, why not use a region data object with the same bounds as the covering grid?\n",
      "in principle it could work in a manner analogous to the cut region selector, but it would need to be wired up\n",
      "probably simpler actually because you could just use the selector of the region that covers the same volume\n",
      "the issue is that i need the velocity divergence, which is not implemented for amr\n",
      "ah i see\n",
      "it would work if you used `save_as_dataset` i think\n",
      "on the covering grid\n",
      "and reloaded the data as a dataset\n",
      "ok\n",
      "i actually got around the issue using mesh-on-particle deposition (similar to <https://github.com/yt-project/yt/pull/1994>)\n",
      "btw, i haven’t forgotten about your pr\n",
      "i’ve been focussing on getting 3.5 out the dor\n",
      "i'm trying to save a vr image (demesh yt):\n",
      "\n",
      "```\n",
      "sc = yt.create_scene(ds2)\n",
      "sc.camera.zoom(2)\n",
      "sc.save(dpi=300)\n",
      "```\n",
      "\n",
      "but the code doesn't like the dpi=300.  is there an obvious way to increase the resolution?\n",
      "(i'm trying to make an image for a lorentz workshop poster)\n",
      "<@u37dtbl6n> that particle filter code you suggested; which version of yt are you running it on?\n",
      "<@u046k2qnk> `sc.camera.resolution = (1600, 1600)`\n",
      "there isn’t a concept of a dpi in the volume rendering machinery\n",
      "ah!  thanks :)\n",
      "fwiw the vr machinery is super nice :)\n",
      "hi nathan,\n",
      "can you give me any guidance on how to add change #1850 to my yt?\n",
      "i don't think i have a local copy, i just merge from github.\n",
      "<@u91855pa9> i'm on the dev version of yt, but this _should_ work on 3.4\n",
      "<@ud6cg0fl4> i don’t understand what your last message means\n",
      "how did you install yt? from a checkout of the repo?\n",
      "#1850 is merged so you just need to pull on the master branch\n",
      "sure don't worry\n",
      "in the meantime i'm using a slower yet working version\n",
      "<@u042fh0rb> hm.  okay.  then i guess i don't understand your comment for issue #2048\n",
      "<https://github.com/yt-project/yt/issues/2048>\n",
      "what about it?\n",
      "i just ran the test script from the issue on the current master branch and didn’t see the behavior kacper reported\n",
      "so i concluded it’s fixed\n",
      "probably by merging #1850\n",
      "is \"current master branch\" = yt 3.5.dev0\n",
      "maybe my problem is that i am not on the stable yt?\n",
      "no, it’s not the stable version\n",
      "it’s the development version\n",
      "the version number we give you (3.5.dev0) on the development version doesn’t change, so it could be the right version\n",
      "it might also not be\n",
      "the only want to check is to go to your clone of the repo\n",
      "do “git checkout master”\n",
      "“git pull origin master”\n",
      "then “pip install -e .”\n",
      "that will ensure you’re on the latest version\n",
      "and this should not wreck the clump stuff britton fixed because it is on the master?\n",
      "britton’s pr is also merged\n",
      "i don't need your fix at the moment, i did a work-around.\n",
      "ok!\n",
      "when i typed git checkout master i got told:\n",
      "already on 'master'\n",
      "your branch is ahead of 'origin/master' by 3 commits\n",
      "should i still go for git pull origin master, etc?\n",
      "(sorry, that was confusing, by \"your fix\" i meant the clump fix about the info_items.  i am still having cut_region problems.)\n",
      "(the ones reported in 2048)\n",
      "ugh, sorry, have to parent, will try and come back to bashing my head against this tonight!  thanks, as always, for all your help!\n",
      "<@ud9l1d44t> has joined the channel\n",
      "yes, you need to pull and then recompile\n",
      "<@u042fh0rb> just to follow-up and make sure i get everything, the command\n",
      "\"pip install -e .\"\n",
      "will recompile yt?\n",
      "it worked!!!  thanks <@u042fh0rb>!\n",
      "<@u012usm2k16> has joined the channel\n",
      "(sorta) quick question: how is `\"relative_velocity_x\"` defined in yt ? i think it’s something like `velocity_x - bulk_velocity_x` , but i’m not sure where/when this bulk velocity is defined and how.\n",
      "<@uf1uc363z> has joined the channel\n",
      "i see there's a note about amiga support in the 3.4 release notes, where's the documentation for this feature?\n",
      "<http://yt-project.org/docs/dev/examining/loading_data.html#ahf>\n",
      "does yt not have a traditional searchable api reference?\n",
      "i instinctively look for it\n",
      "it does but that won’t help you here\n",
      "here’s yt’s api reference\n",
      "<http://yt-project.org/docs/dev/reference/api/api.html>\n",
      "because `yt.load` has a dynamic return type?\n",
      "yeah, although i guess we should probably include the ahfdataset class in the api docs\n",
      "i guess we missed that when we added the frontend\n",
      "the things that show up in the api doc search need to be added manually\n",
      "that's awkward\n",
      "here’s the pr that added the ahf frontend\n",
      "<https://github.com/yt-project/yt/pull/1477/files>\n",
      "<@u91855pa9> do you have a github handle?\n",
      "saethlin\n",
      "just opened a pr, thanks for reporting the issue\n",
      ":ok_hand:\n",
      "is there an nchilada front end to yt?\n",
      "also i'm not sure if its grammatically correct to say 'an nchilada' or 'a nchilada'\n",
      "i just issued a pr for this <https://github.com/yt-project/yt/pull/1993>, i hope it doesn't break anything\n",
      "<@ufjkypt0v> has joined the channel\n",
      "<@u010dh0dw1x> has joined the channel\n",
      "<@utulqhwke> has joined the channel\n",
      "<@u010qgdbuv6> has joined the channel\n",
      "btw, is there something fancy to do to install the yt-4.0 branch? i'm struggling to `pip install` it\n",
      "yup\n",
      "<https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb>\n",
      "courtesy of <@u042j5bn6>\n",
      "there is also a <#c046hvb59|particles> channel which is generally used to discuss yt-4.0 stuff\n",
      "yeah, sorry that’s hard right now, i really need to do the packaging for cykdtree properly\n",
      "that’s the big pain point atm\n",
      "or at least make the error nicer\n",
      "isn't it available on pip?\n",
      "not the version that we need\n",
      "can you do `pip install -e git+<https://github.com/cykdtree/cykdtree@a939f2e746b80fbb8e4b3c03717d7d3f91a760fd#egg=cykdtree>`?\n",
      "is that easier to include in the instructions, maybe?\n",
      "i’d prefer people have the repo checked out, but yeah that would work too\n",
      "just on general principle :slightly_smiling_face:\n",
      "i like setting people up to be able to hack on code if they’d like\n",
      "good point\n",
      "<@u9ce5d9lz> <@u042fh0rb> how do you manage to switch between different version of yt?\n",
      "right now i'm doing checkout then reinstall, but that's cumbersome when jumping from e.g. yt 3.5 and yt 4.\n",
      "i have two repos cloned\n",
      "same, otherwise the build takes *ages*\n",
      "yeah, there’s a lot of differences in c code\n",
      "i think <@u042hlt7u> has some bash shell helpers that he uses to switch back and forth quickly\n",
      "i just manually do “pip uninstall yt”\n",
      "then “pip install -e .” in whichever one i want to activate\n",
      "i just do \"python3 setup.py develop\" in the repo i want to activate\n",
      "<@u042fh0rb> yeah, i have these functions in my zshrc:\n",
      "```\n",
      "alias 'apy3'='source activate /home/mturk/conda-py3/'\n",
      "alias 'apy2'='source activate /home/mturk/conda-py2/'\n",
      "\n",
      "ppbr()\n",
      "{\n",
      "    pp=\"${home}/yt/$1/\"\n",
      "    if [ ! -d ${pp} ] &amp;&amp; echo \"${pp} doesn't exist\" &amp;&amp; return\n",
      "    export pythonpath=${pp}\n",
      "    #echo \"pythonpath=${pythonpath}\"\n",
      "    export ytpath=${pp}\n",
      "    export ytname=$1\n",
      "    setprompt\n",
      "}\n",
      "ppbr yt\n",
      "```\n",
      "i like that `apy2` alias - i'm gonna steal that!\n",
      "you should steal the `apy3` :stuck_out_tongue:\n",
      "cool! thanks.\n",
      "another question then: how do you do that on the yt-4.0 branch?\n",
      "this is specifically for sph data\n",
      "because there’s no global octree mesh anymore\n",
      "if you do `ad = ds.all_data(); ad['gas', 'density']` in the yt-4.0 branch for sph data, you get back data at the particle positions\n",
      "does that work if you do `yt.load_particles`?\n",
      "yup\n",
      "ah, yeah, not just sph data\n",
      "all the particle frontends\n",
      "although `('gas', 'density')` isn’t defined for a pure-particle dataset\n",
      "i'm confused\n",
      "you can load an sph dataset using `load_particles` if you pass in a `smoothing_length` field\n",
      "about what?\n",
      "(more details about all this in <https://ytep.readthedocs.io/en/latest/yteps/ytep-0032.html> as well)\n",
      "<@ugyk2pqac> has joined the channel\n",
      "<@umk6n8yfl> has joined the channel\n",
      "agreed! i am super-psyched too! :slightly_smiling_face:\n",
      "hi guys! is there a current work around for this issue? <https://github.com/yt-project/yt/issues/1212>\n",
      "i am getting the same error on an amrex dataset\n",
      "it also only happens when my datasets are too large.\n",
      "hey, so this should be fixed if you pull the latest updates from the yt-4.0 branch, let me know if it isn't :) \n",
      "<@u9ce5d9lz> the url at the bottom of the splash paper doesn't work anymore, is the original splash code out there somewhere? i think i found where the cython code that uses splash is in yt, but i'm just curious if i can read the original\n",
      "<http://users.monash.edu.au/~dprice/splash/>\n",
      "you are looking for splash source code?\n",
      "we don’t use splash in yt, fwiw\n",
      "we use some of the algorithms from splash\n",
      "i'm interested in learning about how my pretty plots are made. so i have this nice splash paper, but for my brain it's really helpful to see the code that implements these quations, and there doesn't seem to be a very clear mapping in the yt codebase. i'll check out that url\n",
      "yeah, that ^. i didn't take the equations from the splash source code, i got them from the paper. the splash source code isnt that readable. it is on github if you want to look at it or try it out \n",
      "<https://github.com/danieljprice/splash.git>\n",
      "yeah already got to the repo\n",
      "fortran!\n",
      "here’s the code: <https://github.com/yt-project/yt/blob/yt-4.0/yt/utilities/lib/pixelization_routines.pyx#l965>\n",
      "the yt implementation\n",
      "`itab` in that function is an instance of `sphkernelinterpolationtable`, which is where most of the complicated bits are isolated to\n",
      "we use a lookup table to evaluate the projected sph kernel as a function of the impact parameter\n",
      "so for each pixel that a particle could contribute to (by comparing the distance from the particle to the pixel under consideration) you loop over the pixels and calculate the projected sph kernel, and then add that particle’s contribution to the pixel\n",
      "then you loop over all the particles that might contribute to the image\n",
      "i really should write the paper! \n",
      "this is section 4.3.1 in the splash method paper\n",
      "we use a different definition of the smoothing kernel (following gadget) in yt than what dan price uses so there’s some factors of two that are different in the paper and the yt source\n",
      "<https://arxiv.org/abs/0709.0832>\n",
      "thanks for the directions\n",
      "i think the yt patch to fix the multi-particle datasets works, but yt still isn't cooperating with what i'm trying to accomplish\n",
      "if you can share more detail about what problems you’re having we might be able to advise\n",
      "i'm still triple-checking but here's the code i'm working with:\n",
      "\n",
      "the objective is to make a density plot that only includes particles that are part of a particular galaxy\n",
      "sure, i don’t see anything obviously wrong with that code, but i haven’t used caesar much\n",
      "what’s going wrong?\n",
      "output looks like this right now\n",
      "well that’s not right :slightly_smiling_face:\n",
      "is there any chance you can share those two data files so i can try to reproduce the issue?\n",
      "my extremely-naive projection code produces this\n",
      "gah\n",
      "if i had to guess you’re hitting a bug in yt that we should fix :slightly_smiling_face:\n",
      "actually instead of sharing, i'm going to try to reproduce on my end with the yt gizmo data\n",
      "it’s probably something like we’re not using the correct smoothing lengths\n",
      "ah yeah, that would work too :slightly_smiling_face:\n",
      "thanks in advance for the report, <http://github.com/yt-project/yt/issues/new>\n",
      "i have actually had some issues recently with particle filters and some yt-4.0 stuff \n",
      "my code was actually error-ing rather than producing strange output  \n",
      "i suspect there's at least one mistake on my end because i've gotten dubious output from yt-3.4 as well\n",
      "i’m not sure it would necessarily give you a sensible output in 3.4 either\n",
      "the bookmark i made for the yt data hub thing doesn't work, can somebody give me a link?\n",
      "<http://yt-project.org/data/gizmo_64.tar.gz> ?\n",
      "<http://hub.yt|hub.yt> is down atm because of a power outage this morning at uiuc\n",
      "that would explain a lot\n",
      "but <http://yt-project.org/data|yt-project.org/data> is hosted elsewhere\n",
      "i've got some stuff do just now, but i can try and see if i get weird stuff with a dataset i have locally in about 30 mins. hopefully we can get this sorted \n",
      "also, out of curiosity, does the projection work ok for just a gas density?\n",
      "if i don't attempt any filtering it looks perfect\n",
      "i bet it’s using the `('gas', 'smoothing_length')` values\n",
      "just a guess though :slightly_smiling_face:\n",
      "i wouldn't be surprised if we are using \"ds._sph_ptype\" to grab those ...\n",
      "which would be bad ...\n",
      "we are\n",
      "line 136 of off_axis_projection.py\n",
      "yeah, so right there we need to check if we’ve been given a sph particle type or a particle filter derived from one\n",
      "which may need some kind of flag somewhere?\n",
      "i don’t remember offhand if we already have a way of figuring that out\n",
      "i'm writing up an issue for this :)\n",
      "there is a check in that file, but clearly that check isn't working\n",
      "i'll try and debug that check, in principle, once that is fixed and we use the field[0] and not ptype, that *should* be good to go\n",
      "<https://github.com/yt-project/yt/issues/2066>\n",
      "i think i have a fix, but will let you know in 5 mins! (fingers crossed)\n",
      "that would be _awesome_\n",
      "seems to work locally\n",
      "i'll make a pr\n",
      "heres a link, this is the only change i've made\n",
      "<https://github.com/yt-project/yt/pull/2067/commits/751b2239ec4ef5495b966386ae4dec2da0c33dd6>\n",
      "feel free to try locally\n",
      "the issue was that the code was essentially hardcoded to grab a bunch of data using `ds._sph_ptype`as the particle type (which is 'parttype0' in this case) so it was bypassing the particle filter\n",
      "it works\n",
      "woo hoo, awesome :) let us know when you find more bugs!\n",
      "<@u91855pa9> yup, keep the bug reports coming!\n",
      "and if you see yt doing something weird there’s a strong possibility it’s yt’s fault, not yours\n",
      "awesome thread to follow!\n",
      "i'm not convinced the precise image values coming out when i do the density projection are correct. needs more checking, but this may not be totally over\n",
      "ok, fair enough, love to hear more :slightly_smiling_face:\n",
      "thanks for working with us\n",
      "opening an issue\n",
      "<https://github.com/yt-project/yt/issues/2068>\n",
      "smells like a normalization issue to me but i'm not sure where to hunt.\n",
      "try this `python -c \"import yt; print(yt.__version__)\"`\n",
      "hi <@u5et0rnke> , sorry i didn’t see this until now. glad you got it working and thanks for posting an example of what you did. i will be doing some runs on stampede2 in the near future.\n",
      "no, i meant doing something like this:\n",
      "```\n",
      "ts = yt.datasetseries([\"dd0030/dd0030\", \"dd0040/dd0040\"])\n",
      "```\n",
      "this will work for frontends for which the `simulation` functionality is not supported, which is most of them.\n",
      "<@ul4nvracw> has joined the channel\n",
      "hi,\n",
      "i am looking to better understand how profile plots work. specifically, when i use yt.create_profile, the total sum of all the y-values varies depending on the number of bins used. i was hoping that they would total to the actual total of that quantity. is there a way to make that happen, or an explanation for why it can't?\n",
      "thanks!\n",
      "<@ul4nvracw> i'm not sure i follow - can you explain more how you're comparing and what you expect?\n",
      "ah, i see! thanks!\n",
      "i have\n",
      "ad = ds.all_data()\n",
      "     profile = yt.create_profile(ad, [\"n_e\"],\n",
      "                                      fields=[\"y2\"],\n",
      "                                      n_bins=\n",
      "\n",
      "where n_e and y2 are derived fields and i vary n_bins.\n",
      "then i take\n",
      "sum(profile['y2'])\n",
      "i was hoping this sum would be equal to\n",
      "sum(ad['y2'])\n",
      "but it is not, and varies with the number of bins.\n",
      "i was hoping to clarify why this works this way. i originally noticed the problem when making two profile plots with different x terms but the same y term, and noticed the axis scales were different by a couple orders of magnitude.\n",
      "hope that is clearer. thank you!\n",
      "<@ul4nvracw> what do you specify the `weight_field` to be?\n",
      "i have not been actively specifying it\n",
      "<@ul4nvracw> so i think that you should try specifying it as `weight_field=none` to make sure it's defaulting to summation.\n",
      "that's what i was looking for! thanks so much!\n",
      "thank you, guys. this helps a lot.\n",
      "glad to help!\n",
      "this might be a naive question, but in the haloanalysis modules is it possibly to know the stellar mass within the halos? it seems that you can use the callback functionality to calculate gas properties, but not the stellar properties. for example, it would be great to calculate the galaxy stellar mass function. if anyone knows, thanks!\n",
      "hi doug, sure. you can make a callback to do particle properties as well.\n",
      "something like the following:\n",
      "```\n",
      "def stellar_mass(halo):\n",
      "    sphere = halo.data_object\n",
      "    return sphere['star', 'particle_mass'].sum()\n",
      "add_quantity('stellar_mass', stellar_mass)\n",
      "```\n",
      "this is assuming you had a sphere callback defined earlier in your halo pipeline. sou you might have something like this:\n",
      "```\n",
      "hc = halocatalog(...)\n",
      "hc.add_callback('sphere')\n",
      "hc.add_quantity('stellar_mass')\n",
      "```\n",
      "does that make sense?\n",
      "thank you britton, it does -- except for \"sphere = halo.data_object\"\n",
      "how does it know data_object is the sphere?\n",
      "ah right, great question. the `sphere` callback creates a sphere data container centered on the halo with the radius set to the virial radius. it attaches that sphere object to the halo object as `data_object`\n",
      "so if you add that `hc.add_callback('sphere')` first in your pipeline, that object will be available to you in your stellar_mass function\n",
      "fantastic! thanks so much. could i ask one more question? what if i wanted other quantities, such as those within r500/r2500, instead of just the virial radius? can i add more sphere callbacks?\n",
      "i suppose they would just have to go into separate halo catalogs\n",
      "with quantities based on r200/r500/r2500, etc\n",
      "yeah, totally. there is a “recipe” for calculating properties at different critical density values.\n",
      "<https://yt-astro-analysis.readthedocs.io/en/latest/halo_catalogs.html#recipes>\n",
      "you can then use those new values to alter the behavior of the sphere callback\n",
      "arg, looking for documentation. we really need to improve these docs (that’s on me)\n",
      "anyway, the virial_quantities recipe will give you new fields, like `radius500`, that you can then tell the ‘sphere’ callback to use\n",
      "fantastic! i have a problem though with the stellar mass callback, and it may be because i'm using yt_astro_analysis instead of the yt project haloanalysis module\n",
      "```python\n",
      "---------------------------------------------------------------------------\n",
      "typeerror                                 traceback (most recent call last)\n",
      "&lt;ipython-input-10-6b9262d90adc&gt; in &lt;module&gt;\n",
      "----&gt; 1 hc.create()\n",
      "\n",
      "/project/b/babul/rennehan/anaconda3/envs/pynbody/lib/python3.6/site-packages/yt_astro_analysis/halo_analysis/halo_catalog.py in create(self, save_halos, save_catalog, njobs, dynamic)\n",
      "    335 \n",
      "    336         \"\"\"\n",
      "--&gt; 337         self._run(save_halos, save_catalog, njobs=njobs, dynamic=dynamic)\n",
      "    338 \n",
      "    339     def load(self, save_halos=true, save_catalog=false, njobs=-1, dynamic=false):\n",
      "\n",
      "/project/b/babul/rennehan/anaconda3/envs/pynbody/lib/python3.6/site-packages/yt/utilities/parallel_tools/parallel_analysis_interface.py in barrierize(*args, **kwargs)\n",
      "    299     def barrierize(*args, **kwargs):\n",
      "    300         if not parallel_capable:\n",
      "--&gt; 301             return func(*args, **kwargs)\n",
      "    302         mylog.debug(\"entering barrier before %s\", func.__name__)\n",
      "    303         comm = _get_comm(args)\n",
      "\n",
      "/project/b/babul/rennehan/anaconda3/envs/pynbody/lib/python3.6/site-packages/yt_astro_analysis/halo_analysis/halo_catalog.py in _run(self, save_halos, save_catalog, njobs, dynamic)\n",
      "    451                           self.data_source[quantity][int(i)]\n",
      "    452                     elif callable(quantity):\n",
      "--&gt; 453                         new_halo.quantities[key] = quantity(new_halo)\n",
      "    454                 else:\n",
      "    455                     raise runtimeerror(\n",
      "\n",
      "/project/b/babul/rennehan/anaconda3/envs/pynbody/lib/python3.6/site-packages/yt_astro_analysis/halo_analysis/halo_quantities.py in __call__(self, halo)\n",
      "     37 \n",
      "     38     def __call__(self, halo):\n",
      "---&gt; 39         return self.function(halo, *self.args, **self.kwargs)\n",
      "     40 \n",
      "     41 def center_of_mass(halo):\n",
      "\n",
      "typeerror: stellar_mass() takes 1 positional argument but 2 were given\n",
      "```\n",
      "this happens when i run:\n",
      "\n",
      "`hc.create()`\n",
      "ah, here’s the sphere call back doc: <https://yt-astro-analysis.readthedocs.io/en/latest/generated/yt_astro_analysis.halo_analysis.halo_callbacks.halo_sphere.html#yt_astro_analysis.halo_analysis.halo_callbacks.halo_sphere>\n",
      "yt_astro_analysis is definitely the way to go here\n",
      "can you show me what your script looks like?\n",
      "sure\n",
      "`import yt\n",
      "\n",
      "ds = yt.load('%s/snapshot_189.hdf5' % data_dir)\n",
      "halos_ds = yt.load('%s/snapshot_189.hdf5.parameter' % data_dir)\n",
      "\n",
      "from yt_astro_analysis.halo_analysis.api import *\n",
      "hc = halocatalog(data_ds = ds, halos_ds = halos_ds)\n",
      "\n",
      "def stellar_mass(halo):\n",
      "    sphere = halo.data_object\n",
      "    return sphere['star', 'particle_mass'].sum()\n",
      "add_quantity('stellar_mass', stellar_mass)\n",
      "\n",
      "hc.add_callback('sphere', factor = 2.0)\n",
      "hc.add_quantity('stellar_mass', stellar_mass)\n",
      "\n",
      "hc.create()`\n",
      "ugh\n",
      "````import yt\n",
      "\n",
      "ds = yt.load('%s/snapshot_189.hdf5' % data_dir)\n",
      "halos_ds = yt.load('%s/snapshot_189.hdf5.parameter' % data_dir)\n",
      "\n",
      "from yt_astro_analysis.halo_analysis.api import *\n",
      "hc = halocatalog(data_ds = ds, halos_ds = halos_ds)\n",
      "\n",
      "def stellar_mass(halo):\n",
      "   sphere = halo.data_object\n",
      "   return sphere['star', 'particle_mass'].sum()\n",
      "add_quantity('stellar_mass', stellar_mass)\n",
      "\n",
      "hc.add_callback('sphere', factor = 2.0)\n",
      "hc.add_quantity('stellar_mass', stellar_mass)\n",
      "\n",
      "hc.create()```\n",
      "ah, this line: `hc.add_quantity('stellar_mass', stellar_mass)`\n",
      "should be: `hc.add_quantity('stellar_mass')`\n",
      "ah, thank you\n",
      "no prob\n",
      "the first `add_quantity` call is add it to a registry of known functions\n",
      "you’re saying, associate “stellar_mass” with the function `stellar_mass`. i could have made that more illustrative by naming the function with a different name than the string.\n",
      "in `hc.add_quantity`, your picking “stellar_mass” out of the registry that you added it to previously\n",
      "do all of the submodules know about the registry? or just the halocatalog module\n",
      "thanks for the explanation i appreciate it, the yt code is very abstract so it takes a while to get a grasp on what is going on under the hood\n",
      "the only thing that knows about the registry is the runtime environment of that script\n",
      "you could put the callback functions and `add_quantity` in another file and import that in your halo catalog script, and it would then be in there\n",
      "but this all all just halocatalog specific too\n",
      "i suppose this is because we imported * from the halo_analysis.api? maybe that's why i was a bit confused because it looked like a global function but i forgot about that import\n",
      "ah yeah, good point. it’s all coming from that import\n",
      "specifically, `add_quantity`, `add_callback`, `add_recipe`, those are all in that import\n",
      "and they “know” about the registry\n",
      "great, this is very well done. since you're here, i ran into a problem with the code again but i believe i fixed it. i'm using gizmo, and i don't have a ('star', 'particle_mass') field in my derived list. i replaced the `sphere['star', 'particle_mass']` with `sphere['parttype4', 'masses']`. this should work just as fine, no?\n",
      "it is definitely creating the catalog haha\n",
      "yeah, that should work. the `star` particle type is something that may or may not exist, but whatever your particle type for stars is, that’ll do\n",
      "great, this makes my life so much easier. i was running into huge problems with pynbody + gizmo + ahf, but yt seems to have it covered! i didn't know there was such a rich halo backend\n",
      "thank you!\n",
      "brilliant, glad this is working for you. spread the word!\n",
      "also, you aren’t the first person to ask these types of questions about the halocatalog. if you can think of ways to improve the docs, i would really love that.\n",
      "hmm, well i'll think about it a bit more. i just started with this this week so i'm going to read through the docs more carefully today. i'll definitely let you know, and i'll spread the word :slightly_smiling_face:\n",
      "fantastic. best of luck with the analysis!\n",
      "can i make a projectionplot for only the particles that correspond to the values in an array of particle ids? i've looked through the section on filtering and i might be able to make it work with `add_particle_filter` but that seems very roundabout\n",
      "yes, that would be the way to do it\n",
      "i’m not sure offhand if that actually works at the moment though in the demeshening\n",
      "you can create new particle types by creating a particle filter\n",
      "i just don’t know off hand if the filter is recognized as an sph particle type\n",
      "<@u91855pa9> i'm usually using this to filter particle based on id\n",
      "```\n",
      "from yt import particle_filter\n",
      "from numpy import in1d\n",
      "\n",
      "def add_filter_for_particle_ids(ds, selected_ids, new_particle_name='filtered_particle',\n",
      "                                filtered_type='io'):\n",
      "    @particle_filter(new_particle_name,\n",
      "                     requires=['particle_identity'],\n",
      "                     filtered_type=filtered_type)\n",
      "    def _filter(pfilter, data):\n",
      "        ids = data[(pfilter.filtered_type, 'particle_identity')]\\\n",
      "            .astype(int)\\\n",
      "            .value\n",
      "\n",
      "        ret = in1d(ids, selected_ids)\n",
      "        return ret\n",
      "    ds.add_particle_filter(new_particle_name)\n",
      "```\n",
      "interesting.\n",
      "hey, can you remind me how one can get transparent backgrounds for yt plots?\n",
      "or in general how to set the background color of the figure (i'm not talking about the matplotlib ax which can be set using `p.set_background_color`)\n",
      "i seem to remember that not being straightforward due to details of how matplotlib works\n",
      "let me see if i can make an example, one sec\n",
      "<@ud6cg0fl4> has joined the channel\n",
      "this seems to work:\n",
      "\n",
      "<@ud6cg0fl4>, i just update the pull request to remove those dead imports. it should work now\n",
      "okay, thanks!  i changed the import call to what you recommended.  will let you know if it runs!\n",
      "it is no longer failing, just not finding the clumps i am expecting, either.  now i need to figure out if the problem is me :slightly_smiling_face:\n",
      "anyone know how to find the commit number corresponding to a yt installation from inside the notebook?\n",
      "do “git status” inside of the repo?\n",
      "if there’s no repo we don’t encode that information in the installed copy\n",
      "i see, thanks\n",
      "it would be possible to do that via e.g. versioneer though\n",
      "<@u042hlt7u> did you ever get a moment to check this out?\n",
      "in trying to update yt to dev tip, i’m getting some breakages due to sympy not playing well with other packages.  is this a known issue and is there a known solution, or are we just to wipe and start over?  i saw this recently-merged pr, but wasn’t sure it was related: <https://github.com/yt-project/yt/pull/2407>\n",
      "rather, i’m trying to update yt to yt-4.0 tip.\n",
      "you might need to update unyt\n",
      "when i try to `pip install -u unyt`, i get the same error, unfortunately.\n",
      "what sympy and unyt versions do you have installed in that environment?\n",
      "and what errors are you seeing?\n",
      "if you can make a script i can run to reproduce it i can debug a little\n",
      "looks like sympy is 1.1.1, and i guess i don’t have unyt at all.\n",
      "that's odd, i don't think yt should be importable if unyt isn't installed\n",
      "well, i was on trail for the second half of 2019, before unyt was as integrated into yt as it is now, i think.\n",
      "in yt-4.0?\n",
      "i’m just  now trying to get up to snuff\n",
      "yt-4.0 depends on unyt now\n",
      "did it in july 2019?\n",
      "i don't remember when that got merged\n",
      "but i thought you said you were trying to update to the latest yt-4.0\n",
      "yes, i am trying to update my yt-4.0 from the state it was in circa july 2019 to the yt-4.0 tip.\n",
      "and i am unable to do so because of this sympy error.\n",
      "```installing collected packages: sympy, unyt\n",
      "  attempting uninstall: sympy\n",
      "    found existing installation: sympy 1.1.1\n",
      "error: cannot uninstall 'sympy'. it is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.```\n",
      "\n",
      "what pip version?\n",
      "pip 20.0.2 from /users/chummels/src/yt-conda/lib/python3.6/site-packages/pip (python 3.6)\n",
      "ah, you shouldn't be using pip in a conda environment like that\n",
      "`conda upgrade -c conda-forge sympy` should work\n",
      "you can get `unyt` from conda-forge too\n",
      "interesting.  i had thought that the appropriate way to install yt from the repo was to:\n",
      "```cd $yt_src\n",
      "git checkout yt-4.0\n",
      "git pull upstream yt-4.0\n",
      "pip install -e .```\n",
      "which is what i did, which led to this error.\n",
      "yeah, that should work in a conda environment as long as you have all of the depenencies updated first via conda\n",
      "i didn't realize that a conda-installed sympy would refuse to be updated with pip, that's new to me :slightly_smiling_face:\n",
      "thanks for the help with this, <@u042fh0rb>.\n",
      "i’m trying your conda upgrade command.\n",
      "actually i've always done that install of `yt` too (though, thanks to <@u042j5bn6>'s trident notebook instructions :slightly_smiling_face: )\n",
      "afaik that used to work too\n",
      "ok, so using your suggested command, it looks like i’m upgraded to sympy 1.5.1!  thanks\n",
      "so for getting unyt, should i use conda or pip? or just do the `pip install -e .?`\n",
      "`conda install -c conda-forge unyt`\n",
      "got it.\n",
      "i think pip would probably install it if you let it but it's best not to mix pip-installed and conda-installed stuff\n",
      "i need to run, hope you get sorted, if you run into new issues introduced in the last six months please let us know\n",
      "glad you're back from your big pct trip\n",
      "thanks for the help!\n",
      "i think once yt-4 merges, i/we may have to modify the install instructions to streamline them a bit, so others don’t get caught in the pip/conda limbo. :slightly_smiling_face:\n",
      "i want to make a lot of `offaxisprojectionplot`s, but for a small part of a large dataset. is there a way to slice the entire dataset?\n",
      "hi ben, i’m not sure if this is what you mean, but `offaxisprojectionplot` accepts a `data_source` keyword argument allowing you to do the projection on a subset of the full box\n",
      "where `data_source` is some data container, like a sphere or whatnot\n",
      "sounds like exactly what i want. didn't know about spheres and all that before, thanks!\n",
      "you bet!\n",
      "i think just about all of the plot functions accept this keyword\n",
      "hi <@uppavuhsm> !\n",
      "the warning you’re getting is only about conflicting options for drawing grid ids (unique numbers attached to each grid patch). it _is_ a somewhat minor issue with how certain default values are colliding, but it should not affect your plot.\n",
      "moreover, the issue you linked was recently fixed, so if it affects you, just update your yt clone !\n",
      "you can possibly reproduce a similar plot by making slices of the `('ramses', 'dx')` field + annotate the grid (see comments below)\n",
      "(also, i just opened a pr to fix that warning, so expect it to be fixed too soon !)\n",
      "<@u042hlt7u> thank you! my current version is 3.5.1 (i hope this is what you mean when you say yt clone) - does that need to be updated? i'm currently working on a cluster and so i don't have permissions to update anything myself, but will wait till monday incase that is necessary\n",
      "i was asssuming that you had your own git clone of yt, installed from source. however the 3.6 release is in the making and should be available in the coming days or weeks, so maybe wait for it before you ask your it department for an update :)\n",
      "sorry, what do you mean exactly by \"visualize the grid structure\"?\n",
      "like you want to make a plot of the amr grids in your simulation and then do a scatter plot on top of that?\n",
      "there isn't a plot type in yt that does that, you could probably get a sliceplot to look the way you want with a lot of effort\n",
      "it might be easier to just make the plot manually\n",
      "you can get access to the grid metadata for all of the grids in your simulation via `ds.index.grids`\n",
      "that's a python list of grid objects, you can get at the left and right edges for each grid like so:\n",
      "```\n",
      "for grid in ds.index.grids:\n",
      "   le = grid.leftedge\n",
      "   re = grid.rightedge\n",
      "```\n",
      "and then use the left edges and right edges to construct your plot in whatever plotting software you'd like to use\n",
      "<@u042fh0rb> yes, this is exactly what i want to do. i will give this a try. thanks a lot.\n",
      "if i have a storage dictionary doing iterations in parallel, is there a simple way to access all the `sto.result_id`’s in a similar manner to `storage.values()` to get all the `sto.result`’s?\n",
      "hi mike, the `result_ids` will be the keys of the `storage` dictionary\n",
      "excellent, thank you!!\n",
      "you’re welcome!\n",
      "has anyone run yt in parallel on an hpc system, specifically a dod machine? i am having trouble getting yt to recognize to use mpi4py or an error occurs when i load the module: `python: error while loading shared libraries: libimf.so: cannot open shared object file: no such file or directory`.\n",
      "i haven’t had this exact error, but i have had problems getting mpi4py to run on hpc systems like blue waters.\n",
      "do you know if there is a yt module that has been installed on the system?\n",
      "i.e., `module load yt`?\n",
      "would you mind sharing some of your tricks?\n",
      "if so, that is oftentimes a good place to look to see how mpi4py is installed for the module\n",
      "nope, but they do have `module load mpi4py`\n",
      "ok great!\n",
      "so load that and try to track down where it is installed on the system\n",
      "and look at their .cfg file\n",
      "i installed yt using conda from my home directory\n",
      "yeah, you may have to rely on a different means.\n",
      "a lot of hpc systems don’t play nice with isolated conda installs\n",
      "i gotta run this moment, but i’ll be back in the next hour.\n",
      "i figured not. it gets the job done for running serial, but obviously some seriously wasted computing power\n",
      "check out the config file in the `module load mpi4py`\n",
      "will do, thanks!\n",
      "and see if you can reinstall mpi4py from source using a similar config\n",
      "that’s the first thing to attempt.\n",
      "ok be back in a bit\n",
      "did that help at all?\n",
      "i made some head way and am very close to getting it. essentially what i did was load modules i needed from my submit script (mpi4py, etc.). it gave me an error not being able to find yt. so, at the top of my python script i gave it `sys.path.append('/p/home/mime5507/yt-conda/src/yt_conda')`. now it is saying `attributeerror: module 'yt' has no attribute 'enable_parallelism'`, which doesn’t make sense because if i run python from the terminal, i can do `import yt` then `yt.enable_parallelism()` just fine.\n",
      "hmm….\n",
      "the only thing i can think of is in the `yt-conda/scr` directory, there is both `yt-git` and `yt_conda`. when i imported yt in the terminal and wrote `yt.__file__`, it outputted `'/p/home/mime5507/yt-conda/src/yt-git/yt/__init__.py'`\n",
      "as opposed to in `src/yt_conda`\n",
      "but i am not sure why one would have an enable_parallelism feature and the other doesn’t\n",
      "that’s odd, given your $path.\n",
      "did you install yt twice using different methods? one from conda or pip and one from src?\n",
      "i just installed from source using `curl -ol <https://raw.githubusercontent.com/yt-project/yt/master/doc/install_script.sh>` then `bash install_script.sh`\n",
      "maybe i could try just a conda install?\n",
      "when you tried to install mpi4py on your conda python, you got a failure, right?\n",
      "you just did `pip install mpi4py` or something?\n",
      "or you’re just using the module?\n",
      "i actually didn’t install mpi4py, i am just using the module.\n",
      "yep!\n",
      "try installing mpi4py with your existing python setup.  try `module unload mpi4py; pip install mpi4py`\n",
      "i did try installing mpi4py a while back but it didn’t really work\n",
      "that would at least have mpi4py running on your specific python setup, if the install works.\n",
      "when i did that before, it gave me:\n",
      "```--------------------------------------------------------------------------\n",
      "the application appears to have been direct launched using \"aprun\",\n",
      "but ompi was not built with alps support. this usually happens\n",
      "when ompi was not configured --with-alps and we weren't able\n",
      "to discover an alps installation in the usual places.\n",
      "\n",
      "please configure as appropriate and try again.\n",
      "--------------------------------------------------------------------------\n",
      "*** an error occurred in mpi_init_thread\n",
      "*** on a null communicator\n",
      "*** mpi_errors_are_fatal (processes in this communicator will now abort,\n",
      "***    and potentially your mpi job)\n",
      "[nid01613:43817] local abort before mpi_init completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!```\n",
      "when i installed my own mpi4py\n",
      "ooof.\n",
      "i think the hpc system really wants you to use their mpi4py\n",
      "well, it likely wants you to use their python too.\n",
      "yeah… i also load in their module for that. the rest of the modules seem to be taken care of adding `sys.path.append('/p/home/mime5507/yt-conda/src/yt_conda')` to the top of my python script. but then no enable_parallelism. so close!\n",
      "what if you use `sys.path.append('/p/home/mime5507/yt-conda/src/yt-git` instead?\n",
      "it fails to find the module `sympy`.\n",
      "even if i include both.\n",
      "can you pip install --user sympy ?\n",
      "let me give that a try!\n",
      "thanks to <@u042j5bn6> help, we were successful! let me know if anyone else needs help with this task. also, apologies for cluttering this slack channel.\n",
      "in short, the method was to use the system wide install of python/mpi4py, then `pip install --user` any remaining yt dependencies, then `git clone yt` and `pip install --user -e .` yt from the git source.\n",
      "hey all - i know there's a method for depositing particles into octrees (yt 3.x) using *just* the gas particles (as opposed to all particles) -- but i can't seem to find the relevant source code or documentation.  does anyone know offhand either the syntax, or the docs?\n",
      "oh is it this:\n",
      "\n",
      "<https://yt-project.org/doc/examining/loading_data.html#indexing-criteria>\n",
      "<@u046k2qnk> that changes how the octree is built, but not deposition\n",
      "deposition is controleld by field name\n",
      "so something like [\"deposit\",\"gas_density\"]\n",
      "sorry, i misspoke - what i actually meant wsa how the octree is built!\n",
      "thanks!  (its amazing how just saying the question outloud sometimes all of a sudden elucidates the answer)\n",
      "so the issue is that somehow your ramses version dumps invalid binary files. the \"best\" fix would be to just skip the reading of `mass_sph`\n",
      "the easiest i see would be to patch yt. could you try using the following patch against yt dev?\n",
      "```\n",
      "from 3526ad0f76cccff9825f4ee25b5a8758fa45c48b mon sep 17 00:00:00 2001\n",
      "from: corentin cadiou &lt;contact@cphyc.me&gt;\n",
      "date: sun, 5 may 2019 19:02:08 +0200\n",
      "subject: [patch] quick fix\n",
      "\n",
      "---\n",
      " yt/frontends/ramses/data_structures.py | 1 +\n",
      " yt/frontends/ramses/definitions.py     | 3 ++-\n",
      " 2 files changed, 3 insertions(+), 1 deletion(-)\n",
      "\n",
      "diff --git a/yt/frontends/ramses/data_structures.py b/yt/frontends/ramses/data_structures.py\n",
      "index c5802cbba..12979474e 100644\n",
      "--- a/yt/frontends/ramses/data_structures.py\n",
      "+++ b/yt/frontends/ramses/data_structures.py\n",
      "@@ -139,6 +139,7 @@ class ramsesdomainfile(object):\n",
      " \n",
      "         for header in ramses_header(hvals):\n",
      "             hvals.update(f.read_attrs(header))\n",
      "+        f.seek(4+8+4)  # skip reading of mass_sph\n",
      "         # for speedup, skip reading of 'headl' and 'taill'\n",
      "         f.skip(2)\n",
      "         hvals['numbl'] = f.read_vector('i')\n",
      "diff --git a/yt/frontends/ramses/definitions.py b/yt/frontends/ramses/definitions.py\n",
      "index 9cc8278ed..a3235580a 100644\n",
      "--- a/yt/frontends/ramses/definitions.py\n",
      "+++ b/yt/frontends/ramses/definitions.py\n",
      "@@ -42,7 +42,8 @@ def ramses_header(hvals):\n",
      "                  ('stat', 3, 'd'),\n",
      "                  ('cosm', 7, 'd'),\n",
      "                  ('timing', 5, 'd'),\n",
      "-                 ('mass_sph', 1, 'd') )\n",
      "+                 #('mass_sph', 1, 'd') \n",
      "+                 )\n",
      "     yield next_set\n",
      " \n",
      " field_aliases = {\n",
      "-- \n",
      "2.21.0\n",
      "```\n",
      "hey, i did comment out mass_sph part to dev version but ending up with another error:\n",
      "hi, i am trying to plot exodus 2d data (with .e extension) for which sliceplot worked fine. i am trying to plot vectors using annotate_quiver over a sliceplot as shown here: <http://yt-project.org/doc/visualizing/callbacks.html#overplot-quivers> my vector variable is of lagrange_vec type called nlc. my question is how can i access x and y components for plotting? i have tried using nlcx, nlcy and nlc_x, nlc_y but they give the error: `ytplotcallbackerror: annotate_quiver callback failed with the following error: could not find field '('io', 'nlc_x')' in lcn0_other.e.`\n",
      "here is my code:\n",
      "\n",
      "what's `ds.field_list`?\n",
      "if `nlc_x` is in there then this sounds like a bug, in which case please report it on github and include a dataset to trigger the issue with\n",
      "<@ucc3waxe3> has joined the channel\n",
      "i have been making some nice visualizations with combinations of particleplot when filtered for various interesting cuts. i would like to align these plots with a galaxy’s disk axis but there seems to be no analog to offaxisprojectionplot that operates on particles. the deposited particle fields are too crude for my purposes. have i missed something? is there a good workaround or replacement that will render particles in an off-axis projection?\n",
      "you could reload the particle data using yt.load_particles after rotating the particle positions\n",
      "in principle we could add a offaxisparticleplot or maybe add an option to particleplot that rotates the particles, i think there’s even fast code for doing the rotations (at least on the yt-4.0 branch), that would be a great improvement\n",
      "<@uc6l85lbb> sounds vaguely like a bug to me, any chance you can make a reproducible example using one of the test datasets on <http://yt-project.org/data|yt-project.org/data> that demonstrates the issue? also make sure you’re using at least the newest stable release.\n",
      "<@u046k2qnk> has joined the channel\n",
      "<@u5t7v4urg> has joined the channel\n",
      "hi, i was trying to create an interactive data visualization with a code very similar to this <https://yt-project.org/doc/cookbook/opengl_vr.py>, but i keep getting errors like ```/anaconda/lib/python3.6/site-packages/yt/units/yt_array.py:1293: runtimewarning: invalid value encountered in log10\n",
      "  out_arr = func(np.asarray(inp), out=out, **kwargs)\n",
      "/anaconda/lib/python3.6/site-packages/yt/visualization/volume_rendering/interactive_vr.py:422: runtimewarning: all-nan axis encountered\n",
      "  self.min_val = min(self.min_val, np.nanmin(block.my_data[0].min()))\n",
      "/anaconda/lib/python3.6/site-packages/yt/visualization/volume_rendering/interactive_vr.py:423: runtimewarning: all-nan axis encountered\n",
      "  self.max_val = max(self.max_val, np.nanmax(block.my_data[0].max()))```\n",
      "i loaded a fits file with which i successfully created an idv before, but since i updated some of my packages this does not seem to work again\n",
      "not sure what's the problem...any hint is much appreciated\n",
      "what sort of data is this?\n",
      "a fits file with axes ra, dec, and velocity\n",
      "ah, so a fits file?\n",
      "i’m not aware of an issue related to this\n",
      "if you file an issue with steps to reproduce the problem one of us could try to take a look\n",
      "i don’t think there have been any changes to the idv stuff recently\n",
      "so i don’t know offhand what the issue might be\n",
      "file an issue on github?\n",
      "yup\n",
      "if it’s possible, include a link to the data you’re working with\n",
      "thank you! doing it right now\n",
      "you can share data using “yt upload path/to/data.fits”\n",
      "and then copy/paste the link that prints out\n",
      "<@u9s817f7w> has joined the channel\n",
      "howdy folks. so i'm currently thinking about a change to the code, which if i get working i'd like to share. i have the latest pip install, should i uninstall it and pull the git (as in on the yt install page) or is there a fancy way to \"fake it\"?\n",
      "the pull request i mean.\n",
      "<http://yt-project.org/doc/installing.html#installing-yt-from-source>\n",
      "i really want quiver to have an annotated scale arrow, and i've already done this once by hand for a matplotlib quiver plot i made :stuck_out_tongue:\n",
      "i’d do the “pip install -e .” instructions at the bottom there\n",
      "for the yt development workflow, see <http://yt-project.org/doc/developing/developing.html#making-and-sharing-changes>\n",
      "okay, but i should install the git version... that's what i figured.\n",
      "yup\n",
      "okay thanks!\n",
      "you want to be developing from git :slightly_smiling_face:\n",
      "that sounds like a great addition btw :slightly_smiling_face:\n",
      "cool, yeah i see the post in the email forums about it from 2014, and your quiver plot stuff looks almost exactly like mine, so i think this will be pretty easy.\n",
      "issue opened\n",
      "<@u5t7v4urg> those lines in `interactive_vr.py` don't make any sense:\n",
      "```\n",
      "self.min_val = min(self.min_val, np.nanmin(block.my_data[0].min()))\n",
      "```\n",
      "remove that inner .min() and see if it works for you\n",
      "same for .max()\n",
      "it should be:\n",
      "```\n",
      "self.min_val = min(self.min_val, np.nanmin(block.my_data[0]))\n",
      "self.max_val = max(self.max_val, np.nanmax(block.my_data[0]))\n",
      "```\n",
      "where should i remove it?\n",
      "it's the line that raises warnings in your bug report\n",
      "```\n",
      "/anaconda/lib/python3.6/site-packages/yt/visualization/volume_rendering/interactive_vr.py:422: runtimewarning: all-nan axis encountered\n",
      "  self.min_val = min(self.min_val, np.nanmin(block.my_data[0].min()))\n",
      "/anaconda/lib/python3.6/site-packages/yt/visualization/volume_rendering/interactive_vr.py:423: runtimewarning: all-nan axis encountered\n",
      "  self.max_val = max(self.max_val, np.nanmax(block.my_data[0].max()))\n",
      "```\n",
      "we should probably fix that on the yt side i guess\n",
      "yup\n",
      "that's in the yt\n",
      "i can't recall if that's still there in the idv pr\n",
      "the idv pr seems to show there is no change to these two lines\n",
      "cool\n",
      "<@u5t7v4urg> sounds like a good material for a pr :slightly_smiling_face:\n",
      "emmm what could i do? (sorry that i'm not so familiar with github)\n",
      "there are instructions here: <http://yt-project.org/doc/developing/developing.html#making-and-sharing-changes>\n",
      "but if you don’t want to do that you don’t need to\n",
      "i’d comment on the issue what the solution is though\n",
      "ah i just wish to fix it:joy:i'll look at the website and learn\n",
      "<@u5t7v4urg> i am out sick today but will try tomorrow to get the idv stuff in a place you can use it.  or if you're around we can try meeting up.\n",
      "i made the two changes as <@u042f73r7> suggested, it worked but produced something really weird...\n",
      "looks like an interstellar cloud to me!\n",
      "[kidding]\n",
      "<@u042hlt7u> sorry to hear that you are sick! i'm on campus all the time. just email me if you'd like to meet\n",
      "i remember it worked before:joy:somehow i got it messed up...\n",
      ":slightly_smiling_face:\n",
      "it's probably not your fault -- the idv has been in need of some loving upgrades\n",
      "has anyone experienced any errors with pastebin and connection errors recently?\n",
      "i remember having issues a couple of years ago with this, but i thought we moved to a different service.\n",
      "i can give a full traceback if it is useful, but i think the main thing is that the pastebin site is not accepting socket connections.\n",
      "<@u37dtbl6n> has joined the channel\n",
      "<@u042j5bn6> it's been recently migrated by <@u042f73r7>\n",
      "but it should still work?\n",
      "<@u042j5bn6> try now\n",
      "ok.\n",
      "no dice. :disappointed:\n",
      "it may be dns issue\n",
      "```\n",
      "[cambot:~/scratch/yuguang] chummels% yt pastebin halo.py\n",
      "/users/chummels/src/yt-conda/lib/python3.6/site-packages/cmocean/tools.py:76: matplotlibdeprecationwarning: the is_string_like function was deprecated in version 2.1.\n",
      "  if not mpl.cbook.is_string_like(rgbin[0]):\n",
      "yt : [info     ] 2018-05-16 16:29:33,037 loading plugins from /users/chummels/.config/yt/my_plugins.py\n",
      "traceback (most recent call last):\n",
      "  file \"/users/chummels/src/yt-conda/bin/yt\", line 11, in &lt;module&gt;\n",
      "    load_entry_point('yt', 'console_scripts', 'yt')()\n",
      "  file \"/users/chummels/src/yt-conda/src/yt-git/yt/utilities/command_line.py\", line 1413, in run_main\n",
      "    args.func(args)\n",
      "  file \"/users/chummels/src/yt-conda/src/yt-git/yt/utilities/command_line.py\", line 237, in run\n",
      "    self(args)\n",
      "  file \"/users/chummels/src/yt-conda/src/yt-git/yt/utilities/command_line.py\", line 833, in __call__\n",
      "    private=args.private, clipboard=args.clipboard)\n",
      "  file \"/users/chummels/src/yt-conda/src/yt-git/yt/utilities/lodgeit.py\", line 315, in main\n",
      "    pid = create_paste(code, language, filename, mimetype, private)\n",
      "  file \"/users/chummels/src/yt-conda/src/yt-git/yt/utilities/lodgeit.py\", line 211, in create_paste\n",
      "    private)\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/xmlrpc/client.py\", line 1112, in __call__\n",
      "    return self.__send(self.__name, args)\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/xmlrpc/client.py\", line 1452, in __request\n",
      "    verbose=self.__verbose\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/xmlrpc/client.py\", line 1154, in request\n",
      "    return self.single_request(host, handler, request_body, verbose)\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/xmlrpc/client.py\", line 1166, in single_request\n",
      "    http_conn = self.send_request(host, handler, request_body, verbose)\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/xmlrpc/client.py\", line 1279, in send_request\n",
      "    self.send_content(connection, request_body)\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/xmlrpc/client.py\", line 1309, in send_content\n",
      "    connection.endheaders(request_body)\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/http/client.py\", line 1234, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/http/client.py\", line 1026, in _send_output\n",
      "    self.send(msg)\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/http/client.py\", line 964, in send\n",
      "    self.connect()\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/http/client.py\", line 936, in connect\n",
      "    (self.host,self.port), self.timeout, self.source_address)\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/socket.py\", line 724, in create_connection\n",
      "    raise err\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/socket.py\", line 713, in create_connection\n",
      "    sock.connect(sa)\n",
      "connectionrefusederror: [errno 61] connection refused\n",
      "```\n",
      "i forgot to update it for paste, it works for me now\n",
      "can you try using browser?\n",
      "i’m not sure how to do that.\n",
      "<https://pastebin.yt-project.org>\n",
      "see if it opens\n",
      "nope.\n",
      "```\n",
      "this site can't be reached\n",
      "<http://pastebin.yt-project.org|pastebin.yt-project.org>'s server ip address could not be found.\n",
      "```\n",
      "try again in 30m - 60m it may take a while for a dns change to propagate\n",
      "ok cool.\n",
      "thanks for looking into this!\n",
      "i’ll try again in an hour and update here my results.\n",
      "```$ dig <http://paste.yt-project.org|paste.yt-project.org>\n",
      "\n",
      "; &lt;&lt;&gt;&gt; dig 9.11.1-p3 &lt;&lt;&gt;&gt; <http://paste.yt-project.org|paste.yt-project.org>\n",
      ";; global options: +cmd\n",
      ";; got answer:\n",
      ";; -&gt;&gt;header&lt;&lt;- opcode: query, status: noerror, id: 34372\n",
      ";; flags: qr rd ra; query: 1, answer: 1, authority: 0, additional: 1\n",
      "\n",
      ";; opt pseudosection:\n",
      "; edns: version: 0, flags:; udp: 512\n",
      ";; question section:\n",
      ";paste.yt-project.org.          in      a\n",
      "\n",
      ";; answer section:\n",
      "<http://paste.yt-project.org|paste.yt-project.org>.   3563    in      a       141.142.211.195\n",
      "\n",
      ";; query time: 17 msec\n",
      ";; server: 75.75.75.75#53(75.75.75.75)\n",
      ";; when: wed may 16 18:32:11 cdt 2018\n",
      ";; msg size  rcvd: 65\n",
      "```\n",
      "it should return 141.142.211.195\n",
      "if it returns something different it's dns\n",
      "`yt pastebin &lt;file&gt;` now works!\n",
      "thanks, <@u042f73r7>\n",
      "we should make that command use gist tbh\n",
      "yeah, that would make it easier to use with nbviewer\n",
      "thanks for linking this !\n",
      "i’ve been wondering if there’s a canonical way in yt to vizualize non-cartesian geometries within their respective native coordinate system. for instance, given a polar dataset, visualising it in a r vs theta frame is often very useful. in essence, allowing this should be pretty easy because it’s more or less equivalent to pretending the dataset is cartesian. anyone would know about that ?\n",
      "i guess you can use phase plots for this\n",
      "good idea, unsettling results\n",
      "here’s what i get\n",
      "it is probably due to pixellisation issues no ?\n",
      "i guess so, the only region that appears contiguous has the highest amr level\n",
      "<@ut8s6hl95> has joined the channel\n",
      "hi, i've got some really basic question; i got a plot that's very high but thin. i am trying to get things readable by increasing fontsize but then ticklabels for x axis (the thin one) overlap. the easiest solution i think would be to change ticks, but i found no answer in yt-cookbook regarding that and currently doubt that's even possible. so - is there an easy and straightforward way to edit ticks? if no, what's the way? (example picture attached, look at the bottom ax)\n",
      "<@usvszutm1> has joined the channel\n",
      "i'm getting the \"blocky\" issue with boxlib type data as well. yt version 3.5.1, and the issue shows up in both python2 and python3.  (see pictures below: same dataset visualized with yt and with visit).\n",
      "i only observe the issue when visualizing *nodal* `multifab` data. visualization is fine when the `multifabs`  are cell-based.,\n",
      "in the issue that got opened on this andrew myers said that it’s fixed on master already\n",
      "can you try building yt from source?\n",
      "<https://github.com/yt-project/yt/issues/2427|https://github.com/yt-project/yt/issues/2427>\n",
      "<@usvumgyv9> has joined the channel\n",
      "<@ut9k5tyna> has joined the channel\n",
      "<@ud9l1d44t> i *think* if you explicitly set the geometry as cartesian when you load the dataset, `ds = yt.load(..., geometry=\"cartesian\")` it'll do what you want, haven't tried though so dunno if it will actually work. the axes names will be \"xyz\" if you do that, but yt should interpret the data as cartesian.\n",
      "i actually implemented a `geometry_override` load option in the amrvac frontend for exactly that purpose, but it still feels like a hack\n",
      "<@ut8s6hl95> yes, see this notebook for an example: <https://gist.github.com/ngoldbaum/d01e576c9a31e7cdb903bff1974532ec>\n",
      "<@ud9l1d44t> what would feel less like a hack to you?\n",
      "there's a big if statement in the pixelization routines that tell yt which pixelizer to pick based on the geometry\n",
      "presumably that's the code you want to mess with\n",
      "then i think that what i want would be the option of using the cartesian pixelizer within any geometry.\n",
      "it’s very low priority but it is on my wish list nonetheless\n",
      "ok, well, you can take a look at the geometry handlers\n",
      "and their `pixelize` methods\n",
      "to see how they work\n",
      "yeah, thanks for pointing that out :slightly_smiling_face:\n",
      "for example, here's the one for cylindrical geometries: <https://github.com/yt-project/yt/blob/master/yt/geometry/coordinates/cylindrical_coordinates.py#l98>\n",
      "thanks nathan ! if i can manage to spend some time with this i might have follow up questions, but i’ll ask <#cbe6579cz|development> then :slightly_smiling_face:\n",
      "<@usvumgb0b> has joined the channel\n",
      "hi all, i want to ask does yt amr structure supports both 2d data and in cylindrical coordinates?? there aren't much examples about them. most of the examples are working with 3d data in either cartesian or spherical coordinates. i am working with a new data from a newly developed hydrodynamic code, in hdf5 format. but i am having trouble loading it in to yt. i am currently looking in the function `yt.load_amr_grids`. is this the right function to do this?\n",
      "also, i try the example code `loading_generic_array_data.py` in the session \"generic amr data\" at `<http://yt-project.org/doc/examining/generic_array_data.html>`. when i reduce `grid_data` to a 2d structure, the `yt.load_amr_grids` function failed. does that mean this function only supports 3d amr structure? thanks for answering!\n",
      "it should work, passing `geometry='cylindrical'`, it’s possible you’re hitting a bug though, can you give more details about what failed?\n",
      "for these sorts of discussions, it’s most helpful to have a test script that anyone can run (e.g. that includes links to all necessary data files or uses fake random data) to trigger the behavior that you’re having trouble with\n",
      "<@uf14fnztq> i’m gonna need to run in a little bit, but if you can create a github issue about this (sounds like a bug to me!) including relevant detail (e.g. if there’s a crash, please include the traceback and error message) and if you have time to put it together, a test script that one of us can run to trigger the issue, you can file issues at <http://github.com/yt-project/yt/issues/new>\n",
      "<@u042hlt7u> i recently developed support for _adapta_hop, which, confusingly, is not compatible with hop catalogs\n",
      "<@uppavuhsm>, is hop not finding any halos or just putting them in the wrong places? i’ve seen similar things in the past when some of the fields aren’t being passed correctly to the halo finder\n",
      "<@u042s6y2g> well, its finding them now but it doesn't come up with the right image unless i make the width of the projection plot really huge (on the order of 1000kpc) - if i put in the center coordinates and make it about 120kpc then the image is just coloured squares but 1000kpc is a high resolution, very zoomed out image of the galaxy and im just not really sure what's going on\n",
      "this is the 1000kpc image:\n",
      "this is the 120kpc image\n",
      "same center coordinates for both\n",
      "<@uppavuhsm> judging by the colorscale you're just hitting void, can you center your 120kpc wide image on any halo?\n",
      "<@u042f73r7> by hitting void do you mean that im hitting the low resolution part of the simulation? because i don't really understand how that's happening if im using the exact same coordinates for the centre for both of these plots\n",
      "and no i can't get any 120kpc image, they all look like this one^ but different\n",
      "not necessarily low resolution, just a place where there's no gas\n",
      "if you do 400kpc wide, does it still look reasonable? also can you share the code that you're using to make those plots?\n",
      "sure\n",
      "give me a few mins im gonna try and make them again just to triple check im not doing anything stupid\n",
      "what object are using for projection? the entire domain or i.e. just a small sphere?\n",
      "the entire domain\n",
      "soooooo it'll take some time\n",
      "setting `center` argument to `\"max\"` would probably the quickest way of verifying if what i say makes sense\n",
      "lets make this a thread\n",
      "im not sure what you mean\n",
      "`projectionplot(..., center=\"max\")`\n",
      "it will find the maximum in gas density and center the projection on it\n",
      "but im trying to find a specific galaxy from the catalogs\n",
      "i'll run that as well\n",
      "with the width set to 400kpc and we'll see what comes up\n",
      "thank you btw!!!\n",
      "ok, i think britton's suggestion that coordinates are somehow wrong is the valid one\n",
      "yeah i was thinking that might be it, but it doesn't explain how i get this massive galaxy right in the middle of my 1000kpc plot of the same center coordinates\n",
      "really boggled me\n",
      "hey all,\n",
      "\n",
      "(admittedly i know very little about enzo datasets) -- i'm trying to load an enzo data set, cut out a subregion of this, and then create a new ds.   at first i thought maybe creating a `region` via:\n",
      "\n",
      "```import yt\n",
      "import numpy as np\n",
      "\n",
      "snapshot = '/users/desika.narayanan/dropbox/yt_datasets/enzo_iso_galaxy/galaxy0030/galaxy0030'\n",
      "\n",
      "ds = yt.load(snapshot)\n",
      "center = ds.arr([0.5,0.5,0.5],'code_length')\n",
      "len = ds.quan(0.25,'code_length')\n",
      "\n",
      "\n",
      "region = ds.region(center,center-len,center+len)```\n",
      "was the right path, but then when i try to access `region.ds`, i get a weakproxy:\n",
      "\n",
      "```in [30]: region.ds\n",
      "out[30]: &lt;weakproxy at 0x117f684c8 to enzodataset at 0x119ca9090&gt;\n",
      "\n",
      "in [31]: ds\n",
      "out[31]: galaxy0030```\n",
      "is there a smarter way to accomplish this?\n",
      "the weakproxy is a reference to the dataset\n",
      "as long as the dataset itself hasn’t been garbage collected you can use it just the same as the original dataset\n",
      "also in the script there you don’t use region.ds so i’m a little confused where that’s coming from\n",
      "ah - so this is just a self-contained snippet out of a larger program.   what i'm trying to do is create a new ds that is a smaller sub-region of the original ds (because, i want to set up an enzo simulation to do radiative transfer, but don't need the entire box).\n",
      "\n",
      "is the appropriate way to do this via a `region`  keyword, and if so, any tips on avoiding garbage collection?\n",
      "so the issue you’re having is that the weakproxy is no longer valid?\n",
      "you can do this two ways, first by reloading using the ytdata frontend just like i presume you do with sph data\n",
      "or just by using a region and making sure the dataset you created the region from doesn’t go out of scope\n",
      "i can elaborate a bit more if you can share a more full example that triggers whatever problem you’re having with the weakproxy\n",
      "in python an object gets garbage collected when no other objects refer to it\n",
      "so if you’re doing something like iterating over a time series and saving a list of regions to then operate on later you’ll need to either make sure something has a reference to the original dataset (e.g. by putting it in a list) or restructure so you operate on the region right after creating it\n",
      "so actually the above script, if i run it in a python terminal and then just type\n",
      "```region.ds```\n",
      "like before will trigger the weakproxy\n",
      "why is that a problem?\n",
      "you can use region.ds just like ds so long as the dataset hasn’t been garbage collected\n",
      "“trigger the weakproxy” means just seeing that the object is a weakproxy in the repl?\n",
      "yeah - exactly\n",
      "<https://docs.python.org/3.8/library/weakref.html|https://docs.python.org/3.8/library/weakref.html>\n",
      "^ to learn more about this\n",
      "the issue is that downstream in the code i call something like\n",
      "```    if region.ds.cosmological_simulation == false:```\n",
      "and it fails because of the weakproxy.  but i think the answer is to use the region.ds  soon after i create it (i'm not sure i do) to keep it from getting garbage collected\n",
      "yes, that means the dataset is being garbage collected\n",
      "cool - thanks for the advice i'll see if i can correct that\n",
      "you could refactor so the dataset is passed into that function, that would force you to make sure the dataset is still in scope somewhere\n",
      "thats good advice - that wouldn't be very hard\n",
      "another issue with this, i think, is that because the grids are selected before the parallel iteration begins, all the processes end up generating the icoords etc for all grids\n",
      "yup, true, the data selection operation happens on all processors (i think we also assume that the amr hierarchy can be held in memory on all processors)\n",
      "i’m a little confused why disabling the caches for those doesn’t fix this though\n",
      "i think it's `amrgridpatch._cache_mask`\n",
      "ah so it’s just the masks\n",
      "does setting that to false horrendously slaughter performance?\n",
      "not sure, need to confirm and test\n",
      "but i think that in this situation, we can special case the selection anyway\n",
      "probably, adding special logic for handling selection on unigrid runs would make a lot of sense\n",
      "as would e.g. a config option to disable problematic chaches\n",
      "disabling that cache does bring the memory down to about what i'd expect :+1:\n",
      "without seeming to hurt the performance too much - however, i do think it's spending a lot of time doing unnecessary selection checks, cache or no cache\n",
      "gonna try to work on that\n",
      "<@uuf099k2n> has joined the channel\n",
      "hi. i am trying to visualise a `&lt;hdf5 dataset \"3d_array\": shape (1024, 1024, 1024), type \"&lt;f8\"&gt;` as follows:\n",
      "\n",
      "```data = {\"initial theta\": (f['3d_array'][()], 'radians')}\n",
      "bbox = np.array([[-np.pi, np.pi], [-np.pi, np.pi], [-np.pi, np.pi]])\n",
      "ds = yt.load_uniform_grid(data, data[\"initial theta\"][0].shape, length_unit=2*np.pi/1024, bbox=bbox, nprocs=16, \n",
      "                       periodicity=(false,false,false))```\n",
      "but when i do `yt.projectionplot(ds, 2, \"initial theta\")` i get an error saying `ytfieldnotfound: could not find field '('stream', 'initial theta')' in uniformgriddata`. how should i resolve this?\n",
      "somehow `ds.derived_field_list` does not contain `('stream', 'initial theta')` whereas `ds.field_list` does. is this the reason?\n",
      "this is indeed odd!  what happens if you change the name from \"initial theta\" to something parseable, such as \"initial_theta\"?  does that result in any differences?\n",
      "<@u042hlt7u> does not make a difference.\n",
      "can you try changing `radians`  to `dimensionless`?\n",
      "you got it. now it's included in `ds.derived_field_list` and there are no errors thrown (although the kernel keeps dying for some reason). thanks! so, i can't use radians as units?\n",
      "i am not sure actually!  let me check into it.\n",
      "it *might* work if you do a projection that doesn't integrate -- for instance, using a method like \"mip\"\n",
      "i suspect the issue may be that multiplying the units of radians by a length is causing freakouts, or maybe \"radians\" is the wrong term for it\n",
      "okay, now with “radians” replaced by “dimensionless”, i consistently get a dying kernel whenever i try to do the projection plot. what do you think is probably happening? i am new to yt. i was trying to replicate this “loading generic array data” notebook with my own dataset which contains a 3-dimensional angular field. any suggestions how i could visualise it?\n",
      "nevermind, i can do a volume-render. thank you! :grin:\n",
      "hi max,\n",
      "i try to illustrate my task, actually, i have two distribution functions. one is maxwell-juttner (mj) distribution and power-law distribution in momentum space. i have to find the intersection point (say p_int) between these two distributions on each cell. now the thing is that mj distribution needs the value of  temperature and density field of the cell and power-law distribution needs the value of density and mach number field of the same cell to compute the intersection point of the said distribution functions on that cell which i will select based on some criteria.\n",
      "\n",
      "hope this will illustrate you what exactly i am looking for.\n",
      "since these functions are scalar function with variable p (momentum), i need to pass the field values to the functions to compute the intersection point.\n",
      "<@upy5j1ug3> has joined the channel\n",
      "hi, when i try to do a slice/projection plot of the cic field of my particles, i got a figure with a lot of artificial filaments. the figure is attached below. i am wondering if i will be able to make a more smooth-looking cic field.\n",
      "<@ubju11gju> i think that's an unavoidable artifact from the grid edges where you see the domain subdivided.  i think a long time ago someone (maybe <@u043bna00>?) worked on getting rid of it, but it's really tricky to do.  the stuff where you';re seeing it in voids and whatnot is probably not \"real\" in the sense that it's an extremely constrained (and not visually well-represented!) colorbar.\n",
      "<@u042hlt7u> i see. this plot is the most visible one for the effect. i can also see a lot of the filaments if i have a wider range of color bar. what is annoying is that it will make the true colors less visible especially for real very low density voids. see the plots attached.\n",
      "<@ubju11gju> yes, i see it in this too, but i genuinely don't know that there's a *lot* we can do without changing the not-quite-smoothing criteria.  the cic deposition is inextricably linked to the cell size, so there may just not be a lot we can do with it.  that being said, it's *possible* you could add a smoothing field (that also calculated the smoothing length) but i don't know that it would be worht the additional effort.  you might try making a particleplot, which changes the way the deposition is done to be more general and sometimes results in higher-quality images.\n",
      "<@u042hlt7u> i have tried the particleplot, there are still a lot of filaments seen there. i am wondering if there is any way for me to just export the cic density as the grid data.\n",
      "<@ubju11gju> yup, you can do a `covering_grid` and query the cic there -- that would also help you change the resolution.  you can do this with, for instance, `ds.r[::128j, ::128j, ::128j]` which will give you the full dataset (i.e., `::`) with 128 steps along each dimension.\n",
      "<@u042hlt7u> thank you. i will try that.\n",
      "when i look at the code, i did not get it… the main code for the sum up should be:\n",
      "```\n",
      "i_i, j_i, k_i = np.mgrid[0:3, 0:3, 0:3]\n",
      "for i, j, k in zip(i_i.ravel(), j_i.ravel(), k_i.ravel()):\n",
      "    sl = (slice(i, nx-(2-i)), slice(j, ny-(2-j)), slice(k, nz-(2-k)))\n",
      "    new_field += data[(ftype, basename)][sl] * data[(ftype, weight)][sl]\n",
      "```\n",
      "i can see it sums up `3^3=27` times of the sliced dataset — how can it be the sum up of entire dataset?\n",
      "sorry, i don’t follow, what do you mean be “sum up of the entire dataset”?\n",
      "maybe i’m not understanding what you were originally asking for\n",
      "the linked function defines a field that computes a local volume average\n",
      "of the 27 nearest neighbors of every zone\n",
      "it uses some fancy indexing tricks to do it in a concise way\n",
      "what i would like to do is to create a derived field, which contains the average along one axis over the entire volume.\n",
      "ah ok, there isn’t a way to do that right now\n",
      "although that’s basically a projection?\n",
      "you could do a projection and weight by the “ones” field\n",
      "yes, it’s basically a projection, while i’m thinking whether is possible to incorporate it into derived field, since what i finally want to get is the field perturbation relative to the mean field averaged along one axis.\n",
      "i think you could do this in an extremely hacky way with a global variable\n",
      "i’m not sure offhand whether a field parameter can be a 2d array\n",
      "if that works you could use that\n",
      "unfortunately right now fields can’t do global operations like you’re wanting them to do\n",
      "so only hacky solutions will work\n",
      "i see. even with a 2d array as field parameter, i need to worry that the date might be a block only, not the entire domain, so i need some coordinate index matching…\n",
      "in this case, maybe it’s simpler to extract some fix resolution data first.\n",
      "you could pass the 1d flattened projection\n",
      "although going from the flattened projection to 3d indices wouldn’t be straightforward\n",
      "yeah, i can see this could work, but will cost more time in coding… it’s nice to confirm that there’s no super easy way to do this in derived field. thank you!\n",
      "this is something we’re funded to work on\n",
      "adding the ability to do things like ray tracing and domain convolutions in a field definition\n",
      "i’ll try to remember this as a use case\n",
      "thanks! the perturbation field relative to the mean of entire field is easy to compute, while relative to the mean of one axis is useful in plane-parallel configuration, or in curvilinear geometry to get the non-axisymmetric component.\n",
      "<@u0610ftp0> i think i did this once, to get deviation from a radially-varying, azimuthally averaged field...  it was annoying.\n",
      "<@uk6ku2khs> has joined the channel\n",
      "hi <@uu72v7a31>, here are some instructions for getting changes from my forks of yt and yt_astro_analysis.\n",
      "\n",
      "get my fork of yt:\n",
      "git remote add britton <https://github.com/brittonsmith/yt>\n",
      "git fetch britton halopart\n",
      "git checkout halopart\n",
      "pip install -e .\n",
      "\n",
      "get my fork of yt_astro_analysis:\n",
      "git remote add britton <https://github.com/brittonsmith/yt_astro_analysis>\n",
      "git fetch britton particles\n",
      "git checkout particles\n",
      "pip install -e .\n",
      "\n",
      "with both of those, you should be able to do the following:\n",
      "ds = yt.load(&lt;halo_catalog_file&gt;)\n",
      "halo = ds.halo(‘halos’, &lt;halo_id&gt;)\n",
      "halo[‘member_ids’]\n",
      "ok, thank you so much for your help!\n",
      "<@ue3gnrc0h> has joined the channel\n",
      "this should be easy but where are the datasets used in the cookbooks?\n",
      "ds = yt.load(\"isolatedgalaxy/galaxy0030/galaxy0030\")\n",
      ":thinking_face:\n",
      "nevermind, i found the data page!\n",
      "<@uja0yus03> has joined the channel\n",
      "is it possible to ask yt load the simulation data into float32 type arrays? i have a very big simulation snapshot, which can not be fitted into the server memory if float64 type arrays are used. thanks!\n",
      "<@u1tmqqb38> unfortunately i am pretty sure this isn’t possible. there are a lot of places in the code where things are explicitly converted to `float64` if it isn’t that type already\n",
      "well, thanks for the information. it is a pity that there will be about half of the memory are wasted because the simulation data is normally `float32` type. how yt by design to handle the very large simulation if the analysis has to be done in a memory limited server? can it be paralleled in multiple nodes with mpi?\n",
      "<@u1tmqqb38> <http://yt-project.org/doc/analyzing/parallel_computation.html>\n",
      "<@u010ltm19hd> has joined the channel\n",
      "<@u0107maqwqk> has joined the channel\n",
      "<@u0107n419v1> has joined the channel\n",
      "<@uqpmqdf0t> has joined the channel\n",
      "hey, i'm having a little trouble with getting covering_grid working with some eagle data. only just started using yt, so maybe just missing something obvious.\n",
      "i'm just getting the following error message whenever i run covering_grid (or arbitrary grid):\n",
      "\n",
      "attributeerror: 'ytcoveringgrid' object has no attribute 'smooth'\n",
      "all i'm doing is the following:\n",
      "\n",
      "```ds = yt.load(fname)\n",
      "covering_grid = ds.covering_grid(2, 0, 16)\n",
      "cg_density = covering_grid[('gas', 'density')]```\n",
      "projection plots (and ray creation and spectrum generating) having been generating fine with the data, but haven't been able to figure out where i'm going wrong with covering_grid. anyone got any suggestions of what i'm getting wrong?\n",
      "what version of yt is this? you can print `yt.__version__`\n",
      "ah, 3.5.1. do i need 4?\n",
      "~no, i think this should work yt-3 but, i'm not very knowledgeable of yt-3. does changing to 'gas' to 'parttype0' help?~\n",
      "this may need yt-4 actually...\n",
      "~i think `('parttype0', 'density')` will work by adding up mass of particles in an oct and dividing by oct volume which is not quite an sph density but it is often good enough for most things~\n",
      "\n",
      "ah i'm wrong. `('parttype0', 'density')` just returns the particles densities i think\n",
      "yeah, i think that's what i'm seeing when i run that. currently installing yt-4 to have a go with that instead.\n",
      "*hopefully* things work with yt-4 if not please post\n",
      "also i think yt-3 can do a density deposit with `covering_grid[('deposit', 'parttype0_density')]`\n",
      "but it isn't the _correct_ sph deposition\n",
      "thanks for the pointers ash. think i'm up and running now - it does at least run without errors and outputs a 3d array anway...\n",
      "great!\n",
      "is it the 4.x that works or the 3.x with (`deposit`,`parttype0_density`)?\n",
      "that’s the way it works in 3, i think it still works in 4 for back compat reasons\n",
      "rich said it was yt-4.0 which installed and worked\n",
      "sorry, yes the yt.covering_grid code in yt-4.0 seemed to work fine. i didn't do a detailed check of the output, but did do a quick sanity check by summing along one axis to compare to what i'd previously got with the same data using yt.projectionplot in yt-3. the same density structures were there at least in both projections!\n",
      "cool, glad you got things working :slightly_smiling_face:\n",
      "yt-4 should also be way, way faster\n",
      "<@u9ce5d9lz> btw would love to see a pr for those octree changes you've been working through with desika\n",
      "it's ok if it's not \"done\" it just needs to be an improvement over the status quo, which i understand is quite broken\n",
      "yeah me and desika chatted in private and we've got some things planned with a timeline etc\n",
      ":+1:\n",
      "happy to review whenever you're ready\n",
      "yeah i know. it's a bigger change than we first anticipated but it's difficult to do whilst attending a conference this week. i know know, i need to work faster\n",
      "lol no worries at all on the wait, it's a holiday in the us anyway\n",
      "enjoy the conference\n",
      "<@u9ce5d9lz> thanks so much for the work you're doing for this to help enable 4.x to be baackwards compatible with the 3.x octrees!\n",
      "hi, im having a little trouble with projection plots of different particle types. i'm using ramses data and im trying to make a non weighted projection plot of a small part of the simulation of specifically just the gas or just the star particles, but i'm not sure how to specify this in yt.projectionplot\n",
      "<@u042fh0rb> i was wondering where the issue on the yt github page for this problem <https://github.com/conda-forge/netcdf4-feedstock/issues/44>  is...i could not find it but i have the same problem when loading a flash file (netcdf4 version 1.4.1)\n",
      "that error was due to conda-forge building a debug version of netcdf4 which had some runtime debug checks that should have been turned off. not sure why you’re hitting it if your netcdf4 package is up to date (is it?). if you’re reading flash data a workaround would be to uninstall netcdf4 completely.\n",
      "<@u2212qe6b> has joined the channel\n",
      "<@u31lwtknw> has joined the channel\n",
      "<@ukg7hc22x> has joined the channel\n",
      "<@u2s6z0p0t> has joined the channel\n",
      "do you mean an average over a certain volume? you can create a region over which you want to calculate the average (<https://yt-project.org/doc/analyzing/objects.html#region-reference>`) and then use `mean` or `weighted_average` (see example: <https://yt-project.org/doc/analyzing/objects.html#quickly-processing-data>)\n",
      "has anyone done a vr with multiple fields and displayed both transfer functions as legends in the image?\n",
      "i might try to hack this into the code, but i was curious if anyone else has already\n",
      "\n",
      "actually wasn't too hard to hack\n",
      "<@udj5lcug2> has joined the channel\n",
      "<@uk2dlmfcn> has joined the channel\n",
      "<@ue9t33skw> has joined the channel\n",
      "hi!\n",
      "i am new to yt and having trouble using halo catalogs... i am trying to identify and analyze galaxies in a galactic scale simulation and created a halo catalog using hop.\n",
      "the problem is that the halo catalog for some reason changes the coordinates and i have no idea how and why.\n",
      "i thought i could just overlap both coordinate systems (the one from the original data and the one from the halo catalog) by setting the origin to the center of mass, but it either is changed by creating the halo catalog or the accuracy is not good enough.\n",
      "i hope someone can help me with my problem\n",
      "can you share an example of what you’re seeing (e.g. copy/paste the script you’re running and the output you’re seeing)? what simulation code are you working with?\n",
      "nevermind, i just realized my mistake, i accidently deleted a line that would do unit conversion which messed everything up :see_no_evil:\n",
      "sorry for bothering you\n",
      "yay, a thanksgiving miracle!\n",
      ":turkey:\n",
      "hey! i wanted to use yt's defaults to make a matplotlib plot, can you remind me how to do this?\n",
      "i remember nathan showing me a matplotlib context just for that\n",
      "yt.funcs.matplotlib_style_context\n",
      "thanks!\n",
      "hi! i had a question if anyone knows the answer: is there a method of slicing in a sphere data object that can only look within a certain radius? say i have a sphere already made and then i only want to look within 10% of the radius for gas, is this possible without building a new sphere?\n",
      "if you want to make images, that might be tough. but if you want to just grab data, you might be able to do something like: `sphere[some_field][sphere['radius'] &lt;= 0.1 * sphere.radius]`\n",
      "or you could make a cut_region:\n",
      "```\n",
      "cr = ds.cut_region(sphere, [\"obj['radius'] &lt; value\"])\n",
      "```\n",
      "in that one, i think you’d have to give the radius value explicitly\n",
      "hm the first one seems fine, thank you! there was a problem though, it threw an error when i used the syntax you provided\n",
      "i replaced the condition with a numpy where statement and that worked though\n",
      "what was the syntax error?\n",
      "it’s ok, glad you got something working though!\n",
      "```---------------------------------------------------------------------------\n",
      "indexerror                                traceback (most recent call last)\n",
      "&lt;ipython-input-35-f189ebe56afa&gt; in &lt;module&gt;\n",
      "----&gt; 1 sph['parttype1', 'masses'][sph['radius'] &lt; 0.5 * sph.radius]\n",
      "\n",
      "/project/b/babul/rennehan/anaconda3/envs/pynbody/lib/python3.6/site-packages/yt/units/yt_array.py in __getitem__(self, item)\n",
      "   1056 \n",
      "   1057     def __getitem__(self, item):\n",
      "-&gt; 1058         ret = super(ytarray, self).__getitem__(item)\n",
      "   1059         if ret.shape == ():\n",
      "   1060             return ytquantity(ret, self.units, bypass_validation=true)\n",
      "\n",
      "indexerror: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 8```\n",
      "(i know there are no particles within the radius, it gives the same error if there are)\n",
      "ah, it might have to be `sph['parttype1', 'radius']`\n",
      "in fact, it’s likely to be incorrect with the `np.where` without that\n",
      "because `sph['radius']` will be for all particles, so that array and the `sph['parttype1', 'masses']` array will not be the same size\n",
      "i see i see\n",
      "but i don't think there is a 'parttype1' radius field\n",
      "i believe there should be, as long as you can get positions for `parttype1` there should be a radius field\n",
      "strange then, i get ytfieldnotfound when i just tried\n",
      "oh, maybe it’s `particle_radius`?\n",
      "always forget particle field naming conventions\n",
      "yes, you are correct!\n",
      "excellent\n",
      "thanks so much\n",
      "no problem!\n",
      "jumping on this a couple of days late--currently we're using the standard sph viz stuff for slices and projections of arepo data. it'd be really nice to eventually use the voronoi mesh for those also\n",
      "sadly this is something i am not an expert on\n",
      "and then every time i mention this to arepo people they just shrug and say even outside of yt they always do sph-type stuff anyway\n",
      "slices would be easier than projections, although still hard\n",
      "we'd need a representation of the 3d voronoi mesh, couldn't do it in the image plane\n",
      "projections would require a voronoi volume renderer\n",
      "which again would need a 3d representation of the voronoi mesh\n",
      "for slices we wouldn't need the mesh, right?  we'd just need to iterate over all possible contributing particles (which should be constrained) and track a second distance field, which we minimize?\n",
      "i.e., a distance plane, and `if distance(particle) &lt; plane[pos]` we set the value?\n",
      "i don't see how you'd be able to regenerate a slice of the mesh that way but i'm bad at visualizing this sort of thing\n",
      "i guess what i'm saying is that the voronoi mesh is an emergent property that comes from computing the nearest particle, and since we'd be doing that at fixed sample points, we would be able to compute it ourselves.\n",
      "<@ucu9e8btm> has joined the channel\n",
      "hello, my regrets for the rookie question. i want to use the `streamlines` function with a 3d vector field. i followed the streamlines tutorial (<http://yt-project.org/doc/visualizing/streamlines.html>), but i cannot figure out how to use this with my own data\n",
      "\n",
      "in my implementation of the tutorial i can see that the `ds` object has fields `(gas, velocity_x)`, `(gas, velocity_y)`, and `(gas velocity_z)` which i assume provide velocity values on a regularly-spaced mesh. however, from which field does the function get the mesh coordinates for each vector?\n",
      "\n",
      "to put this another way: suppose i start with numpy arrays corresponding to (x,y,z) coordinates, and (vx, vy, vz) coordinates. what is the simplest possible structure i can put this into, which would allow me to use streamlines? i'm not working with astrophysical data, just simulation output. thank you very much.\n",
      "<@uu72v7a31> has joined the channel\n",
      "<@ube5d3bll> has joined the channel\n",
      "i am trying to use plugins in a  jupyter notebook. i am following along with the yt \"the plugin file\" and am not able to call functions in my .ipynb....the call \"yt.say_hello()\" doesn't work. i defined def say_hello() in my .py file\n",
      "that’s not how the plugins file works, the functions you define in there won’t show up in the yt namespace\n",
      "where did you get that impression?\n",
      "oh huh, it says in the docs that should work\n",
      "tries\n",
      "hmm so i guess it used to do that a while ago but then when we changed how the plugin file works we didn’t make it inject functions into the yt namespace\n",
      "and you’re the first person to notice\n",
      "or at least to report it\n",
      "<@ube5d3bll> <https://github.com/yt-project/yt/pull/1855>\n",
      "hi, could i have a question about this code, please? why is `x_streamline`\n",
      "empty? the part of code before `###################`\n",
      "is for generating magnetic field and the rest is part with mistake in yt. thank you very much\n",
      "\n",
      "\n",
      "`import numpy as np`\n",
      "`import matplotlib.pyplot as plt`\n",
      "`from numpy import array`\n",
      "`from scipy.interpolate import regulargridinterpolator,interpn`  \n",
      "`from scipy.integrate import solve_ivp                   # runge-kutta method` \n",
      "`import matplotlib as mpl`\n",
      "`import math`\n",
      "`import code`\n",
      "`import yt`\n",
      "`from yt import ytarray  # arrays in yt module`\n",
      "`from yt.visualization.api import streamlines  # force lines`\n",
      "\n",
      "`# constants`\n",
      "`q     =-1.6e-19      # electron charge`\n",
      "`m     = 9.1e-31      # electron mass`\n",
      "`v_par = 2000         # parallel volocity` \n",
      "`v_per = 2000         # perpendicular velocity`\n",
      "\n",
      "`x0=np.zeros(6)      # vector - 6 dim, give zeros into it (3 components - positions, 3 components - velocity)`\n",
      "\n",
      "`# cartesian coordinates`\n",
      "`xmin =-0.15`\n",
      "`xmax = 0.15`\n",
      "`ymin = -0.1`\n",
      "`ymax = 0.1`\n",
      "`zmin = -0.1`\n",
      "`zmax = 0.1`\n",
      "`sampling = 100`\n",
      "\n",
      "`x_ = np.linspace(xmin, xmax, sampling)`\n",
      "`y_ = np.linspace(ymin, ymax, sampling)`\n",
      "`z_ = np.linspace(zmin, zmax, sampling)`\n",
      "\n",
      "`x, y, z = np.meshgrid(x_, y_, z_, indexing='ij')`\n",
      "\n",
      "`assert np.all(x[:,0,0] == x_)`\n",
      "`assert np.all(y[0,:,0] == y_)`\n",
      "`assert np.all(z[0,0,:] == z_)`\n",
      "\n",
      "`# cylindric coordinates (r, theta, z) - transformation` \n",
      "`r_coor = []`\n",
      "`theta_coor = []`\n",
      "`z_coor = []`\n",
      "\n",
      "`for i in range(sampling-1):`\n",
      "    `r = np.sqrt(y_[i]**2 + z_[i]**2)`\n",
      "    `theta = math.atan2(y_[i], z_[i])`     \n",
      "    `z = x_[i]`\n",
      "    `r_coor.append(r)`\n",
      "    `theta_coor.append(theta)`\n",
      "    `z_coor.append(z)`\n",
      "\n",
      "`# computation of magnetic field according to the equations in cylindrical coordinates`\n",
      "`def mag_field(grid):`\n",
      "    `b_theta = np.zeros((sampling-1, sampling-1, sampling-1), dtype=np.float)`\n",
      "    `if grid[2].any() &lt; 0:`\n",
      "        `b_z = grid[0]`\n",
      "        `b_r = 0`\n",
      "    `elif grid[2].any() &lt; np.pi:`\n",
      "        `b_z = 0.25 * grid[0] * np.cos(grid[2]) + 0.75 * grid[0]`\n",
      "        `b_r = (0.25/3) * grid[0] * grid[0] * np.sin(grid[2])`\n",
      "    `else:`\n",
      "        `b_z = 0.5 * grid[0]`\n",
      "        `b_r = 0`\n",
      "    `return (b_r, b_theta, b_z)`\n",
      "\n",
      "`b_r, b_theta, b_z = mag_field(grid=np.meshgrid(r_coor, theta_coor, z_coor))`\n",
      "\n",
      "`bx = b_r * np.cos(b_theta)`\n",
      "`by = b_r * np.sin(b_theta)`\n",
      "`bz = b_z`\n",
      "\n",
      "`#############################################################################################`\n",
      "`# choose point in field where force line will be integrated`\n",
      "`x_point = 0.025`\n",
      "`y_point = 0`\n",
      "`z_point = 0`\n",
      "\n",
      "`# dictionary of numpy arrays - magnetic field data`\n",
      "`data = dict(b1=bx, b2=by, b3=bz)` \n",
      "`bx = data[\"b1\"]`\n",
      "`by = data[\"b1\"]`\n",
      "`bz = data[\"b1\"]`\n",
      "\n",
      "`# 3d array`  \n",
      "`bbox = np.array([[0, 2.5], [-1,1], [-1,1]])                               # border`\n",
      "\n",
      "`ds = yt.load_uniform_grid(data, b_r.shape, length_unit=\"mpc\", bbox=bbox, nprocs=100) # data, dimenze`\n",
      "\n",
      "`# define c: the center of the box, chosen point`\n",
      "`c = ds.arr([x_point, y_point, z_point], 'code_length')`  \n",
      "`c1 = ds.domain_center`\n",
      "`# n is number of streamlines`\n",
      "`n = 1` \n",
      "`# scale is the spatial scale of the streamlines relative to the boxsize`\n",
      "`scale = ds.domain_width[0]` \n",
      "`pos = c`\n",
      "\n",
      "`# create streamlines and integration`\n",
      "`streamlines = streamlines(ds, pos, 'b1', 'b2', 'b3', length=none)` \n",
      "`streamlines.integrate_through_volume()`\n",
      "\n",
      "`for stream in streamlines.streamlines:`\n",
      "    `stream = stream[np.all(stream != 0.0, axis=1)]`\n",
      "\n",
      "`# data of force line`\n",
      "`x_streamline = stream[:,0]`\n",
      "`y_streamline = stream[:,1]`\n",
      "`z_streamline = stream[:,2]`\n",
      "\n",
      "`print(x_streamline)`\n",
      "hi !\n",
      "i tried your script out and it looks to me that your filtering condition in the final for loop is too strict so nothing comes out of it\n",
      "this is not strictly related to yt though, it’s plain numpy array manipulation\n",
      "thank you. so, this is wrong `stream = stream[np.all(stream != 0.0, axis=1)]` ? how to write the filtering better, please?\n",
      "i follow this example: <https://yt-project.org/doc/visualizing/streamlines.html> . could you explain me the condition, please? what is the aim of that line?\n",
      "sorry i’m not familiar with that part of the api, i actually have no idea why this filtering is there but there must be a good reason...\n",
      "thank you for finding where the mistake is.\n",
      "not even sure that it’s there actually, i haven’t been able to reproduce the recipe itself\n",
      "it is solved.\n",
      "great ! how did you solve it ?\n",
      "with\n",
      "`for stream in streamlines.streamlines:`\n",
      "    `stream[np.all(stream, axis=1)]`\n",
      "is there a blessed way to hash an entire dataset? i'm looking at a codebase that uses `yt.funcs.get_hash` but that just ends up hashing the first file if you think you're pointing it at a multi-file snapshot\n",
      "<@u91855pa9> hmm, it's supposed to hash all the files!  i'm surprised it doesn't.\n",
      "i'm on 4.0, dunno if that matters.\n",
      "hmm.  if you report an issue i can come back to that -- i think the original implementer may not be around the community anymore, but i am not sure.\n",
      "curious... just from looking at the source i don't see anything that suggests it would find all the files involved in the snapshot\n",
      "<https://yt-project.org/docs/dev/_modules/yt/funcs.html#get_hash>\n",
      "there is a thing that does this in the demeshening\n",
      "for sph data specifically\n",
      "one sec\n",
      "```\n",
      "in [1]: ds._file_hash\n",
      "---------------------------------------------------------------------------\n",
      "attributeerror                            traceback (most recent call last)\n",
      "~/documents/yt-git/yt/mods.py in &lt;module&gt;\n",
      "----&gt; 1 ds._file_hash\n",
      "\n",
      "attributeerror: 'gadgethdf5dataset' object has no attribute '_file_hash'\n",
      "\n",
      "in [2]:\n",
      "\n",
      "in [2]: ds.index\n",
      "out[2]: &lt;yt.frontends.sph.data_structures.sphparticleindex at 0x10e576c88&gt;\n",
      "\n",
      "in [3]: ds._file_hash\n",
      "out[3]: 249704968950119223\n",
      "```\n",
      "<@u042fh0rb> set the channel topic: general help with yt. please mute if you feel it’s too noisy.\n",
      "<https://github.com/yt-project/yt/blob/yt-4.0/yt/geometry/particle_geometry_handler.py#l268>\n",
      "it’s a method of `particleindex` so that should work for any particle dataset, not just sph\n",
      "we could do that for grid datasets too but someone would need to wire it up\n",
      "scipy has a good implementation of a kdtree that can do this relatively quickly: <https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ckdtree.html>\n",
      "if you want only immediate neighbour, you can probably do it with yt, otherwise, if you want to access non-local data (as in, at an arbitrary distance), zach's suggestion is probably the best\n",
      "is there a way to use `phaseplot` so it is more like a 2d histogram? or is the easier thing to do jsut do a 2d histogram?\n",
      "is the `accumulation` keyword what you are looking for?\n",
      "i seem to recall that there is a function to sample a given field at an arbitrary location in a dataset, but i forget the name and i forget if it works for particle-based datasets.\n",
      "someone is asking me if there is a way to arbitrarily specify an x,y,z location and get out a density-weighted metallicity value for that location.\n",
      "any ideas?\n",
      "ds.point() ?\n",
      "not sure what it means to do a weighted average with a single data point though\n",
      "unless you're talking about sph data\n",
      "i think ds.point() works for sph in the demeshening?\n",
      "yeah, this is for sph data.\n",
      "yeah, i remember ds.point.  i just thought there was something like: “query_field_at_location()”\n",
      "but i guess that is ds.point()\n",
      "there might be a low-level routine that ash would have written\n",
      "ok cool.  thanks!\n",
      "interesting.\n",
      "so `ds.point()` just grabs all of the particles within a smoothing length of the point you specify.\n",
      "yeah, that sounds right\n",
      "i guess this makes sense, as it’s just like the other geometric data objects\n",
      "i guess she’d have to write something more detailed to appropriately apply the smoothing kernel to each of those points to get the interpolated field value at that location\n",
      "yup, it wouldn’t be crazy to have a utility function in yt for that\n",
      "yeah, seems like it could be useful.\n",
      "also would work with gather smoothing too\n",
      "i’m wondering how much would piggy back on what was done in the projection stuff\n",
      "all the low-level machinery is there, just needs to be wired up\n",
      "like this is the function you want for gather: <https://github.com/yt-project/yt/blob/yt-4.0/yt/utilities/lib/pixelization_routines.pyx#l1067>\n",
      "and i guess in principle you could use this function to do scatter on a one zone “grid”\n",
      "<https://github.com/yt-project/yt/blob/yt-4.0/yt/utilities/lib/pixelization_routines.pyx#l1316>\n",
      "or refactor it or whatever so that function calls another function that does scatter smoothing at a single point\n",
      "<@uh593l8tu> has joined the channel\n",
      "i’m actually not entirely clear about this function.  so one would provide a kdtree, with the locations, densities, masses, smoothing lengths and then the field you wanted to query, and it would give you the interpolated value of the field at that location using the gather method?  not sure what the tree_positions are.\n",
      "welcome, <@uh593l8tu>.\n",
      "you could look at the place it’s being used inside yt: <https://github.com/yt-project/yt/blob/yt-4.0/yt/data_objects/construction_data_containers.py#l2335>\n",
      "it’s called tree_positions because it’s used to interpolate onto an octree but it could be an array of arbitrary positions\n",
      "ok, this is useful.  thank you.\n",
      "also <@u9ce5d9lz> might be interested in this discussion, which perhaps might be best to continue in <#cbe6579cz|development> or <#c046hvb59|particles>\n",
      "thanks <@u042j5bn6> for starting the convo, and <@u042fh0rb> for some initial thoughts\n",
      ":slightly_smiling_face:\n",
      "yeah i think all this code exists but could maybe written in a slightly nicer way to make it easy to generalise for a single point \n",
      "definitely.  i may be able to try this down the road, but am swamped right now.  thanks for all the pointing in the right direction, <@u042fh0rb>\n",
      "and the coding, <@u9ce5d9lz>\n",
      "<@uh593l8tu> if you’re interested i think this would be a fun first contribution to yt :slightly_smiling_face:\n",
      "we have `ds.find_field_value_at_point` which i guess we could make use this\n",
      "that is the function i was thinking of!\n",
      "but couldn’t find it.\n",
      "presumably it only works for grid datasets?\n",
      "i think it uses `ds.point`\n",
      "i have no idea what it does with sph datasets offhand\n",
      "looking at it now\n",
      "it’s all grid.\n",
      "i think it would just barf on particle data\n",
      "since it’s using this.leftedge and such\n",
      "yeah\n",
      "doing this would finally justify its existence given that `ds.point` exists now :wink:\n",
      ":slightly_smiling_face:\n",
      "<@u042fh0rb> thanks for the encouragement, i'm down to take on the challenge :slightly_smiling_face: can you think of a good short-yet-sweet example function i could use as a template? i see the yt style guide etc but always work better from examples and haven't worked with yt before\n",
      "maybe as a start you could try refactoring `pixelize_sph_kernel_arbitrary_grid` and `interpolate_sph_positions_gather` so that each if them call another function you’d write that does gather and scatter interpolation at a single point, i’d also check that there aren’t any performance implications of doing that by doing some timing tests with ds.octree and ds.arbitrary_grid with sph data. both of those functions are written in cython in <https://github.com/yt-project/yt/blob/yt-4.0/yt/utilities/lib/pixelization_routines.pyx> on the yt-4.0 branch\n",
      "cython is sort of a combination of python and c but since this is a refactor i think you might be able to do it by example\n",
      "cython has pretty good docs too: <https://cython.readthedocs.io/en/latest/>\n",
      "you’d need to set up a development build of yt by cloning the yt git repository, checking out the yt-4.0 branch, and doing `pip install -e .` in a fresh virtualenv (or however you manage your python environments)\n",
      "there are guides to how to make pull requests and work with git in the developer guide\n",
      "<http://yt-project.org/doc/developing/index.html>\n",
      "since this initial thing would just be a refactor you wouldn’t need to touch tests (proabably) but if you wanted to finish this and add the utility function you’d need to add new tests and docs, which is also explained in the developer guide\n",
      "happy to help out with any of these steps, this is all pretty standard stuff so once you get up and running with yt development setting yourself up to contribute to pretty much any open source project on github will be way easier :slightly_smiling_face:\n",
      "see the <#cbe6579cz|development> channel too :slightly_smiling_face:\n",
      "<@ut5elptaa> has joined the channel\n",
      "hello! hello! i was just looking at the pytest infrastructure pr (<https://github.com/yt-project/yt/pull/2401>) and trying to figure out why the build is failing. it seems (for python 2.7, at least) the coverage command isn't found. also, for the other builds, it seems like my blacklist file for the pytest answer tests didn't do its job and i'm not sure why. any thoughts/help would be greatly appreciated! thanks!\n",
      "i think you need to add coverage to `test_requirements.txt`, probably also `test_minimal_requirements.txt` and `appveyor.yaml`\n",
      "it looks like nose has a --ignore-files command-line option\n",
      "another way to do it would be to make those functions not have names that begin with `test_`\n",
      "even under pytest those aren't really tests, right? just utility functions that might be used by tests\n",
      "thanks! it seems both coverage and codecov are already in test_requirements.txt and test_minimal_requirements.txt (but not appveyor). unless i'm missing something? here is the travis output: <https://travis-ci.org/yt-project/yt/jobs/636545805?utm_medium=notification&amp;utm_source=github_status>\n",
      "so if you look inside .travis.yml, it's setting up a virtualenv and then isntalling a bunch of stuff into that virtualenv, and then running the tests inside it\n",
      "but the post-test step is using the python outside of the virtualenv\n",
      "so i think you just need to install coverage into the global python environment\n",
      "oh hmm, actually nm\n",
      "that's wrong\n",
      "it's dying trying to run the tests\n",
      "ah!\n",
      "the real problem is that the pip install step fails\n",
      "because you're trying to install pytest 5.2.2 on python 2.7\n",
      "you need to expand the \"setup environment\" step in the travis output\n",
      "<@ut5elptaa> ^\n",
      "ohh! gotcha, thanks!\n",
      "probably would be better if our .travis.yml errored out completely if a pip install fails\n",
      "<https://stackoverflow.com/questions/22250483/stop-pip-from-failing-on-single-package-when-installing-with-requirements-txt>\n",
      "ah no, that's not quite what we want, we probably need to check the return code from \"pip install\" and execplitly kill the script if it returns an error\n",
      "i added pytest==4.6; python_version &lt; '3.0' and pytest==5.2; python_version &gt;= '3.0' to the test_requirements.txt and test_minimal_requirements.txt files. i also removed the _test suffix, as you suggested. hopefully all is well, now!\n",
      "i think you want those to be single equal signs\n",
      "pytest==4.6 is 4.6.0\n",
      "and you probably want the bugfix releases\n",
      "pytest=4.6 resolves to 4.6.9 atm\n",
      "(the newest 4.6 release)\n",
      "hi, yes, that’s exactly right. the gadget fof/subfind catalogs refer to those made by gadget’s inline halo finder. there may be a standalone version of that, but i’m unaware of it.\n",
      "\n",
      "unfortunately, the yt halo finders don’t write out the particle information for each halo right now. i recently made this work in a fork, so i can push that. after that, there still is the matter of properly accessing that information in yt, and that machinery doesn’t yet exist. this may be something i try to do in the near future. if you’re interested in helping out, please let me know.\n",
      "thanks for your help, and looking forward to your work.\n",
      "<@uc8flf97u> has joined the channel\n",
      "<@uc9rgj4ty> has joined the channel\n",
      "<@upyhr9l8g> has joined the channel\n",
      "in yt 2.x there was some fields called 'totalmass' and 'dark_matter_mass'. is there any equivalent one in yt 3.5? i am familiar with the fields \"cell_mass\", and \"particle_mass\". by adding \"cell_mass\", and \"particle_mass\", one can calculate total mass. but i need to calculate star particle mass. dose any one know how to get star particle mass?\n",
      "<#c2c9d1rv5|enzo>\n",
      "you need to make a particle filter for the star particle type\n",
      "and then calculate the total particle mass for that particle filter\n",
      "thanks <@u042fh0rb> :slightly_smiling_face:\n",
      "this example will likely be helpful: <https://yt-project.org/doc/analyzing/filtering.html#filtering-particle-fields>\n",
      "hello again\n",
      "im trying to do the tutorial from jupyter notebook and when i tried to download the datasets it came up with this when i ran the program\n",
      "\n",
      "im not sure how to fix this as this was exactly what was written in the instructions for the tutorial\n",
      "ah, that's a bug in the notebook\n",
      "there should be an exclamation mark before the tar command\n",
      "like the other ones\n",
      "ah perfect\n",
      "ok i've run it but nothing is printing..\n",
      "it's probably downloading the data\n",
      "ok cool thank you !\n",
      "so i tried it on a random jupyter notebook that isn't linked to yt and it completed running really fast - why is it that on this one its slower? or am i doing something wrong (probably)\n",
      "the notebook that you're running downloads about 10 gb of data\n",
      "on a slow network connection that might take a long time\n",
      "so its ok that its still running..?\n",
      "oh i just got your second message  ok ok my internet is clearly vvvvvv slow\n",
      "iirc with gadget non-cosmological sims you need to set the bounding box at load time, we fixed this on the yt-4.0 branch which you might want to try out\n",
      "if you look in the docs on loading gadget data there’s an example of how to set the bounding box manually\n",
      "<@uu72v7a31> checkout box 2 here:\n",
      "\n",
      "<https://yt-project.org/doc/cookbook/gadget_notebook.html>\n",
      "well, i know how to set the bounding box, but i don't know why only one snapshot file has this error, all the snapshot files are come from the same one cosmological simulation, i think they have the same boxsize. for another snapshot files, there's @no error when i laod them, even i don't set the bounding box. so why for this one snapshot, the particle bound exceed the default bounding box which i think is the boxsize ?\n",
      "strange, any chance you can share the data file?\n",
      "well,  i update yt and its dependencies, then problem solved, thanks for your help. <@u042fh0rb> <@u046k2qnk>\n",
      "what is the simplest way to get access raw data of a field in the entire domain?\n",
      "is this what you’re looking for ?\n",
      "```ad = ds.all_data()\n",
      "field_data = ad[&lt;field_key&gt;] ```\n",
      "thank you. it is what i was looking for.\n",
      "<@u014a7md2lt> has joined the channel\n",
      "hello everyone and thank you for accepting me in your forum (or whatever i should call this workspace). since this “space” is for asking questions i will dive straight into it. i am currently working on a structure formation project and i’m analyzing data from the dark sky simulations project. this is a cosmological simulation having only dark matter particles. so what i want to do is to use the rockstar halo finder in order to identify and analyze sub-halos that constitute the bigger halos given to me by the halo catalog produced from the aforementioned project. now, when i run the rockstar algorithm it returns a “dataset” like object having some fields, like particle_position_x particle_velocity_x etc. so regarding these fields i was wandering how were they produced. for example, the position is given as the position of the center of mass of the particles in this particular subhalos? and the velocity is it mass weighted or not? thank you in advance for your reply.\n",
      "<@uelt6g0f6> welcome!  i believe the coordinates that rockstar has a pretty complicated algorithm for determining the centroid of halos.  i believe that it will be mass-weighted for both spatial and velocity coordinates.  but this isn’t really a yt question.  i would read over the rockstar documentation / method paper to see exactly how these values are calculated.\n",
      "thank you very much for your reply. well you are right, as it stands the question is not very much related to yt. but yt gets involved in the next step. that is, when i invoke the halo finder to find the subhalos i then use the halocatalog object of yt to analyze the results. in particular i feed the halocatalog with the rockstar output and the original data set of the whole halo and apply some filters; for example i ask for halos with mass &gt; 10^13 solar masses. here is where my real problem starts. i compare the particle identifier field of the rockstar_ds and the catalog_ds and find that indeed every id in the catalog_ds already exists in the rockstar_ds as it should. but when i do this for the particle position field or the particle velocity fields i find a discrepancy. for example only half of the values in the catalog_ds match the ones in rockstar_ds. so i guess that when i used the halocatalog object it changed some of the values in position and velocity. i could just proceed and analyze the output of the halo catalog but i want to be sure that it has not changed something under my nose. i hope i am not confusing you. sorry for the long question but i really don’t now how to phrase it better.\n",
      "<@u042hlt7u> thanks for the suggestion. i found that a python visualization package, `plotly` will make this easy. i would like to visualize unstructured mesh data so, i think, it will be possible to export slice images in gray with colorbar range matching using `yt` and align those images in 3d space with `plotly`.\n",
      "<@ulqt4pxbp> that sounds cool.  can you show me, once you have a working version?\n",
      "<@u042hlt7u> definitely i will!\n",
      "interesting.  i’m not sure the answer to your question, but <@u042s6y2g> will likely know since he implemented most of the halo functionality in yt.\n",
      "<@u97q2bvfz> has joined the channel\n",
      "hi, i am new to using yt and am running into some trouble/questions. i am currently trying to extract isosurfaces (using the function `extract_isocontours()`) from a 3d mesh with data tagged on each cell and i am running into some trouble. i am starting with a pyne mesh with tagged data. i can successfully load in the data using `mesh = yt.load(filename)`. from there it appears to be type `moabhexdataset`. i can also then do `d = mesh.all_data()` and it appears to then convert `d` to type moabhex8mesh. i am running into an issue then when i try to use `d.extract_isocontours(\"ww_n\", 0.002, \"test.obj\", true)`. i get an error `'moabhex8mesh' object has no attribute 'get_vertex_centered_data'`. so my question is, is there a way to go from the dataset i do have to some dataset that can be used with `extract_isocontours()`? or is there a better way to get isosurface data from my mesh? thanks!\n",
      "for reference i was trying to follow this example: <http://yt-project.org/docs/dev/reference/api/yt.data_objects.construction_data_containers.html?highlight=extract_isocontours#yt.data_objects.construction_data_containers.ytarbitrarygrid.extract_isocontours>\n",
      "hi kalin\n",
      "unfortunately right now there’s no support in yt for extracting isocontours from pyne meshes\n",
      "i’m not sure offhand how hard it would be to add support for it\n",
      "sorry to not have more helpful advice :confused:\n",
      "that was my suspicion. do you know offhand if there is any support for converting a pyne mesh into a different type of mesh that is supported?\n",
      "one thing you could do is interpolate your data to a 3d uniform resolution grid\n",
      "using e.g. ds.arbitrary_grid\n",
      "and then reload that data using load_uniform_grid\n",
      "i don’t know what your data looks like, but if it has lots of fine detail but also lots of empty regions then that might be expensive\n",
      "hmm, i will try that. thanks!\n",
      "hi all! i am a first-year grad student at the university of michigan doing some work with yt.  i’m trying to create a 1d profileplot with the y-axis as a derived field that returns the result `numpy.apply_along_axis(function, 0, array)` , where each element of `array`  is a yt field converted to an ndarray and `function`  is a previously defined python function, and the x-axis as just temperature.  however, yt gives me the error: `runtimeerror(\"ytquantity instances must be scalars\")` .  i’ve been able to make sliceplots with the values of the derived field, so i know the field is giving numerical values (with units).  does anyone know what can be done about that type of error?\n",
      "hi, can i ask if volume rendering is available for the ramses grid data now? i still got the error “amrkdtree does not support particle or octree-based data.”\n",
      "<@ubju11gju> i think right now the answer is, sadly, still no.\n",
      "this is one that we made a lot of progress on, but then had to stop due to lack of resources.  :disappointed:\n",
      "anyway, it is good to know that.\n",
      "thanks\n",
      "hi! any suggestions about how to set up a python environment to switch between python 2 and 3 and to test yt?\n",
      "yes!   \n",
      "\n",
      "```\n",
      "conda create --name mypy2env python=2.7\n",
      "```\n",
      "\n",
      "then to turn on\n",
      "\n",
      "```\n",
      "source activate mypy2env\n",
      "```\n",
      "\n",
      "and to turn off:\n",
      "\n",
      "```\n",
      "source deactivate mypy2env\n",
      "```\n",
      "\n",
      "(h/t to <@u042fh0rb> for showing me this a few weeks ago!)\n",
      "thanks! i suppose i need to install yt for different pythons separately.\n",
      "i'm no expert and  there might be some cool way of linking installations...but yeah i just re-installed everything in a py2 environment as well\n",
      "got it. i’ll give it a try. thanks!\n",
      "you can just have both installed simultaneously, it’s ok to do that. just call the executables python3 and python\n",
      "since i want to built yt from source using ‘python setup.py develop’, should i simply create two yt root directories and compile them separately?\n",
      "ah i see what you’re saying. yes, that should work if you want to quickly switch. but make sure you’re using the right `python` in the setup.py call :)\n",
      "i think matt has some shell shortcuts to switch which one is “activated”\n",
      "i just have both simultaneously installed\n",
      "i’ll try it. thanks!\n",
      "as an update, i was able to spoof a background color, by drawing a set of three thousand opaque lines and positioning them appropriately in the image. although this was very difficult as orienting them without being able to view the axes of the 3d volume was confusing. specifically the default view cube is degenerate in the typical illusion sense about which side is front or back, up or down. also, i had to modify the line object opacities, as opaque lines would block the view of the density in the interior of the cube, even if they were positioned “behind” the density. is that expected behavior? not your typical yt image i imagine, but i have attached it for reference. a komolgorov power spectrum.\n",
      "yeah, it’s physically motivated ray tracing so if the opacity reaches unity anywhere along the line of sight you’ll wash out that pixel\n",
      "cool!\n",
      "glad to hear you figured out a workaround\n",
      "that bodes well for us actually be able to fix it once and for all\n",
      "if you comment on the issue with a script demonstrating your “fix” that might help us figure out a real solution\n",
      "apologies for the broken behavior, i realize that’s an important issue for publication quality graphics\n",
      "we just have lots of other fires that need putting out :confused:\n",
      "also for the volume rendering infrastructure in particular, it was written by someone who left the field\n",
      "so none of us know the code very well, especially the low level parts\n",
      "so it’s a tough issue to fix in that context\n",
      "not to just make a bunch of excuses of course, but just to give you some context about why it’s hard to fix\n",
      "that is good to know, thank you! i’ll share the script in the github thread, maybe someone will find it useful, though i doubt it will lead to a real solution, as it was very much hacked together. i appreciate you sharing the context for why things behave the way they do, its totally understandable. just know that this is still the best 3d rendering library i’ve found to integrate with python! though it may be worthwhile to at least add a comment on the page with the cookbook saying that this behavior is broken at the moment.\n",
      "hi! is there a way to do a particle union of filtered particles?\n",
      "does that not work?\n",
      "it should i think\n",
      "i mean, defining the particle union just as you’d define any other particle union, assuming the filter has already been defined\n",
      "this line <https://github.com/yt-project/yt/blob/3dd357861ff89d8417efb7c42941cf5cfc1b143c/yt/data_objects/static_output.py#l624> prevents it\n",
      "i guess try replacing that with `self.particle_types` instead of `self.particle_types_raw`. i don’t know offhand what would break.\n",
      "but ` f = self.particle_fields_by_type` has only keys for raw particles, right?\n",
      "like i said, i don’t know offhand what would break\n",
      "i guess the answer to your question is that no one has tried to implement it yet\n",
      "i don’t see any particular reason why this shouldn’t work but it looks like the current implementation was written without this use case in mind\n",
      "ok fair enough :slightly_smiling_face:\n",
      "i think i managed to find where the missing part is but i'm a bit lost\n",
      "if i force yt to also accept derived fields, there is an issue in  `io_handler.py:201` because the code tries to read derived fields as if they were raw fields\n",
      "and hence fails\n",
      "what’s the traceback?\n",
      "probably that needs to be explicitly handled at a higher level somewhere\n",
      "i opened an issue\n",
      "<https://github.com/yt-project/yt/issues/1932>\n",
      "so `gas_tracer` and `star_tracer` are particle filters defined by the ramses frontend?\n",
      "yes\n",
      "it’s going to probably be somewhere in `get_data`, there’s like an assumption somewhere in there that all the fields that make up a particle union are on-disk fields, which you’re breaking\n",
      "in particular, the fact that it’s going into `_read_particle_fields` indicates that\n",
      "i’d try to see what happens in that function when you pass it a particle filter field, it should be going down the same code path when you pass it a particle union\n",
      "at least for the parts of the union that are fields associated with a particle filter\n",
      "the issue is around here i guess <https://github.com/yt-project/yt/blob/0ce549f83a25e31cb9897349678141fb0f988553/yt/utilities/io_handler.py#l184-l199>\n",
      "in the case of a union made with raw particles, ptf contains a raw particle field\n",
      "i’d be a little surprised if the issue was there actually\n",
      "at that point we should only be dealing with fields that we actually need to read from disk\n",
      "but i might be wrong\n",
      "the lines i sent are finding all the fields that are making up the union and read-them, expecting them to be on disk\n",
      "(<https://github.com/yt-project/yt/blob/0ce549f83a25e31cb9897349678141fb0f988553/yt/utilities/io_handler.py#l178>)\n",
      "so i guess we also need to add logic to `ytdatacontainer.get_data` that handles the case where particle unions might be made up of particle filters as well as on-disk fields.\n",
      "i think\n",
      "again, i’d look at how i/o for particle filter fields work in that function\n",
      "i don’t know offhand how that works, i’d need to test and poke at that function with a debugger\n",
      "i've set up a minimal version that doesn't crash anymore\n",
      "it does so by reading from disk only raw particles types\n",
      "now i have to figure out how to get `ytdatacontainer.get_data` to append the relevant data\n",
      "is there a reason why `ds._get_field_info(('tracer', 'particle_family')) == on-disk field`\n",
      "should it not be? i don’t know enough about the ramses frontend or how those fields are set up.\n",
      "well `tracer` is the particle union\n",
      "so i'd expect not to be `on disk`\n",
      "ah ok, getting closer\n",
      "i added a test in `geometry_handler:index._split_fields` to filter out derived fields\n",
      "it’s because `dataset._setup_particle_type` assumes it’s always setting up an on-disk field\n",
      "(that function gets called by `dataset.add_particle_union`)\n",
      "to be precise, what is exactly an on-disk field?\n",
      "a field that we can read directly from disk\n",
      "without doing any processing on it\n",
      "so it should in principle exclude fields that are unions of derived fields?\n",
      "of particle filters, yes\n",
      "hi, is it possible to load two cubes (fits file, matching size), into the same scene while doing the volume rendering? the idea is to compare the two cubes\n",
      "sure, you need to create two volumesource instances, one for each dataset\n",
      "like this example <http://yt-project.org/doc/cookbook/complex_plots.html#volume-rendering-multiple-fields>\n",
      "note that i’ve never tried to do what you’re doing, there might be issues i’m not aware of\n",
      "the \"volume rendering multiple fields\" example?\n",
      "yup, the one i linked to\n",
      "that example shows how to do what you want with two fields from the same dataset\n",
      "but it should work with two fields from different datasets, assuming the units are the same\n",
      "i do have to match the units before that. i'll follow that example and give it a try\n",
      "thank you so much!\n",
      "hey all - i have a simple question about halo finding.  i want to load a `gizmo` ds, and find \"halos\" of gas particles only\n",
      "i try something like:\n",
      "```\n",
      "n [2]: ds = yt.load('snapshot_100.hdf5')\n",
      "yt : [info     ] 2018-08-15 20:03:28,448 calculating time from 4.444e-01 to be 1.585e+17 seconds\n",
      "yt : [info     ] 2018-08-15 20:03:28,449 assuming length units are in kpc/h (comoving)\n",
      "yt : [info     ] 2018-08-15 20:03:28,467 parameters: current_time              = 1.5852279102079226e+17 s\n",
      "yt : [info     ] 2018-08-15 20:03:28,467 parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [info     ] 2018-08-15 20:03:28,467 parameters: domain_left_edge          = [0. 0. 0.]\n",
      "yt : [info     ] 2018-08-15 20:03:28,467 parameters: domain_right_edge         = [50000. 50000. 50000.]\n",
      "yt : [info     ] 2018-08-15 20:03:28,468 parameters: cosmological_simulation   = 1\n",
      "yt : [info     ] 2018-08-15 20:03:28,468 parameters: current_redshift          = 1.2500002398009817\n",
      "yt : [info     ] 2018-08-15 20:03:28,468 parameters: omega_lambda              = 0.7\n",
      "yt : [info     ] 2018-08-15 20:03:28,468 parameters: omega_matter              = 0.3\n",
      "yt : [info     ] 2018-08-15 20:03:28,468 parameters: hubble_constant           = 0.68\n",
      "\n",
      "in [3]: from yt.analysis_modules.halo_analysis.api import halocatalog\n",
      "   ...:\n",
      "\n",
      "in [4]: hc = halocatalog(data_ds = ds,finder_method='fof',finder_kwargs={\"ptype\":\"gas\"}\n",
      "   ...: )\n",
      "hc.create()\n",
      "```\n",
      "but then it barfs with:\n",
      "```\n",
      "in [5]: hc.create()\n",
      "yt : [warning  ] 2018-08-15 20:04:58,628 dm_only is deprecated.  use ptype to specify a particle type, instead.\n",
      "---------------------------------------------------------------------------\n",
      "runtimeerror                              traceback (most recent call last)\n",
      "&lt;ipython-input-5-6b9262d90adc&gt; in &lt;module&gt;()\n",
      "----&gt; 1 hc.create()\n",
      "\n",
      "~/yt/yt/analysis_modules/halo_analysis/halo_catalog.py in create(self, save_halos, save_catalog, njobs, dynamic)\n",
      "    335\n",
      "    336         \"\"\"\n",
      "--&gt; 337         self._run(save_halos, save_catalog, njobs=njobs, dynamic=dynamic)\n",
      "    338\n",
      "    339     def load(self, save_halos=true, save_catalog=false, njobs=-1, dynamic=false):\n",
      "\n",
      "~/yt/yt/utilities/parallel_tools/parallel_analysis_interface.py in barrierize(*args, **kwargs)\n",
      "    299     def barrierize(*args, **kwargs):\n",
      "    300         if not parallel_capable:\n",
      "--&gt; 301             return func(*args, **kwargs)\n",
      "    302         mylog.debug(\"entering barrier before %s\", func.__name__)\n",
      "    303         comm = _get_comm(args)\n",
      "\n",
      "~/yt/yt/analysis_modules/halo_analysis/halo_catalog.py in _run(self, save_halos, save_catalog, njobs, dynamic)\n",
      "    404         if self.halos_ds is none:\n",
      "    405             # find the halos and make a dataset of them\n",
      "--&gt; 406             self.halos_ds = self.finder_method(self.data_ds)\n",
      "    407             if self.halos_ds is none:\n",
      "    408                 mylog.warning('no halos were found for {0}'.format(\\\n",
      "\n",
      "~/yt/yt/analysis_modules/halo_analysis/halo_finding_methods.py in __call__(self, ds)\n",
      "     42\n",
      "     43     def __call__(self, ds):\n",
      "---&gt; 44         return self.function(ds, *self.args, **self.kwargs)\n",
      "     45\n",
      "     46 def _hop_method(ds, **finder_kwargs):\n",
      "\n",
      "~/yt/yt/analysis_modules/halo_analysis/halo_finding_methods.py in _fof_method(ds, **finder_kwargs)\n",
      "     59     \"\"\"\n",
      "     60\n",
      "---&gt; 61     halo_list = fofhalofinder(ds, **finder_kwargs)\n",
      "     62     halos_ds = _parse_old_halo_list(ds, halo_list)\n",
      "     63     return halos_ds\n",
      "\n",
      "~/yt/yt/analysis_modules/halo_finding/halo_objects.py in __init__(self, ds, subvolume, link, dm_only, ptype, padding)\n",
      "   1605             raise runtimeerror(\n",
      "   1606                 \"if dm_only is true, ptype must be none.  \" + \\\n",
      "-&gt; 1607                 \"dm_only must be false if ptype is set.\")\n",
      "   1608\n",
      "   1609         if ptype is none:\n",
      "\n",
      "runtimeerror: if dm_only is true, ptype must be none.  dm_only must be false if ptype is set.\n",
      "```\n",
      "so i see that `dm_only` as a keyword is deprecated.    i set `ptype` as \"gas\" -- it's possible i need to set it as `parttype0` or something but i can't quite figure out what that setting ought to be\n",
      "<@u046k2qnk> `finder_kwargs={'ptype': 'parttype0', 'dm_only': false}` works when i tried\n",
      "ah! i think i tried the `dm_only` part as a keyword and not a `finder_kwarg`.   thanks <@u0860sxlk>!\n",
      "does `annotate_contour` work for `particleplot`? i’m trying to do something like\n",
      "\n",
      "```   p = yt.particleplot( ds, 'particle_position_x', 'particle_position_y', 'particle_mass' )\n",
      "   p.annotate_contour( \"density\" )```\n",
      "but get the error `attributeerror: 'particleprojectionplot' object has no attribute 'annotate_contour'`\n",
      "i’m trying to add velocity quivers on a projection plot (which i have done before but naturally can’t find a working example of…). tried mimicking <https://yt-project.org/doc/visualizing/callbacks.html> but i’m clearly missing something … suggestions? the projection is made doing `p = yt.projectionplot(ds, 'y', ('gas','temperature'), weight_field=(\"gas\",\"density\"))` with centers and things specified\n",
      "<@u0hhgt2v9> i don't think so right now, no\n",
      "<@u4esetp43> any chance you can make a runnable example that triggers that error, using one of the data files from <http://yt-project.org/data|yt-project.org/data>?\n",
      ":wait:\n",
      ":sobs-over-lack-of-emoji-here:\n",
      "<@u042fh0rb> thanks for the quick response! i guess i can create a similar image using the `cic` field. or do you have any other suggestion (even if it requires some hard coding)?\n",
      "it works (well, i get fugly overdrawn lines :upside_down_face:) for the isolated galaxy version\n",
      "are the scripts identical?\n",
      "if there's some difference that might be causing the error, one possibility was you called annotate_quiver incorrectly earlier in your notebook\n",
      "and you're seeing the error for that incorrect call\n",
      "effing notebooks\n",
      "it's a little confusing because the plot callbacks don't actually get called until the plot is drawn\n",
      "ok, let me it from scratch then\n",
      "(part of the whole doing this in a notebook thing was because the making the projection takes nearly a minute and then i forget i’m doing it…)\n",
      "big simulation fun times\n",
      "also hey molly :wave:\n",
      ":wave:\n",
      "yes! ok so remaking the projection plot worked. is there a way to now remove the quivers i have / edit them so i can make modifications without regenerating the entire thing again and again?\n",
      "`plot.annotate_clear()`\n",
      "ooooh\n",
      "`typeerror: __init__() got an unexpected keyword argument 'plot_args'` ?\n",
      "`p.annotate_quiver('x-velocity', 'z-velocity', factor=32, normalize=true, plot_args={\"color\": \"white\"})`\n",
      "(copied from the cookbook …)\n",
      "i don't think there's any reason why it couldn't use the contour callback, it just hasn't been wired up, it might be as simple as just removing callback from the blacklist, one sec to point at the place in the code\n",
      "seems like it should work to me, are you maybe using a yt version older than 2018?\n",
      "<https://github.com/yt-project/yt/blob/master/yt/visualization/plot_window.py#l1083>\n",
      "ugh\n",
      "yes\n",
      "let me give it a try\n",
      "3.4.1\n",
      "updated it on one computer but not all ……..\n",
      "ok, thanks :slightly_smiling_face:\n",
      "dear yt-users,\n",
      "i need to pass the value of the available/derived field corresponds to the index selected based on some criterion (using numpy.where()) to a scalar function. it should list out the indexes wheres the condition satisfies. it happens when this has been done outside the field but instead of getting 3d indexes it gives 1d yt_array indexes i think.\n",
      "but i need to pass this value of field corresponds to the selected index to scalar function inside the field, which may give some problem (please find the attachment to see what exactly i am trying to do).\n",
      "the attached script can run on the enzo test data set, enzo_tiny_cosmology.\n",
      "many thanks and regards,\n",
      "here the script.\n",
      "got the following error:\n",
      "\n",
      "```traceback (most recent call last):\n",
      "  file \"plot__particle.py\", line 68, in &lt;module&gt;\n",
      "    p.save( prefix_out+'_'+ds.basename+'.png', mpl_kwargs={'dpi':dpi} )\n",
      "  file \"/work1/fish/software/yt/yt.py2/yt/visualization/plot_container.py\", line 92, in newfunc\n",
      "    args[0]._setup_plots()\n",
      "  file \"/work1/fish/software/yt/yt.py2/yt/visualization/plot_window.py\", line 1070, in _setup_plots\n",
      "    self.run_callbacks()\n",
      "  file \"/work1/fish/software/yt/yt.py2/yt/visualization/plot_window.py\", line 1127, in run_callbacks\n",
      "    sys.exc_info()[2])\n",
      "  file \"/work1/fish/software/yt/yt.py2/yt/visualization/plot_window.py\", line 1121, in run_callbacks\n",
      "    callback(cbw)\n",
      "  file \"/work1/fish/software/yt/yt.py2/yt/visualization/plot_modifications.py\", line 75, in _check_geometry\n",
      "    return func(self, plot)\n",
      "  file \"/work1/fish/software/yt/yt.py2/yt/visualization/plot_modifications.py\", line 620, in __call__\n",
      "    if self.take_log: zi=np.log10(zi)\n",
      "yt.utilities.exceptions.ytplotcallbackerror: annotate_contour callback failed with the following error: local variable 'zi' referenced before assignment```\n",
      "i’ll play with it a bit further\n",
      "well i guess that's why it was on the blacklist :slightly_smiling_face:\n",
      "most of the entries on the blacklist could be fixed, someone just needs to do the work\n",
      "no problem. i can give it a try. thanks again for the help :slightly_smiling_face:\n",
      "i’ll file an issue first if that’s ok\n",
      "sure\n",
      ":+1:\n",
      "another way around this blocker for you would be to get the image from the frb attribute and then make the plot manually in matplotlib\n",
      "i think `particleplot` uses the frb interface anyway\n",
      "`image = plot.frb[field]`\n",
      "good suggestion. thanks!\n",
      "hi prateek, not sure what you are exactly trying to achieve…do you want to have a field which will take certain values given another field? then maybe you can direclty add the field and not take the detour via the indices…?\n",
      "does anyone else have a problem when setting `njobs` explicitly in `parallel_objects` ? if i run\n",
      "\n",
      "```for i in yt.parallel_objects(range(10), njobs = x):\n",
      "    print(i)```\n",
      "for `x=0` everything works fine but for `x&gt;0` (and `x != n` where `n` is the processors i run the script on) every thread will `print(i)` , i.e., i am getting several zeros etc.\n",
      "sounds like a bug to me\n",
      "i am wondering if it is just my mpi or everybody has this issue…\n",
      "i dunno, let me try\n",
      "it seems to be working for me\n",
      "```\n",
      "$ cat test.py\n",
      "import yt\n",
      "yt.enable_parallelism()\n",
      "\n",
      "for i in yt.parallel_objects(range(10), njobs = 2):\n",
      "    print(i)\n",
      "\n",
      "$ mpirun -n 4 python test.py\n",
      "yt : [info     ] 2019-10-31 14:20:00,857 global parallel computation enabled: 2 / 4\n",
      "yt : [info     ] 2019-10-31 14:20:00,857 global parallel computation enabled: 3 / 4\n",
      "yt : [info     ] 2019-10-31 14:20:00,857 global parallel computation enabled: 0 / 4\n",
      "yt : [info     ] 2019-10-31 14:20:00,857 global parallel computation enabled: 1 / 4\n",
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "0\n",
      "1\n",
      "3\n",
      "5\n",
      "9\n",
      "7\n",
      "9\n",
      "0\n",
      "2\n",
      "2\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "8\n",
      "8\n",
      "```\n",
      "there are two parallel for loops with 4 mpi processes\n",
      "and the output is as expected, i think\n",
      "as expected…? you get multiple times every number\n",
      "that’s the same output i get but i thought that one will get every number just once (as you obtain if you set `njobs=0`\n",
      "you get two of them\n",
      "one for each of `njobs`\n",
      "maybe i'm wrong about how `parallel_objects` should work?\n",
      "i don't really remember, it's been several years since i've used it\n",
      "ok maybe i just completely misunderstood the example here <https://yt-project.org/docs/dev/analyzing/parallel_computation.html#multi-level-parallelism> i thought the sentence “…if the inner parallel_objects() call were removed from the loop, the two-processor work group would work together to project each of the density and temperature fields. this is because the projection functionality itself is parallelized internally.” means that then `p.save('figures/')` is called only once\n",
      "you might very well be right, i don't think `parallel_objects` is commonly used without `njobs=0`, you might want to look at the source code and see if you can understand how it works\n",
      "i think everything is working properly. since you have `njobs=2.`you have two processors working on each loop and double outputting. if you have `njobs=1` , then 4 outputs for every number (assuming you are running it with 4 processors). setting `njobs&gt;1` doesn’t really seem that effective though from some of the things i have been doing, maybe even slowing because of communication overhead, unless the problem is massive that i am doing on each step, like extracting a bunch of uniform volumes.\n",
      "<@u04rl26hx> has joined the channel\n",
      "thanks for your feedback to kiran on issue #1921 -- he's an ug student working with me\n",
      "<@ubykep7ek> has joined the channel\n",
      "trying to build some intuition of the current state: `find_field_value_at_point` has the functionality of evaluating a field at an arbitrary point in a simulation but currently only works with grid codes, not sph. we can get that functionality with sph data if i refactor (1) `pixelize_sph_kernel_arbitrary_grid` and (2) `interpolate_sph_positions_gather`.  the documentation for `interpolate_sph_positions_gather` says\n",
      "&gt; this function takes in arbitrary positions, field_positions, at which to perform a nearest neighbor search and perform sph interpolation.\n",
      "that sounds like what i want, but you're suggesting it can't do it at an arbitrary position?\n",
      "the documentation for `interpolate_sph_grid_gather` says\n",
      "&gt; this function takes in the bounds and number of cells in a grid (well, actually we implicity calculate this from the size of buff). then we can perform nearest neighbor search and sph interpolation at the centre of each cell in the grid.\n",
      "i'm struggling to see the differences in their functionality :thinking_face:\n",
      " <@u042fh0rb> can you say a bit more about what gather and scatter interpolation is?\n",
      "also, should i move this conversation to <#cbe6579cz|development>?\n",
      "for gather scatter see <http://adsabs.harvard.edu/full/1989apjs...70..419h>\n",
      "thanks <@uggk0erpa>  for the reference\n",
      "i have a gizmo snapshot of a zoomed galaxy and want to rotate and translate into the frame of the high res galaxy. i have the coordinates, rotation matrix and code to do the rotation/translation, but how do i access the coordinates to change them? specifically in such a way that i can then do visualization stuff\n",
      "to my knowledge, we don’t have a mechanism for changing the in-memory coordinate system from one to the other.  however, it would be relatively straightforward to write a function in python that could just do the trig to convert from one to the other, i think.\n",
      "i guess you could make derived fields for the new coordinate system, but not change the existing coordinates on disk.\n",
      "to access the original coordinates, just treat them like any other spatial field:\n",
      "\n",
      "```\n",
      "ad = ds.all_data()\n",
      "ad['('gas', 'x')]\n",
      "```\n",
      "you can set the field parameters associated with a specific data object (in this case, `all_data()` returns a `region` data object).  so you could set the “center” of your galaxy by:\n",
      "\n",
      "```\n",
      "ad = ds.all_data()\n",
      "ad.set_field_parameter('center', [x_center, y_center, z_center])\n",
      "```\n",
      "you can also set the normal vector for a data object this way using the `set_field_parameter('normal', [x_normal, y_normal, z_normal])` function.\n",
      "it’s straightforward to get a rotation matrix in yt: <http://yt-project.org/docs/dev/reference/api/yt.utilities.math_utils.html#yt.utilities.math_utils.get_rotation_matrix>\n",
      "as for visualization, you can use the `offaxisprojectionplot` and `offaxissliceplot` to do projections from an arbitrary normal vector to an arbitrary center location.\n",
      "i hope this helps.  others may be able to chime in and offer other suggestions i have missed.\n",
      "awesome, thanks <@u042j5bn6>\n",
      ":slightly_smiling_face:\n",
      "hi <@uu72v7a31>, `treefarm` is something i wrote, so i’m happy to help you with this. unfortunately, i’m in an all day review meeting today, but i will get back to you tomorrow.\n",
      "also, please ping me here if you haven’t heard back after tomorrow. i forget things quite easily.\n",
      "thanks for your reply, i think i know what happens here. in treefarm, halo catalogs must be in the form created by the gadget fof halo finder or subfind substructure finder, but my halo catalogs were created in yt's 'halocatalog', i thought they are the same, but actually not.\n",
      "and do you know how to get gadget fof/subfind halo catalogs, can i get them in yt? or is there another code or somthing, i can't find it online.\n",
      "attached is a poloidal slice through a 3-d disk simulation i am studying.  the x-axis of the `sliceplot` gives radius from the parent object; the y-axis gives elevation in the disk.  the simulation data is on a 3-d cartesian mesh.  does `yt` have an infrastructure to perform an azimuthal (`cylindrical_theta`) average of this plot?  `projectionplot`  can use the `weight_field` keyword to average along the x, y, or z dimension; `create_profile` can perform azimuthal averages of data objects (after supplying `center` and `normal` field parameters) to produce 1-d profiles; but neither of these are exactly what i am after.  as always, thanks in advance for all the support~\n",
      "<@uhkuhfybf> i *believe* it can.  i think a phase plot could do this, where the binning fields were z and cylindrical theta.\n",
      "ooh…let me try!\n",
      "thx <@u042hlt7u>\n",
      "thanks again, <@u042hlt7u>.  looks great~  if anyone else is interested:\n",
      "```# define cylinder\n",
      "my_cylinder = ds.disk(my_center_of_mass, [0,0,1], (5., 'r_earth'), (5./2., 'r_earth'))\n",
      "\n",
      "# create profile\n",
      "profile = yt.create_profile(\n",
      "    data_source=my_cylinder,\n",
      "    bin_fields=[\"cylindrical_radius\", \"cylindrical_z\"],\n",
      "    fields=[\"specific_energy\"],\n",
      "    n_bins=[140,140],\n",
      "    units=dict(cylindrical_radius=\"r_earth\",\n",
      "               cylindrical_z=\"r_earth\"),\n",
      "    logs=dict(cylindrical_radius=false,\n",
      "              cylindrical_z=false),\n",
      "    weight_field='cell_volume',\n",
      "    extrema=dict(cylindrical_radius=(0.,5.),\n",
      "                 cylindrical_z=(-5./2, 5./2.)),\n",
      "    )\n",
      "\n",
      "# phase plot\n",
      "plot = yt.phaseplot.from_profile(profile)\n",
      "plot.set_log('specific_energy', false)\n",
      "plot.set_zlim('specific_energy', 0.8e11, 1.5e11)\n",
      "plot.set_cmap('specific_energy', 'rdbu')\n",
      "\n",
      "# save figure\n",
      "plot.save(\"yt_answer.png\")```\n",
      "\n",
      "<@uhkuhfybf> awesome!!\n",
      "i have some gizmo-style datasets and i'm trying to do some rather custom plotting. this is an sph dataset; when i run this code\n",
      "```\n",
      "    image = yt.on_axis_projection(\n",
      "        ds.box(center - width/2, center + width/2),\n",
      "        center,\n",
      "        normal_vector=[1.0, 0.0, 0.0],\n",
      "        width = ds.arr([width]*3),\n",
      "        resolution=[256, 256],\n",
      "        item=('parttype4', 'masses'),\n",
      "        north_vector=[0.0, 0.0, 1.0],\n",
      "    )\n",
      "```\n",
      "i get the confusing exception:\n",
      "```\n",
      "runtimeerror: can only perform off-axis projections for sph fields, received '('parttype4', 'masses')'\n",
      "```\n",
      "is this a yt internal terminology thing i don't understand? otherwise this smells bug-like\n",
      "<@u91855pa9> that's really strange, but i can't say just now if it's any deeper than what you're seeing, that it thinks you can't do it because it assumes it'll only be asked to project sph (not n-body) particles\n",
      "ahhhhh duh parttype 4 doesn't have a smoothing length so it's not sph\n",
      "yay!\n",
      "hi! i have a problem in compiling yt from source on a cluster. the compile is successful, while when i `import yt`, the error shows `importerror: /home/suoqing/sw/yt/yt/utilities/lib/fnv_hash.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _intel_fast_memcpy`.\n",
      "btw, i can install yt after switching from `intel` to `gcc`.\n",
      "i bet whatever compiler you build with yt must be accessible whenever you're trying to use it. if you compile with intel, you probably need to have the intel module loaded whenever you import.\n",
      "thanks ben, i guess so; therefore i decided to switch to gcc whenever i use yt.\n",
      "<@u37dtbl6n> thank you for your advice. i just got volume rendering images out of my image stack!\n",
      "<@uj7v15sfk> also i made a small mistake in the patch, it should read\n",
      "```\n",
      "from 3526ad0f76cccff9825f4ee25b5a8758fa45c48b mon sep 17 00:00:00 2001\n",
      "from: corentin cadiou &lt;contact@cphyc.me&gt;\n",
      "date: sun, 5 may 2019 19:02:08 +0200\n",
      "subject: [patch] quick fix\n",
      "\n",
      "---\n",
      " yt/frontends/ramses/data_structures.py | 1 +\n",
      " yt/frontends/ramses/definitions.py     | 3 ++-\n",
      " 2 files changed, 3 insertions(+), 1 deletion(-)\n",
      "\n",
      "diff --git a/yt/frontends/ramses/data_structures.py b/yt/frontends/ramses/data_structures.py\n",
      "index c5802cbba..12979474e 100644\n",
      "--- a/yt/frontends/ramses/data_structures.py\n",
      "+++ b/yt/frontends/ramses/data_structures.py\n",
      "@@ -139,6 +139,7 @@ class ramsesdomainfile(object):\n",
      " \n",
      "         for header in ramses_header(hvals):\n",
      "             hvals.update(f.read_attrs(header))\n",
      "+        f.seek(4+8+4, 1)  # skip reading of mass_sph\n",
      "         # for speedup, skip reading of 'headl' and 'taill'\n",
      "         f.skip(2)\n",
      "         hvals['numbl'] = f.read_vector('i')\n",
      "diff --git a/yt/frontends/ramses/definitions.py b/yt/frontends/ramses/definitions.py\n",
      "index 9cc8278ed..a3235580a 100644\n",
      "--- a/yt/frontends/ramses/definitions.py\n",
      "+++ b/yt/frontends/ramses/definitions.py\n",
      "@@ -42,7 +42,8 @@ def ramses_header(hvals):\n",
      "                  ('stat', 3, 'd'),\n",
      "                  ('cosm', 7, 'd'),\n",
      "                  ('timing', 5, 'd'),\n",
      "-                 ('mass_sph', 1, 'd') )\n",
      "+                 #('mass_sph', 1, 'd') \n",
      "+                 )\n",
      "     yield next_set\n",
      " \n",
      " field_aliases = {\n",
      "-- \n",
      "2.21.0```\n",
      "i tested the code on one of my dataset and it reads :thumbsup::skin-tone-3:\n",
      "hi <@ubykep7ek>, thanks for your patience. i’ve been playing around with neater ways to do this, but haven’t done much yet. there’s no way to do `halos['m200c']`, but a shorthand that should at least make things more concise would be to do something like:\n",
      "```\n",
      "m200c = a.arr([h['m200c'] for h in halos])\n",
      "```\n",
      "that will at least give you a ytarray with proper units that you can then do stuff with.\n",
      "sorry to make you wait so long for such an anticlimactic answer, but hopefully that helps a bit. let me know if you have any more questions, and thanks for using ytree!\n",
      "thank you. this does seem to be better than what i was doing.\n",
      "<@urxlgg760> has joined the channel\n",
      "<@utwq72189> has joined the channel\n",
      "is there a way to create radial bins in profiles for particle fields without depositing the derived fields (e.g. particle_velocity_cylindrical_theta) into \"deposit\"?\n",
      "and then just creating the profile against (\"index\", \"radius\")\n",
      "you probably want the particle_radius field\n",
      "you can’t mix grid and particle fields, so if you want to profile particle fields you need to use particle fields for both axes\n",
      "hello everyone,\n",
      "i'm trying to make a multiplot using the example code given under multiplot and slice projections (<https://yt-project.org/doc/cookbook/complex_plots.html#thin-slice-projections>).\n",
      "here's my code :\n",
      "```def _nhi(field,data):\n",
      "    return ((data[('parttype0','density')] * data[('parttype0','hydrogen')] * data[('parttype0','aphi')])/mh)\n",
      "\n",
      "path = '/afs/mpa/temp/mrmgehlm/aurora/snapshot_052/'\n",
      "ds = yt.load(path+'snap_052.0.hdf5')\n",
      "ds.add_field(\"h_p0_number_density\",sampling_type='particle',function=_nhi,units=\"1/cm**3\",force_override=true)\n",
      "\n",
      "trident.add_ion_fields(ds,ions = ['o i','c ii','si ii','fe ii'],sampling_type='particle')\n",
      "\n",
      "orient = 'horizontal'\n",
      "fig, axes, colorbars = get_multi_plot(3,2,colorbar=orient,bw=6)\n",
      "\n",
      "center = ds.domain_center\n",
      "\n",
      "left_corner = ds.domain_left_edge\n",
      "right_corner = ds.domain_right_edge\n",
      "depth = ds.quan(1.0,'code_length')\n",
      "left_corner[2] = center[2] - 0.5 * depth\n",
      "right_corner[2] = center[2] + 0.5 * depth\n",
      "region = ds.box(left_corner, right_corner)\n",
      "\n",
      "res = [1200,1200]\n",
      "width = (6.25,'code_length')\n",
      "width1 = (0.7,'code_length')\n",
      "\n",
      "vv,cc = ds.find_max(\"h_p0_number_density\")\n",
      "\n",
      "left = cc \n",
      "right = cc \n",
      "depth1 = ds.quan(0.75,'code_length')\n",
      "left = left - depth1\n",
      "right = right + depth1\n",
      "reg = ds.box(left,right)\n",
      "\n",
      "slc = yt.projectionplot(ds, \"z\", fields=[\"h_p0_number_density\",\"o_p0_number_density\",\"c_p1_number_density\"], weight_field=none,data_source=region)\n",
      "proj = yt.projectionplot(ds, \"z\", fields=[\"h_p0_number_density\",\"o_p0_number_density\",\"c_p1_number_density\"], weight_field=none,data_source=reg)\n",
      "\n",
      "slc_frb = slc.data_source.to_frb(width,res)\n",
      "proj_frb = proj.data_source.to_frb(width1,res)\n",
      "\n",
      "dens_axes = [axes[0][0], axes[1][0]]\n",
      "temp_axes = [axes[0][1], axes[1][1]]\n",
      "vels_axes = [axes[0][2], axes[1][2]]\n",
      "\n",
      "slc_dens = np.array(slc_frb['h_p0_number_density'])\n",
      "proj_dens = np.array(proj_frb['h_p0_number_density'])\n",
      "slc_temp = np.array(slc_frb['o_p0_number_density'])\n",
      "proj_temp = np.array(proj_frb['o_p0_number_density'])\n",
      "slc_vel = np.array(slc_frb['c_p1_number_density'])\n",
      "proj_vel = np.array(proj_frb['c_p1_number_density'])\n",
      "\n",
      "plots = [dens_axes[0].imshow(slc_dens, origin='lower', norm=lognorm()),\n",
      "         dens_axes[1].imshow(proj_dens, origin='lower', norm=lognorm()),\n",
      "         temp_axes[0].imshow(slc_temp, origin='lower'),\n",
      "         temp_axes[1].imshow(proj_temp, origin='lower'),\n",
      "         vels_axes[0].imshow(slc_vel, origin='lower', norm=lognorm()),\n",
      "         vels_axes[1].imshow(proj_vel, origin='lower', norm=lognorm())]```\n",
      "i get the following error :\n",
      "```p009 yt : [info     ] 2020-02-19 18:22:58,212 making a fixed resolution buffer of (h_p0_number_density) 1200 by 1200\n",
      "  file \"trident_projection.py\", line 69, in &lt;module&gt;\n",
      "    slc_dens = np.array(slc_frb['h_p0_number_density'])\n",
      "  file \"/afs/mpa/temp/mrmgehlm/yt-yt-4.0/yt/visualization/fixed_resolution.py\", line 136, in __getitem__\n",
      "    units = self.data_source[item].units\n",
      "  file \"/afs/mpa/temp/mrmgehlm/yt-yt-4.0/yt/data_objects/data_containers.py\", line 266, in __getitem__\n",
      "    rv = self.ds.arr(self.field_data[key], fi.units)\n",
      "p014 yt : [error    ] 2020-02-19 18:23:13,608 keyerror: 'h_p0_number_density'\n",
      "p014 yt : [error    ] 2020-02-19 18:23:13,608 error occured on rank 14.\n",
      "application called mpi_abort(mpi_comm_world, 1) - process 14```\n",
      "even if i try to run the exact script from the cookbook, i get the same error except `keyerror : 'density'`  on the 3rd last line\n",
      "i'm using `yt4.0.dev0`\n",
      "i can't seem to figure out what's wrong, any ideas whats going on here?\n",
      "thanks!\n",
      "if you make it use `(“gas”, “h_p0_number_density”)` instead of the string `“h_p0_number_density”`\n",
      "does it work?\n",
      "even if that does work this seems like a bug to me though, if you can open an issue on yt’s github with details that would be very much appreciated\n",
      "opening an issue on github helps us not lose track of bug reports\n",
      "let me check if it works with `(\"gas\", \"h_p0_number_density\")` , if not i'll open an issue on github.\n",
      "thanks!\n",
      "even if it does work an issue report would be appreciated\n",
      "passing a string should work\n",
      "alright cool, i'll report it regardless.\n",
      "another question i had is when i try to add the ion fields to the dataset, i get `yt : [warning  ] 2020-02-19 19:03:52,901 field ('gas', 'o_p0_number_density') already exists. not clobbering.` . is there a way i can force override this?\n",
      "with trident? sounds like a trident issue\n",
      "trident has its own slack channel iirc\n",
      "alright, thanks. i'll check it out there.\n",
      "i've opened the issue\n",
      "<@ubfr232p9> has joined the channel\n",
      "<@ubf69etcm> has joined the channel\n",
      "how do you change the name of a plugin file using \"pluginfilename\"? <http://yt-project.org/doc/reference/configuration.html>\n",
      "have an entry in your config file that looks like this:\n",
      "\n",
      "```\n",
      "[yt]\n",
      "pluginfilename = my_filename.py\n",
      "```\n",
      "so i have a file called \"my_plugins.py\" located in ~/.config/yt\n",
      "inside that file all i have is:\n",
      "pluginfilename=\"plug.py\"\n",
      "plug.py is also stored in ~/.config/yt and contains a simple print statement\n",
      "when i call yt.enable_plugins() in my jupyter notebook i want to see my print statement as an output, however all i see is yt : [info     ] 2018-06-27 10:12:59,839 loading plugins from /home/mnielan/.config/yt/my_plugins.py\n",
      "what am i doing wrong?\n",
      "the config file should be located at `~/.config/yt/ytrc`\n",
      "the contents of that file should be:\n",
      "\n",
      "```\n",
      "[yt]\n",
      "pluginfilename = plug.py\n",
      "```\n",
      "and you should delete the `my_plugins.py` file you have in `/.config/yt/ytrc`\n",
      "thank you\n",
      "hi everyone, i am having this strange issue in extracting `grid_level` from my data. when i use `ds.r` on a single 2d slice, `grid_level` seems to work fine (i get varying integer values of values between 0 and the finest). however, when i do this with a volume, using `ds.covering_grid` the data is all at a single level i specify from the `level` argument ,or using `ds.r` i get all my values being 99, which is not correct. any idea about what would be causing this issue or how to solve it?\n",
      "i should add, i did the exact same procedure with `density` and my extracted data was correct.\n",
      "regardless we'd have to compute it ourselves, since it's not stored in the snapshot files\n",
      "i’m gonna have a discussion with nelle varoquaux about this soon\n",
      "updated yt, bug persists :(\n",
      "what yt and matplotlib versions?\n",
      "`print(yt.__version__)` and `print(matplotlib.__version__)`\n",
      "<@ufukdu08p> has joined the channel\n",
      "\n",
      "```\n",
      "&gt;&gt;&gt; import yt\n",
      "&gt;&gt;&gt; yt.__version__\n",
      "'4.0.dev0'\n",
      "&gt;&gt;&gt; import matplotlib\n",
      "&gt;&gt;&gt; matplotlib.__version__\n",
      "'3.0.2'\n",
      "```\n",
      "ah we probably need to merge the 4.0 branch with master to get the fix\n",
      "one sec, let me port the specific fix you need\n",
      "the pr with the fix is <https://github.com/yt-project/yt/pull/2101>\n",
      "hmm i think i need to update some tests with this pr so i shouldn’t just push directly, but i’ll open a pull request and <@u91855pa9> can pull that pull request in locally until it’s merged, sorry again for the trouble :disappointed:\n",
      "<@u91855pa9> do you have a github handle?\n",
      "saethlin\n",
      "i'm in meetings for a bit but i'll do the thing you're describing when i'm done. no worries\n",
      "to get the fix do:\n",
      "```\n",
      "git checkout -b ngoldbaum-yt-4.0 yt-4.0\n",
      "git pull <https://github.com/ngoldbaum/yt.git> yt-4.0\n",
      "```\n",
      "and you’ll have the fix in a new branch named `ngoldbaum-yt-4.0`\n",
      "hi, i'm having a little bit of trouble with `ds.find_max('temperature')` . it's returning a ytquantity in units of code_temperature when i'm after kelvin. is there a way to set the units of a ytquantity? i can't figure it out\n",
      "try this\n",
      "```tmax, coords = ds.find_max(\"temperature\")\n",
      "tmax = <http://tmax.to|tmax.to>(\"k\")```\n",
      "perfect, thank you!\n",
      "this is awesome -- was planning to do something similar, mind sharing your cookbook recipe?\n",
      "<@uhsr0nq5s> has joined the channel\n",
      "hi!  so i re-found this problem with chaining cut_regions.  i looked on github, and nathan had opened an issue for me, but then closed it.  i think it may have gone unfixed, though, so can i reopen it?  or do i just do a new issue?\n",
      "<https://github.com/yt-project/yt/issues/1093>\n",
      "i have made a cleaner example script of the problem using isolatedgalaxy....\n",
      "hi stephanie, i think it would be clearest to go ahead and make a new issue, you can refer to the old issue by saying \"#1093\" somewhere in your issue description, github will automatically link things\n",
      "ok cool will do thanks!\n",
      "since `yt`'s unit support offloads conversions to sympy, you can pass any expression with valid dimensions to `.to_units`\n",
      "is this intentional? i found a use for it and i like it, just curious.\n",
      "<@u91855pa9> can you give a short example of what you’re talking about?\n",
      "`rho = ds.quan(14, \"1e10*msun/kpc**3\")`\n",
      "oh yes, units can have constant multipliers\n",
      "i haven't tried anything more diabolical\n",
      "i'm considering trying it now, but this is something i _wanted_ to work so i was amazed then went back to what i was doing :stuck_out_tongue:\n",
      "yeah, that’s intentional :slightly_smiling_face:\n",
      "thanks :slightly_smiling_face:\n",
      "hey everyone, may i know if it is possible to overplot a line on a phaseplot?\n",
      "you can but it's a bit buried\n",
      "once you have your `phaseplot` object (or similarly for projections and slices), you can access the internal matplotlib ax _via_ `ax = p.plots[field].axes`, which you can use as a regular matplotlib axis\n",
      "<@u37dtbl6n> thanks for your reply. i may have done something stupid but following your instructions, i got keyerror: (‘gas’, ‘temperature’). may i know why?\n",
      "you should look up what are the keys of `pp.plots`\n",
      "i think they are indexed by `cell_mass` and `cell_volume` in your case\n",
      "so that would be `pp.plots['gas', 'cell_mass']`\n",
      "is it possible to force a dataset to regenerate its index and clear its \"cache\" of the loaded data?\n",
      "another way of formulating it is: where are the data cahced once generated/read from disk?\n",
      "i figures out i could delete `ds._instantiated_index` to force the index to be rebuilt, but i would like to flush any cached data\n",
      "thanks for your quick reply. i checked and corrected it to pp.plots[‘gas’, ‘cell_volume’]. it now runs without error but the line is not appearing on the phaseplot.\n",
      "<@u37dtbl6n> if you have any more suggestions, i would greatly appreciate it.\n",
      "to be honest, that's beyond my knowledge! you should check out <http://yt-project.org/doc/cookbook/simple_plots.html#matplotlib-primitives>\n",
      "i’ve been using the clump finding infrastructure in yt, which is great, but pretty slow on some big datasets i’ve got.  when i try to run it with `yt.enable_parallelism()`, i get a segfault.  i guess it’s not enabled to use mpi within the codebase?\n",
      "<@u37dtbl6n> thanks for your help! i have tried everything i know but still can’t get the line to appear. hope i can get more help from more experts on this.\n",
      "<@u042j5bn6> yeah, that sounds right.  i think it might have been once.  it will be slow on big datasets.  lots of room for improvement.  try running cprofile on it to see if we can find high-level slow areas.\n",
      "<@ucybq5kpa> here let me make an example\n",
      "<@u37dtbl6n> not really, i guess you could do `del ds` and reload the dataset?\n",
      "<@ucybq5kpa> <https://gist.github.com/ngoldbaum/a2d10e77715d6f96065e3214045d338a>\n",
      "<@ucybq5kpa> a couple caveats: you need to do your customizations *after* calling `_setup_plots()`\n",
      "otherwise they won’t show up\n",
      "i’m guessing that you were applying your customizations on an axes that was getting clobbered after you did the customizations, i’d like to make the api for this nicer\n",
      "<@u042fh0rb> those ci failiures look spooky. should your branch be good for my limited use anyway?\n",
      "sorry, what ci failures?\n",
      "oh on my pr\n",
      "i didn’t look, thanks for reminding me\n",
      "ah looks like i didn’t handle a corner case, will fix\n",
      ":+1:\n",
      "just pushed again, this should hopefully pass more tests :slightly_smiling_face:\n",
      "argh, still two failing tests, closer though…\n",
      "might have to fix these tomorrow\n",
      "<@udju40mfy> has joined the channel\n",
      "thanks!\n",
      "oh unfortunately it is really slow for octree datasets\n",
      "3.7.4\n",
      "indeed i didn’t think to mention i was attempting to build on python 3.8\n",
      "thanks a lot for your reply. for now, i believe i should be able to do what i want with 3.6 or 3.7\n",
      "report: i was able to complete installation on python 3.7 but not on python 3.6 (:question:)\n",
      "<@ud9l1d44t> i think osx equivalent of `strace` is called `dtruss`\n",
      "it can attach to a running process, so in one terminal run `pip install` and in the other do `sudo  dtruss -p &lt;pid of pip install&gt;`\n",
      "there's also `dtrace` on osx, not sure what's the difference\n",
      "<@uqzhn5cbx> has joined the channel\n",
      "i ran the exact same script with the isolatedgalaxy sample data on `<https://yt-project.org/doc/visualizing/sketchfab.html>` to export to skectfab, but i got `attributeerror: 'ytsurface' object has no attribute 'vertex_data'`\n",
      "any ideas on why? or should i open an issue on github?:flushed:\n",
      "can you pastebin the full traceback?\n",
      "although looking at the code, i think i see the issue\n",
      "yeah, i see the issue\n",
      "gonna make a pr\n",
      "```attributeerror                            traceback (most recent call last)\n",
      "&lt;ipython-input-3-ad08c11cbb2c&gt; in &lt;module&gt;()\n",
      "     16      color_log=true,\n",
      "     17      bounds=bounds,\n",
      "---&gt; 18      api_key=\"9e2d061cee1c4c58916f0eda98e8e19a\"\n",
      "     19 )\n",
      "\n",
      "/anaconda/lib/python3.6/site-packages/yt/data_objects/construction_data_containers.py in export_sketchfab(self, title, description, api_key, color_field, color_map, color_log, bounds, no_ghost)\n",
      "   1913         ply_file = temporaryfile()\n",
      "   1914         self.export_ply(ply_file, bounds, color_field, color_map, color_log,\n",
      "-&gt; 1915                         sample_type = \"vertex\", no_ghost = no_ghost)\n",
      "   1916         ply_file.seek(0)\n",
      "   1917         # greater than ten million vertices and we throw an error but dump\n",
      "\n",
      "/anaconda/lib/python3.6/site-packages/yt/data_objects/construction_data_containers.py in export_ply(self, filename, bounds, color_field, color_map, color_log, sample_type, no_ghost)\n",
      "   1760                 self[color_field]\n",
      "   1761             elif sample_type == \"vertex\" and \\\n",
      "-&gt; 1762                 color_field not in self.vertex_data:\n",
      "   1763                 self.get_data(color_field, sample_type, no_ghost=no_ghost)\n",
      "   1764         self._export_ply(filename, bounds, color_field, color_map, color_log,\n",
      "\n",
      "attributeerror: 'ytsurface' object has no attribute 'vertex_data'```\n",
      "thank you!\n",
      "<https://github.com/yt-project/yt/pull/1792>\n",
      "<@uatgd4b6f> has joined the channel\n",
      "i got the same error but a different traceback after applying the changes:\n",
      "```attributeerror                            traceback (most recent call last)\n",
      "&lt;ipython-input-15-ad08c11cbb2c&gt; in &lt;module&gt;()\n",
      "     16      color_log=true,\n",
      "     17      bounds=bounds,\n",
      "---&gt; 18      api_key=\"9e2d061cee1c4c58916f0eda98e8e19a\"\n",
      "     19 )\n",
      "\n",
      "/anaconda/lib/python3.6/site-packages/yt/data_objects/construction_data_containers.py in export_sketchfab(self, title, description, api_key, color_field, color_map, color_log, bounds, no_ghost)\n",
      "\n",
      "/anaconda/lib/python3.6/site-packages/yt/data_objects/construction_data_containers.py in export_ply(self, filename, bounds, color_field, color_map, color_log, sample_type, no_ghost)\n",
      "\n",
      "attributeerror: 'ytsurface' object has no attribute 'vertex_data'```\n",
      "<@u5t7v4urg> did you reimport yt after applying the patch? `self.vertex_data` that nathan's pr updates is the only occurrence i see in the code\n",
      "if i make any changes to source files while using ipython, i usually exit and restart to make sure the imports are “fresh”\n",
      "sorry:disappointed_relieved:, i should have reimported yt but i somehow forgot\n",
      "no worries :slightly_smiling_face:\n",
      "that’s an easy mistake to make\n",
      "when using `yt.visualization.color_maps.make_colormap()`, the color is supposed to be represented as an array of 3 floats, can we add a fourth float representing the transparency? i tried to run it with an array of 4 floats, it was not raising errors but the final result (when using this colormap and exported to skechfab) was not showing transparency either\n",
      "<@uatdzg45s> has joined the channel\n",
      "hi, i am wondering if it is possible to retrieve the dark matter particles associated with each halo from a rockstar halo catalog.\n",
      "i have the rockstar binary output, which should contain this information but it's not clear to me whether i can access it through yt\n",
      "if i load the halo and/or particle data with yt.load, it seems i can only access the halo data\n",
      "do you have the original dataset the halo finder was run on?\n",
      "yes\n",
      "ok, let me see if i can make a quick example for you to look at\n",
      "i have loaded them together, and there is a \"particle_identifier\" but it seems to refer to the halo\n",
      "that would be amazing, thanks!\n",
      "just a pointer to the right part of the docs would be plenty help though, i just can't seem to find the right place to look for how to do this\n",
      "yeah, i don’t think there’s a worked example exactly like this right now\n",
      "here’s what i just pieced together:\n",
      "(based sort of on this example, but i also had to look at the source code a bit <http://yt-project.org/doc/cookbook/halo_analysis_example.html#halo-analysis-example>)\n",
      "yes, that's the example i was trying too\n",
      "```\n",
      "import yt\n",
      "from yt.analysis_modules.halo_analysis.api import halocatalog\n",
      "\n",
      "halos_ds = yt.load('rockstar_halos/halos_0.0.bin')\n",
      "data_ds = yt.load('enzo_64/dd0043/data0043')\n",
      "\n",
      "hc = halocatalog(data_ds=data_ds, halos_ds=halos_ds)\n",
      "\n",
      "hc.create(save_halos=true)\n",
      "\n",
      "# grab the 0th halo\n",
      "halo = hc.halo_list[0]\n",
      "\n",
      "# print data about this halo:\n",
      "print(halo.quantities)\n",
      "\n",
      "# print the particle id of all particles in this halo:\n",
      "halo.data_object['particle_index']\n",
      "```\n",
      "i don’t think we read in the particle ids from the rockstar output\n",
      "i guess it would be useful to add that ability, i don’t know enough about rockstar’s data format to be helpful with adding that though\n",
      "what i’m doing in this example is creating a sphere data object in the data_ds centered at the halo’s location\n",
      "and then getting all the particles in the data_ds that are inside that sphere\n",
      "not quite the same as the particles that are in the halo identified by rockstar\n",
      "i might be missing something though, i’m not a cosmology person really\n",
      "there is some c code shipped with rockstar that provides functions to do this but i was hoping for a less cumbersome way with yt\n",
      "<@u042s6y2g> might know more, although he’s not around right now\n",
      "(functions to read the binary with particle ids)\n",
      "i suppose some of the particles inside the sphere of the yt catalogue may not be gravitationally bound particles then, but perhaps this will be enough for my needs\n",
      "thanks a lot for the help!\n",
      "np\n",
      "if the data really do exist in the binary outputs, we could probably add a way to get at those\n",
      "it just requires writing the code to do it :slightly_smiling_face:\n",
      "i’m gonna head home for the night\n",
      "have a good evening and welcome to the yt slack and community :slightly_smiling_face:\n",
      "yeah, if i manage to retrieve them with the proper method, i could let you know how i did it\n",
      "rockstar mentions the possibility here:\n",
      "<https://bitbucket.org/gfcstanford/rockstar#markdown-header-controlling-output-formats>\n",
      "have a good night, and thanks again!\n",
      "ah, perhaps this is a new feature in rockstar-galaxies\n",
      "the rockstar frontend was written before that was a thing\n",
      "so i have created the halo catalog using the code snippet above and if i grab a halo from the halo_list, it doesn't have a data_object attribute...\n",
      "perhaps it is the particle data format? i am using the eagle simulation data, which was generated with a modified gadget code\n",
      "maybe yt does not know how to read this (though it didn't throw any errors when i made the original halocatalog object)\n",
      "we have an eagle example dataset on <http://yt-project.org/data|yt-project.org/data>\n",
      "you could try running your example using that dataset\n",
      "and then one of us would be able to reproduce the issue and see what’s going on\n",
      "the relevant code is here:\n",
      "<https://github.com/yt-project/yt/blob/master/yt/analysis_modules/halo_analysis/halo_catalog.py#l435>\n",
      "that’s the main loop for creating the halo catalog\n",
      "if `save_halos` is true, then `halo_list` gets populated\n",
      "my halo_list has been populated\n",
      "ah, but the `halo` doesn’t have a `data_object`, hmm…\n",
      "but the only quantities in the halo items are particle_identifier, _mass, _position, and virial_radius\n",
      "and these seem to refer to the halo\n",
      "so perhaps the particle data wasn't read in properly\n",
      "i can try with the example dataset and see how it goes\n",
      "ah, sorry\n",
      "i forgot a step!\n",
      "`hc.add_callback(\"sphere\", factor=2.0)`\n",
      "you need to tell the halo catalog to create the data object\n",
      "`factor=2.0` sets the radius of the sphere to twice the virial radius\n",
      "aha, got you\n",
      "i'll give it another shot so\n",
      "sorry about that, i should have double checked that the code i pasted in here was working correctly\n",
      "i had it working in an interactive ipython session and i messed up copy/pasting\n",
      "there are a bunch of other callbacks that are predefined\n",
      "fyi, you can also use `yt download <http://yt-project.org/data/orbit.tar.gz>`.\n",
      "(where the url has to be the good one though)\n",
      "oh i didn’t know about `yt download`.  is that recent? does it just wrap `wget`?\n",
      "<https://github.com/yt-project/yt/commit/4d300787d79fb9b94dd2c522d8cdf4912d74155d>\n",
      "it uses requests it looks like\n",
      "hi <@uu72v7a31>, i’ve issued some pull requests that will add support for loading particles from hop and fof catalogs made with yt. you’ll need to pull changes from my fork of yt_astro_analysis and my fork of yt, but it should all work. if you’re still interested in this, let me know and i’ll send detailed instructions.\n",
      "yeah, i'm still working on halo merger tree, it will be really helpful if you can send detailed instructions, and thank you very much.\n",
      "<@ucybq5kpa> has joined the channel\n",
      "hi everyone, is there a way to change the default mass definition of rockstar to change the output? in the documentation of rockstar: <https://bitbucket.org/gfcstanford/rockstar#markdown-header-full-configuration-options>, there is mention of changing the mass definition so as to modify the outputs. i am wondering where do i change this mass_definition as i am interested in the radius at 500c. thanks in advance!\n",
      "<@uf7ndhh0d> has joined the channel\n",
      "does anyone have any experience using the hop catalogs in ramses?\n",
      "im trying to use them to locate galaxies in the simulation but for some reason it's not correctly locating them\n",
      "<@uppavuhsm> i think <@u37dtbl6n> might have\n",
      "<@urebtgdkm> has joined the channel\n",
      "is there an easy way to create a derived field out of the curl of an existing vector field (i.e. the current density from magnetic field?)\n",
      "wait, i see that the gradients of the magnetic field are already available to compute the curl\n",
      "i’m surprised the curl itself isn’t available.\n",
      "<@uas3yas4v> has joined the channel\n",
      "<@usxtx4crh> has joined the channel\n",
      "just a quick note to say that ytree (either stable or dev version) cannot be imported when using the yt-4.0 dev version (i followed all conda python3 install instructions at <https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb>). importing ytree results in modulenotfounderror: no module named 'yt.extern.six'. this is not urgent for me because ytree works perfectly in python2 with yt-3.4.1, but if anyone has an easy/obvious solution please let me know <@u042s6y2g>\n",
      "ytree could probably either bundle its own copy of six or depend on six (e.g. by adding it to `install_requires`) in `ytree`'s `setup.py`\n",
      "you should probably file an issue about this on github\n",
      "or fix it like i suggested and make a pr :slightly_smiling_face:\n",
      "thanks <@u042fh0rb>!\n",
      "does anyone know an easy way to scale a volume rendering?\n",
      "say, if one of the directions is much longer than the others\n",
      "i think you might need to hack the camera and/or lens\n",
      "i was afraid of that\n",
      "i *think* you can do this by setting `vp_pos` in the implementation of `_get_sampler_params` for a new lens\n",
      "i think that's the start position of the rays?\n",
      "i see - i'll play around with that. thanks!\n",
      "<@utcacpwbz> has joined the channel\n",
      "hi, i'm gen chiaki, a postdoc in georgia tech.\n",
      "i removed the `yt-conda` directory by mistake and try to install `yt` again through the install script.\n",
      "i could install all files but when i type `import yt`, it fails with a message:\n",
      "```  file \"/home/genchiaki/yt-conda/lib/python3.7/site-packages/mpmath/libmp/libelefun.py\", line 76, in &lt;module&gt;\n",
      "    cache_prec_steps += [min(2**k,log_taylor_prec)+20] * 2**(k-1)\n",
      "memoryerror```\n",
      "i used `python-3.6` and `yt-3.4.0` . my os is `ubuntu 16.04`. i'm afraid some conflict with the latest version occurs...\n",
      "thank you.\n",
      "looks like you’re running out of ram?\n",
      "thank you for prompt response. yes, the memory usage becomes almost 100% but my machine has 64 gb.\n",
      "that’s very strange, importing yt shouldn’t use that much ram\n",
      "yt 3.4.0 is very old, for what it’s worth\n",
      "although your traceback is from mpmath, which is a dependency of sympy, does importing just sympy run out of ram?\n",
      "i guess someone on stackoverflow had the exact same issue? <https://stackoverflow.com/questions/59844898/how-to-solve-memoryerror-in-conda|https://stackoverflow.com/questions/59844898/how-to-solve-memoryerror-in-conda>\n",
      "i doubt this is a yt issue per se\n",
      "ah yeah, this is <https://github.com/fredrik-johansson/mpmath/issues/513|https://github.com/fredrik-johansson/mpmath/issues/513>\n",
      "<@utcacpwbz> looks like you just hapenned to install yt at a time when there was a bug affecting not just you but lots of conda-forge users, i’ve filed some issues and pinged one of the conda-forge maintainers, hopefully this’ll get fixed soon\n",
      "oh... thank you so much. i should be a lucky guy.\n",
      "for now you can probably install mpmath with pip instead of conda?\n",
      "just a guess, on my phone\n",
      "maybe install mpmath from the default conda channel?\n",
      "good luck, sorry for the brokenness :(\n",
      "thanks, i'll try to install mpmath with pip for now.\n",
      "thanks for your answer. unfortunatly neither of these works. maybe it’s a frontend-level issue ? i’m using the amrvac one.\n",
      "<@ud9l1d44t> i suspect there may be a conditional that should be fixed in the gradient field code that assumes 3d.\n",
      "but not for any particularly substantive reason\n",
      "i just checked `dataset.add_gradient_fields` , can’t find such a conditional upon quick reading. i’m now trying to get a better grasp of it\n",
      "ah well there’s may indeed be something fishy here.\n",
      "i’m seeing that, eg at this line, <https://github.com/yt-project/yt/blob/921e85ff9da79a348229b683c1dc91ea20142f1c/yt/fields/fluid_fields.py#l211>\n",
      "i’m getting `ds` such that `ds.shape == [18,18,18]` , which looks odd for 2d data\n",
      "after toying with the equivalent problem for 3d dataset, comparing amrvac data vs fake ds, it seems pretty clear this is a frontend problem actually. switching this conversation to github !\n",
      "<@ud9l1d44t>, i also would be very interested in using gradient fields for 2d data sets. see the chain of messages here from mid-december. if you find a workaround, i'd be ecstatic to hear it 🤗\n",
      "<@u042hlt7u> <@u37dtbl6n> i found the issue, it was my mistake. apparently `divu` in my code is _not_ computed using centered differences, it is actually a state variable that is solved for explicitly in the code. there is a separate variable `diveru` that is computed directly from the velocity fields, just as done as with yt, and they came out to be identical except for the boundary conditions, but that is something i can handle. thank you for the inputs :slightly_smiling_face:\n",
      "howdy folks. i'm looking to do a 3d linear interpolation of the gas potential in my data onto the (lagrangian) particle positions so i can calculate the pe of the gas on the stars. i see there is an \"arbitrary grid object\" that might save me from looping over np.interp... is this the correct place in the code to be looking to do this?\n",
      "i see this object is really made to be used in the reverse sense of what i want to do... but i'm curious about this \"voxelize the grid\" statement in doc/analyzing/objects.html#arbitrary-grid\n",
      "my fallback plan is to use ds.point to get the cell the particle is in, then index all the neighbors manually to get the data, then do 3 1-d linear interpolations and average them.\n",
      "careful, neighbors might be on different amr level (assuming you’re dealing with an amr code)\n",
      "we don’t expose the precise functionality you’re looking for i don’t think\n",
      "if you’re fine with using interpolated data for neighbors on different amr levels, one way to do this would be to create a covering_grid or arbitrary_grid centered on the location of the particle that is 3x3x3 zones, at the amr level of the cell the particle is in\n",
      "this will probably not be terribly efficient because you’d have to loop over particles\n",
      "if you don’t care about the linear interpolation, you could use `ds.find_field_values_at_points` to do this in a somewhat more efficient manner\n",
      "that just returns the value of the cell that the particle is in?\n",
      "maybe i'll start there and expand if needed... but yes i was just looking at creating a covering grid, then passing that to scipy.interpolate.regulargridinterpolator to get at it.\n",
      "yup, we make the assumption that data are piecewise constant in a zone\n",
      "ah yeah, if you can just make a single big covering grid that would also work\n",
      "or smoothed_covering_grid\n",
      "which will do cascading linear interpolation in regions where data are at a lower resolution than the grid\n",
      "hrrrm.... sounds like possibly a chance to contribute some useful code to the project. only question is do i do the simple thing for now... thesis defense is looming in the next few months, so simple is rather appealing at the moment\n",
      "heh yes, good luck btw!\n",
      "i think we have all the machinery necessary to get what you want in a nice way, it’s just a matter of wiring it up and giving it a user interface\n",
      "sorry to not have an easy “oh yeah you just do this” answer\n",
      "oh no, its okay... i was anticipating doing some wiring. although you made me ask an important question, which is whether the piecewise constant method of ds.get_field_at_point is good enough for my purposes. you might have saved me some time anyway!\n",
      "<@ufagql7u0> has joined the channel\n",
      "i’m having trouble getting `annotate_marker` to work. i’m in a notebook, and i’m just  doing `p = yt.projectionplot(blahblah)` (works and shows up fine) and then `p.annotate_marker(halo_center, coord_system='data')` to which i get this. same thing if i do say `p.annotate_marker((0,0), coord_system='axis')` etc.\n",
      "update: it works in python3, but not 2.7\n",
      "can you file an issue pls?\n",
      "fyi it works for me both on python3 and 2.7\n",
      "(not with the same dataset though)\n",
      "(with steps to reproduce evidently)\n",
      "is there a way with the dusk colormap to set a `bad value` ?\n",
      "can you elaborate a bit more what you’re trying to do?\n",
      "is the bad value inside or outside the colorbar range?\n",
      "i have a plot with some points filled with nans\n",
      "ah ok\n",
      "does `plot.set_background_color` work?\n",
      "and i'd like it to be filled with the first value of the cmap\n",
      "yes, but how do i get the color?\n",
      "i think it chooses that color by default\n",
      "<https://github.com/yt-project/yt/blob/master/yt/visualization/plot_container.py#l724>\n",
      "oh, that's right\n",
      "hello folks. quick q: i'm trying to get an annotated contour all one color (white) with labels. here's the test data (my initial mc) i'm trying to plot: <https://drive.google.com/open?id=1_ifg3bwrdhaeu2n-zki25p58tpqpy3mk>  and here's the code i used: <https://pastebin.com/wfwjnuzb> i'm always getting the plot colormap stuck into the contour annotation colors. :disappointed:\n",
      "i want some cool contours of the ionization fraction overlaid with the mag field and density (or temperature as i may). sorry that flash plt_file is kinda large... but its what i have laying around that i was working on this morning.\n",
      "am i doing this wrong, or did i find a new feature?\n",
      ":slightly_smiling_face:\n",
      "tries to reproduce\n",
      "<@u9s817f7w> you want `plot_args={'colors':'k'}`\n",
      "you have `'color'`\n",
      "aha!\n",
      "thanks :slightly_smiling_face:\n",
      "thinks matplotlib should emit a warning there but what does he know\n",
      "is this code snippet going to produce a model with two surfaces when uploaded to sketchfab? the embed on the website seems to be broken\n",
      "\n",
      "what url is that?\n",
      "i see the sketchfab widget at <http://yt-project.org/doc/visualizing/sketchfab.html#obj-and-mtl-files>\n",
      "oh the url is <https://yt-project.org/doc/visualizing/sketchfab.html>\n",
      "ah, https\n",
      "i bet it’ll work with http\n",
      "it’s probably mixed content errors\n",
      "do you see the first widget above that one?\n",
      "yes\n",
      "yeah, i have a fix\n",
      "for not just use <http://yt-project.org/doc/visualizing/sketchfab.html#obj-and-mtl-files> and you’ll see the widget\n",
      "we’re linking to the second widget with an http:// url, which web browsers these days object to from an https page\n",
      "that's probably a minor issue, but the sketchfab result seems a little bit weird? i though that sample code was supposed to give a model of two surfaces with different transparency?\n",
      "i tried the code and it gives the same result as the widget on the website (as it should)\n",
      "it does, but one model is inside the other\n",
      "since the transparencies are different, shouldn't we still be able to see two surfaces?\n",
      "i guess not?\n",
      "i don’t know offhand why that’s not working\n",
      "it looks like the transparency is being written to the mtl file\n",
      "perhaps sketchfab doesn’t support that?\n",
      "isn’t sure\n",
      "i tried setting different colormaps for each of the surfaces, yet after uploading the model it still looks just white.\n",
      "the sketchfab website says it supports mtl file\n",
      "it’s also possible we’re not writing the data to the mtl file correctly\n",
      "i’m just speculating\n",
      "sorry i can’t be more helpful\n",
      "but when uploading it did tell me that there's something wrong with the mtl file...\n",
      "tries to run that example\n",
      "hmm meshlab doesn’t seem to do anything with the transparency either\n",
      "the colors work fine though\n",
      "\n",
      "i think that's a known thing with meshlab\n",
      "<@u04dacycu> do you remember how you got transparencies?\n",
      "did you have to use blender?\n",
      "<@u04dacycu> has joined the channel\n",
      "the example code does not give any kwarg about colormap, so the color of the surface it set by default?\n",
      "it looks like `export_obj` has a `color_map` keyword argument\n",
      "yes, but the example did not specify a colormap\n",
      "that’s true, i guess it picks a default one if one isn’t specified\n",
      "yup <https://github.com/yt-project/yt/blob/dd695793aa9825b805fc9544e986826f0af8d752/yt/data_objects/construction_data_containers.py#l1404-l1405>\n",
      "looks like the model should at least look purple:joy:\n",
      "one thing that might be worth trying to is encode the transparency as an alpha value in the material specification\n",
      "and not using the “d” parameter as we’re doing now\n",
      "the transparency is getting written to the file, and according to the mtl file spec it should be working\n",
      "but it appears both meshlab and sketchfab ignore it as we’re writing it now\n",
      "i’m not sure why\n",
      "ah, got it\n",
      "in sketchfab go to 3d settings, and under “opacity” set it to “dithered” and then choose a value close to 50% on the slider bar\n",
      "<https://skfb.ly/6zcqm>\n",
      "ah, cute~i guess we could also set the color in the 3d settings\n",
      "i'll try play around with it though this is not the optimal solution\n",
      "we should look at supporting <https://en.wikipedia.org/wiki/gltf>, it seems to be a better specified format than obj/mtl\n",
      "i like it much more\n",
      "i remember reading through the spec once\n",
      "hi mihir, sorry for the delay getting back to you. lots of teaching and events happening right now. the vector fields that exist right now are mainly for combining x/y/z component fields into a (3, n) shaped field.\n",
      "at the moment, i’m not sure how much work it would take add analysis fields with various array shapes, but i’ll look into it in the next few days and get back to you as soon as i can.\n",
      "<@uj7v15sfk> has joined the channel\n",
      "\n",
      "\n",
      "\n",
      "oh, sorry about that. the scripts are supposed to run: first make_the_catalog and then example.py. now that i think about though you probably won't be able to download the data with what i sent you and i wouldn't have you download everything needed to make the load, it would be too much trouble with no reason. and anyway, i think that i wasn't very specific thus far and i should apologize about that. so being specific, question: when i run a command such as this one halocatalog(data_ds=ds, finder_method='rockstar', finder_kwargs={'dm_only':true})  and then applying a filter as this one add_filter('quantity_value', 'particle_mass', '&gt;', 1e12, 'msun') i get 2 outputs. a directory named rockstar_halos  and dir halo_catalogs. hence i can load from these two a rockstar_ds and a catalog_ds. should i expect fields like particle_position or particle_velocity to be different between the 2 (when refering to the common particles between the two of course)?\n",
      "hello! i was wondering if it was possibly to make a plot of the amr grid of any region within the simulation? i'm currently using ramses code.\n",
      "something like this?\n",
      "<@uppavuhsm> you should be able to do this with `annotate_grids`, or `annotate_cell_edges` on a slice or projection plot\n",
      "hi <@u042hlt7u>, i tried to do this for the following projection plot:\n",
      "proj = yt.projectionplot(ds, \"z\", (\"gas\", \"density\"), center=[0.450478, 0.523349, 0.541032], width=(40, \"kpc\"), weight_field=\"density\")\n",
      "where \"ds\" is given by:\n",
      "ds = yt.load(\"/.../output_00448/info_00448.txt\", extra_particle_fields=[(\"particle_birth_time\", \"d\"), (\"particle_metallicity\", \"d\")])\n",
      "yet i got this response\n",
      "yt : [warning ] 2020-04-11 05:03:51,379 supplied id_loc but draw_ids is false. not drawing grid ids\n",
      "when i tried to save the plot\n",
      "i checked if this was an issue that had cropped up before and located this:\n",
      "<https://github.com/yt-project/yt/issues/2293>\n",
      "however, using proj.zoom() did not change the response, and i did not manage to get any annotations regardless of what i tried. also the person who sent in the issue in the link above was using flash data whereas i am using ramses, and i didn't really understand why using zoom() would change anything about whether or not it annotated grids\n",
      "any ideas?\n",
      "i'm not able to install `yt-4.0` on my mac from source. it appears to run into an error of not finding a file called 'ios'. if i `checkout master` instead, it installs fine. i don't think its a `gcc` bug, otherwise the same process wouldn't be able to install `master`, right? has anyone else ran into this?\n",
      "\n",
      "```installing collected packages: yt\n",
      "  found existing installation: yt 4.0.dev0\n",
      "    uninstalling yt-4.0.dev0:\n",
      "      successfully uninstalled yt-4.0.dev0\n",
      "  running setup.py develop for yt\n",
      "    complete output from command /users/claytonstrawn/anaconda2/envs/myenv/bin/python -c \"import setuptools, tokenize;__file__='/users/claytonstrawn/yt/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" develop --no-deps:\n",
      "    unable to compile openmp test program so cython\n",
      "    extensions will be compiled without parallel support\n",
      "    /users/claytonstrawn/anaconda2/envs/myenv/lib/python2.7/distutils/extension.py:133: userwarning: unknown extension options: 'extra_compile_arg'\n",
      "      warnings.warn(msg)\n",
      "    writing yt.egg-info/pkg-info\n",
      "    writing top-level names to yt.egg-info/top_level.txt\n",
      "    writing dependency_links to yt.egg-info/dependency_links.txt\n",
      "    writing entry points to yt.egg-info/entry_points.txt\n",
      "    reading manifest file 'yt.egg-info/sources.txt'\n",
      "    reading manifest template '<http://manifest.in|manifest.in>'\n",
      "    warning: no previously-included files found matching 'scripts/pr_backport.py'\n",
      "    warning: no files found matching '*.svgz' under directory 'doc'\n",
      "    warning: no files found matching '*.pdf' under directory 'doc'\n",
      "    no previously-included directories found matching 'doc/source/reference/api/generated'\n",
      "    no previously-included directories found matching 'doc/build'\n",
      "    writing manifest file 'yt.egg-info/sources.txt'\n",
      "    running build_ext\n",
      "    building 'yt.geometry.particle_oct_container' extension\n",
      "    gcc -fno-strict-aliasing -i/users/claytonstrawn/anaconda2/envs/myenv/include -arch x86_64 -dndebug -g -fwrapv -o3 -wall -wstrict-prototypes -iyt/utilities/lib/ -iyt/utilities/lib/ewahboolarray -i/users/claytonstrawn/anaconda2/envs/myenv/include/python2.7 -i/users/claytonstrawn/anaconda2/envs/myenv/lib/python2.7/site-packages/numpy/core/include -c yt/geometry/particle_oct_container.cpp -o build/temp.macosx-10.6-x86_64-2.7/yt/geometry/particle_oct_container.o\n",
      "    warning: include path for stdlibc++ headers not found; pass '-stdlib=libc++' on the command line to use the libc++ standard library instead [-wstdlibcxx-not-found]\n",
      "    in file included from yt/geometry/particle_oct_container.cpp:632:\n",
      "    in file included from /users/claytonstrawn/anaconda2/envs/myenv/lib/python2.7/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n",
      "    in file included from /users/claytonstrawn/anaconda2/envs/myenv/lib/python2.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12:\n",
      "    in file included from /users/claytonstrawn/anaconda2/envs/myenv/lib/python2.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1822:\n",
      "    /users/claytonstrawn/anaconda2/envs/myenv/lib/python2.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: \"using deprecated numpy api, disable it with \"          \"#define npy_no_deprecated_api npy_1_7_api_version\" [-w#warnings]\n",
      "    #warning \"using deprecated numpy api, disable it with \" \\\n",
      "     ^\n",
      "    yt/geometry/particle_oct_container.cpp:640:10: fatal error: 'ios' file not found\n",
      "    #include \"ios\"\n",
      "             ^~~~~\n",
      "    2 warnings and 1 error generated.\n",
      "    error: command 'gcc' failed with exit status 1\n",
      "    \n",
      "    ----------------------------------------\n",
      "  rolling back uninstall of yt\n",
      "command \"/users/claytonstrawn/anaconda2/envs/myenv/bin/python -c \"import setuptools, tokenize;__file__='/users/claytonstrawn/yt/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" develop --no-deps\" failed with error code 1 in /users/claytonstrawn/yt/```\n",
      "i don't have a solution, and i don't have a mac i could even test on, i just have suggestions. this is a file generated by cython; is it possible this is a cython bug and if you update cython you'll get a fixed version?\n",
      "`ios` is a c++ standard library header, it ought to be enclosed with `&lt;&gt;` not `\"\"`. i also see this is being built with `gcc` not `g++` which seems odd for a c++ file, but that appears to be just what cython does\n",
      "you might be able to follow the instruction in that warning message, `warning: include path for stdlibc++ headers not found; pass '-stdlib=libc++' on the command line to use the libc++ standard library instead [-wstdlibcxx-not-found]` by stuffing that flag into the environment variable `cflags`.\n",
      ":shrug: maybe something here will help while we wait for the people who actually know yt to wake up\n",
      "<@u7ku54sg5> <https://github.com/yt-project/yt/pull/2278>\n",
      "hi! can yt load plot3d format?\n",
      "also, is it possible to use yt to extract data along a contoured 3d boundary and plot the extracted data in an xy plot?\n",
      "no, yt doesn’t yet have a loader for plot3d, and no that’s not possible, although we do have a way of getting field data at the vertices of the countoured data\n",
      "this one? <https://en.wikipedia.org/wiki/plot3d_file_format>\n",
      "yup, this one\n",
      "ok, well the only way to get it in python is i guess to write my own loader then. how much work would this be?\n",
      "what sort of data is it?\n",
      "like, grids? particles?\n",
      "amr?\n",
      "block structured ijk data (no amr)\n",
      "if you can keep it in memory you can load it using yt.load_uniform_grid\n",
      "once you have that working you could add a frontend, we have docs on that here: <http://yt-project.org/doc/developing/creating_frontend.html>\n",
      "it’s not trivial but it’s not a crazy amount of work, especially for a unigrid data format\n",
      "depends on how comfortable you are with python\n",
      "if you are interested we’re happy to work with you e.g. in the <#cbe6579cz|development> channel here or via e-mail on our mailing list\n",
      "<http://yt-project.org/doc/examining/loading_data.html#generic-array-data>\n",
      "well, i will have to say that my priority is to post-process the data that i have and not to develop a front-end for yt (not sure if i am understanding your offer correctly ...) meaning that as soon as i have the data in python, i would move on to post-process it and get the \"meat\" out of it\n",
      "yes, lots of doc around, i have not read yet.\n",
      "ok\n",
      "you said you were interested in writing a reader, but it’s ok if you’re not interested in contributing\n",
      "once you can represent your data as 3d numpy arrays you should be able to load it into yt using load_uniform_grid\n",
      "good luck!\n",
      "you said \"we do have a way of getting field data at the vertices of the countoured data\", just making sure that i will have a way to actually do the post-processing that i want once i have it in python/yt\n",
      "that’s right, take a look at this: <http://yt-project.org/doc/visualizing/sketchfab.html>\n",
      "you can get e.g. surface[some_field] after creating the isosurface\n",
      "that will return the field values at each triangle\n",
      "there should also be surface.vertices (if i’m remembering correctly) which will store the coordinates of all the vertices\n",
      "ok, great. thank you!\n",
      "<@ubktekqc9> has joined the channel\n",
      "this is maybe a bit basic, but how do you specify the field type when creating a profile? for instance, i was trying to make a profile of the (\"gas\", \"cylindrical_velocity_theta\") against radius, and initially tried doing this:\n",
      "```vrot_profile = yt.create_profile(data_source=dsik1 , bin_fields=[\"radius\"], fields=[\"cylindrical_velocity_theta\"], n_bins=256, units=dict(radius=\"kpc\"), logs=dict(radius=false), weight_field= \"cell mass\")```\n",
      "but it came up with the error:\n",
      "```ytfieldnotfound: could not find field '('all', 'cylindrical_velocity_theta')' in info_00448.```\n",
      "and then when i tried to specify\n",
      "```fields=(\"gas\", \"velocity_cylindrical_theta\") ```\n",
      "or\n",
      "```field_type=\"gas\" ```\n",
      "it came up with the same error as before where it couldnt find  '('all', (\"gas\", 'cylindrical_velocity_theta'))' in the info file\n",
      "and also later it couldn't find the cell mass either\n",
      "make a list of tuples?\n",
      "this seems like a bug though\n",
      "if you open an issue on github with a script someone can run to reproduce this issue that will help\n",
      "if i had to guess, `[(“gas”, “cylindrical_velocity_theta”)]` would work\n",
      "assuming that field exists\n",
      "not being able to find the cell mass later also sounds like a different bug\n",
      "i don't think \"cylindrical_velocity_theta\" exists\n",
      "weirdly this worked out when just wrote out\n",
      "```profile = yt.create_profile(disk1, \"radius\", \"velocity_cylindrical_theta\", ...) ```\n",
      "i.e. just not specifying what each argument was\n",
      "interesting, glad you found a workaround :)\n",
      "i have a pretty basic coding question as well - trying to add a field vcirc = sqrt(gm/r), however, i need to include something that tells it not to worry when r = 0 but the following attempt isn't working? (didn't expect it to but this was my attempt anyway)\n",
      "(sorry about using a screenshot)\n",
      "maybe you want np.where?\n",
      "if you haven’t looked at <https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html|https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html> that might also be worth doing\n",
      "ok checking it out\n",
      "i often do something like: \n",
      "\n",
      "wh = radius != 0\n",
      "nwh = radius == 0\n",
      "\n",
      "my_array[wh] = some_function_of(radius[wh])\n",
      "my_array[nwh] = some_default_value\n",
      "where my_array and radius have the same shape\n",
      "oh also in your field definition\n",
      "it’s not the mass enclosed\n",
      "it’s the local mass at that position\n",
      "oh true thank you\n",
      "it’s unfortunately not straightforward in yt to make a field that depends on the enclosed mass\n",
      "you need to do that outside the field system\n",
      "how do you mean\n",
      "like, on a 3d ndarray centered on your halo\n",
      "or whatever object you’re looking at\n",
      "i guess if you have an outside estimate of the radial density profile you could use that to estimate the enclosed mass for a field definition\n",
      "never tried that though\n",
      "it would need to be a per-halo thing though\n",
      "alas\n",
      "this sounds really hard\n",
      "like, you would need to make sure you’re only considering amr zones that are associated with whatever halo or object or whatever you’re calculating the circular velocity for\n",
      "yeah\n",
      "yeah, so you’d need to identify all the halos or objects you want to consider\n",
      "for each one calculate the enclosed mass at a given radius\n",
      "and then use that for your field definition\n",
      "using a 1-d table\n",
      "and interpolating between values\n",
      "if you only have one object it’s often easier to reason about things if you don’t work with the native amr data and instead work with an interpolated uniform resolution grid\n",
      "that’s what i did for my thesis\n",
      "anyway, good luck, have a nice weekend\n",
      "thank you!\n",
      "you too\n",
      "quick question i've been wondering for a while now : does yt have a functionality for converting amr data to a good ol' regular-spaced numpy array (be it at the coarsest or the finest amr level), be it incredibly slow or inefficient ?\n",
      "hi <@ud9l1d44t>, i would start here:\n",
      "<https://yt-project.org/docs/dev/analyzing/objects.html#selecting-fixed-resolution-regions>\n",
      "\n",
      "also, have a look at covering_grids and abritrary_grids (should be on the same page). if that’s not what you meant, please let me know.\n",
      "thanks for your answer. it _looks_ like what i am looking for, although i’m having problems checking for it as yt seems... undecided...\n",
      "\n",
      "it looks like a severe bug affecting non 3d datasets but as i’m working with an experimental and yet unmerged frontend, it might be specific to it. <@u042hlt7u>, any clue ?\n",
      "taking a look at the source... it’s not as much of a bug as a missing feature : it’s never been implemented for datasets with lower dimensionality !\n",
      "```\n",
      "if len(item) != self.ds.dimensionality:\n",
      "            # not the right specification, and we don't want to do anything\n",
      "            # implicitly.  note that this happens *after* the implicit expansion\n",
      "            # of a single slice.\n",
      "            raise ytdimensionalityerror(len(item), self.ds.dimensionality)\n",
      "if self.ds.dimensionality != 3:\n",
      "            # we'll pass on this for the time being.\n",
      "            raise ytdimensionalityerror(self.ds.dimensionality, '3')\n",
      "```\n",
      "try creating an `arbitrary_grid` directly, i'm pretty sure that works for 2d data\n",
      "thank you <@u042fh0rb>, i can confirm this actually works\n",
      "```\n",
      "obj = ds.arbitrary_grid(ds.domain_left_edge, ds.domain_right_edge, dims=(128, 128, 1))\n",
      "d = obj[\"rho\"].squeeze().to_ndarray()\n",
      "```\n",
      "did the trick. although it raises the question as to why we are dealing with 3d edges array while the dataset is actually 2d. i remember forcing this in the frontend but i don’t know why that felt necessary.\n",
      "because logically all data are 3d from yt's perspective\n",
      "alright, thanks\n",
      "<@u01300cqn1m> has joined the channel\n",
      "hi, could i have a question about this code? i got an error on the line before the last line: runtimeerror. i do not know what it means. thank you\n",
      "\n",
      "`import numpy as np`\n",
      "`import matplotlib.pyplot as plt`\n",
      "`from numpy import array`\n",
      "`import matplotlib as mpl`\n",
      "`import math`\n",
      "`import code`\n",
      "`import yt`\n",
      "`from matplotlib.patches import fancyarrowpatch`\n",
      "`from yt import ytarray  # arrays in yt module`\n",
      "`from yt.visualization.api import streamlines  # force lines`\n",
      "`import matplotlib.pylab as pl`\n",
      "\n",
      "`# constants`\n",
      "`q     =-1.6e-19      # electron charge`\n",
      "`m     = 9.1e-31      # electron mass`\n",
      "`v_par = 2000         # parallel volocity` \n",
      "`v_per = 2000         # perpendicular velocity`\n",
      "\n",
      "`x0=np.zeros(6)      # vector - 6 dim, give zeros into it (3 components - positions, 3 components - velocity)`\n",
      "\n",
      "`# cartesian coordinates`\n",
      "`xmin =-2`\n",
      "`xmax = 2`\n",
      "`ymin = -2`\n",
      "`ymax = 2`\n",
      "`zmin = -3`\n",
      "`zmax = 5`\n",
      "`sampling = 10`\n",
      "\n",
      "`x_ = np.linspace(xmin, xmax, sampling)`\n",
      "`y_ = np.linspace(ymin, ymax, sampling)`\n",
      "`z_ = np.linspace(zmin, zmax, sampling)`\n",
      "\n",
      "`x, y, z = np.meshgrid(x_, y_, z_, indexing='xy')`\n",
      "\n",
      "`r=np.sqrt(x**2+y**2)`\n",
      "`theta=np.arctan2(y,x)  # je treba overit!!! je skutecne poradi y,x??? byva, ale obcas je to obracene`\n",
      "\n",
      "`b_r=np.zeros_like(x)`\n",
      "`b_theta=np.zeros_like(x)`\n",
      "`b_z=np.zeros_like(x)`\n",
      "\n",
      "`w=np.where(z&lt;0)`\n",
      "`b_z[w]=r[w]`\n",
      "`b_r[w]=0`\n",
      "\n",
      "`w=np.where(z&gt;math.pi)`\n",
      "`b_z[w]=0.5*r[w]`\n",
      "`b_r[w]=0`\n",
      "\n",
      "`w=np.where( (z&gt;=0) &amp; (z&lt;=math.pi))`\n",
      "`b_z[w] = 0.25*r[w]*np.cos(z[w]) + 0.75*r[w]`\n",
      "`b_r[w] = 0.25/3*r[w]**2*np.sin(z[w])`\n",
      "\n",
      "`# transformation of magnetic field from cylindrical coordinates into cartesian coordinates`\n",
      "`bx = b_r * np.cos(theta)`\n",
      "`by = b_r * np.sin(theta)`\n",
      "`bz = b_z`\n",
      "\n",
      "`# choose point in field where force line will be integrated`\n",
      "`x_point = 0.025`\n",
      "`y_point = 0`\n",
      "`z_point = 0`\n",
      "\n",
      "`# dictionary of numpy arrays - magnetic field data`\n",
      "`data = dict(b1=bx, b2=by, b3=bz)` \n",
      "`bx = data[\"b1\"]`\n",
      "`by = data[\"b2\"]`\n",
      "`bz = data[\"b3\"]`\n",
      "\n",
      "`# 3d array`  \n",
      "`bbox = np.array([[-2,2], [-2,2], [-3,5]])        # border`\n",
      "\n",
      "`ds = yt.load_uniform_grid(data, b_r.shape, length_unit=\"mpc\", bbox=bbox, nprocs=100) # data, dimenze`\n",
      "\n",
      "`# define c: the center of the box, chosen point`\n",
      "`c = ds.arr([x_point, y_point, z_point], 'code_length')`  \n",
      "`c1 = ds.domain_center`\n",
      "`# n is number of streamlines`\n",
      "`n = 1` \n",
      "`# scale is the spatial scale of the streamlines relative to the boxsize`\n",
      "`scale = ds.domain_width[0]` \n",
      "`pos = c`\n",
      "\n",
      "`# create streamlines and integration`\n",
      "`streamlines = streamlines(ds, pos, 'b1', 'b2', 'b3', length=none)   # this line gives error`\n",
      "`streamlines.integrate_through_volume()`\n",
      "hi <@u010scnqyj1>, could you paste the full error? also, you can format nicely code using triple back ticks on slack (` 3 times before and after the code you want formatted)\n",
      "however, when copy/pasting your code it seems to work perfectly fine if you remove the `nprocs=100` from your `load_uniform_grid` call (this would definitely be a bug!). if you confirm this solves your issue, could you fill an issue on the project's github?\n",
      "thank you so much. it solved my issue. you mean i should add this question to github <https://github.com/yt-project/yt/issues|here as a bug?>\n",
      "yes, that would be great!\n",
      "you can fill in the template that will show up to report the bug, so that it can be fixed in the future\n",
      "ok, many thanks again.\n",
      "hi - i've been searching the yt documentation to see if you can extract field values over a 2d surface e.g. at a certain radius, without it being an isocontour but i can't find anything - am i missing something?\n",
      "hi, how is your 2d surface defined? if it is a simple geometrical object (cube, sphere, disk) you can probably use a geometrical selection (<http://yt-project.org/doc/analyzing/filtering.html?highlight=sphere#filtering-fields-by-spatial-location-geometric-objects>).\n",
      "if the region is more complicated to define, you can use a cut region (<http://yt-project.org/doc/analyzing/filtering.html?highlight=sphere#cut-regions>), where the cut would only leave cells within your region of interest.\n",
      "both will return a data container (let's call it `ad`) that you can use like a dictionary `ad['density'], ad['spherical_radius'], ad['spherical_phi'], ad['spherical_theta']`.\n",
      "thank you for your help - as far as i can tell, that first option lets you define a sphere, but then you can only view data as slices through it, not as e.g. (interpolated) values on the 2d surface of the sphere, flux through this surface etc.\n",
      "the only way i can think of doing it would be to output radius as a 'field', and then use that to define an isosurface\n",
      "but it would be a lot more convenient if i could do it using my existing data!\n",
      "yes, that’s how you can do it right now\n",
      "generate a marching cubes isocontour on the radius field\n",
      "for a long time i’ve wanted to add the ability to generate interpolated 2d data like that on the surface of various geometric objects (cylinders and spheres mostly)\n",
      "there are some tricks we can use to speed that up substantially over the marching cubes based approach\n",
      "but that doesn’t exist yet\n",
      "ok great, thanks for getting back to me!\n",
      "is there a way to locate a field’s extrema in a region object ? `ds.find_max()` doesn’t take a data_source argument, while data containers don’t have a `find_max` method.\n",
      "there's the extrema derived quantity that you can use `region.quantities.extrema('field')`\n",
      "or even simpler, `region.min(field)`, `region.max(field)``\n",
      "i have the coordinates of the centers of galaxies stored in arrays x[], y[], z[]. when i used them to define a sphere, such as sp = ds.sphere([x[0], y[0], z[0]], (50, 'kpc')), i can successfully call max_temp=sp.sum(\"temperature\"). however, when i try to call max_emissivity = sp.sum(\"luminosity_field\") (a derived field that i created), i get the error: valueerror: operands could not be broadcast together with shapes (6713,) (0,). with 6713 being the length of the arrays x[], y[], z[]. however, when i define sphere with hard coded values, such as sp = ds.sphere([0.5, 0.5, 0.5], (50, 'kpc')), i can successfully call its luminosity, using the field i derived. any ideas on why this might be happening? thanks!\n",
      "<@ul61m6zv5> has joined the channel\n",
      "<@u015r1bhqgl> has joined the channel\n",
      "hi everyone. i'm currently trying to track a position within my simulation based on a set of criteria. naturally, this can be done using the np.where() function but i was wondering if there was an equivalent within yt such that i don't have to read in the whole dataset?\n",
      "not that i’m aware of if you feel like your position could drift into the entirety of the simulation volume.  what is the nature of your tracking criteria?\n",
      "i'm essentially looking for the minimum value of a ratio between x-y magnetic field vector and z field vector (essentially looking for where the magnetic field is /the most/ into the plane as possible) and then tracking that in time. currently, i have it implemented as:\n",
      "`ad = ds.all_data()`\n",
      "    `opoint=ad[\"field_axis_ratio\"].value`\n",
      "    `asd=np.where(opoint == opoint.min())`\n",
      "    `y_axis=ad[\"y\"]`\n",
      "    `x_axis=ad[\"x\"]`\n",
      "    `store.result=(ds.current_<http://time.in|time.in>_units('s'),x_axis[asd[0]],y_axis[asd[0]])`\n",
      "in its current implementation, it's hardly a slow computation but i just wondered if there was a more elegant way of doing it.\n",
      "hi jack, you could try something like:\n",
      "```value, position = ds.find_min('field_axis_ratio')```\n",
      "beautiful. just, beautiful. thank you so much! :d\n",
      "you bet!\n",
      "okay, now assuming that i would like to do the same but for a subset of the data. is there an equivalent for ad.cut_region? i noticed that ds.sphere doesn't contain a find_min function.\n",
      "yeah, you can do:\n",
      "```ad.quantities.min_location('field_axis_ratio')```\n",
      "the return value is similar to `ds.find_min`, but returns a list of 4 items, you’ll have to make items 2-4 into an array yourself. have a look at `dir(ad.quantities)` to see all the ones that are there.\n",
      "also, just in case you’re interested, all of these as well as things like `ds.find_min` are parallel safe, so you can run that code in parallel and under the hood the work will be divided up.\n",
      "quick question about the units of \"parttype0_smoothed_temperature\" with yt.projectionplot: how come my temperature units for the colorbar are \"k * cm\" and how do i just do kelvin? the colorbar values are on the order of 10^29 k*cm which makes no sense to me. this is for plotting cosmological simulation data from gizmo (the fire simulations).\n",
      "you're doing an unweighted projection\n",
      "so you're looking at the \"column temperature\"\n",
      "e.g. the line inegral of the temperature field along one of the axes of your simulation\n",
      "if you want something with units of temperature at the end, specify a weight field\n",
      "e.g. mass\n",
      "you can also specify \"ones\" as the weight field if you really do want an unweighted projection\n",
      "perfect <@u042fh0rb> thanks, that worked!\n",
      "\n",
      "one more quick question: is there a reason why doing a projectionplot of ('deposit','parttype0_smoothed_temperature') takes so much longer than e.g., ('deposit','parttype0_cic')?\n",
      "\n",
      "currently i'm reading in the whole particle dataset (gizmo hdf5 file) for a single snapshot, and not \"filtering\" the yt dataset object to e.g., only keep particles within 3*rvir of the halo i'm interested in. do you think doing this would help, and is there an option in yt.load() that can easily accomplish this, e.g., maybe bounding_box?\n",
      "the latter is doing an sph projection\n",
      "err, the former\n",
      "esp in yt 3.x, that is very very slow\n",
      "esp for large datasets\n",
      "you probably want to be running on the yt-4.0 branch, things will be much faster there\n",
      "see e.g. <https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb> for details\n",
      "<@utge6j7c5> has joined the channel\n",
      "hello yt,\n",
      "i am a developer of warpx, a 3d particle-in-cell code for laser-plasma interaction, relying on amrex for mesh refinement. for output files, we are using the amrex plotfiles, that yt can read (and give great rendering, thank you!!). i am currently working on a  volume rendering of a 3d simulation with yt (see image), and i observe a behavior that i cannot explain.  the image uses a number of derived fields (one for a laser pulse (red) one for plasma fields (blue-yellow) and one for an external lens (green)), and the image below looks correct.\n",
      "wow, cool looking!\n",
      "here's the same script used some iterations later. the green blob should be in the gap (roughly in the middle of the image), but it is not, and i do not understand why.\n",
      "note 1: when i tried to change the order of the fields, i think the last derived field for which i call `sc.add_source` is only shown on the right of the box, and disappears on the left part.\n",
      "note 2: i am using parallel rendering, with mpi and openmp parallelization.\n",
      "note 3: when plotting with lower resolution, i did not observe the same behavior.\n",
      "has anyone observed this type of issues?\n",
      "thanks!!\n",
      "so each derived field is its own source?\n",
      "sounds like a bug to me but it's going to be hard to track down without a reproducible example\n",
      "yes, the script looks like\n",
      "```sc = yt.create_scene(ds, field='wake', lens_type='perspective')\n",
      "source_laser = yt.visualization.volume_rendering.api.volumesource(ds, field='laser')\n",
      "sc.add_source(source_laser)\n",
      "# [etc. for other sources]```\n",
      "if you do the green source on its own does it show up?\n",
      "maybe it's there but it has very different opacity from the other fields?\n",
      "thanks <@u042fh0rb>\n",
      "i can share the script and some data, but the rendering takes a lot of time. right now, i am trying to do serial rendering, to see if the bug disappears.\n",
      "yeah, it would also help to have a reproduction script that doesn't need a lot of computation time\n",
      "i have not tried the green source on its own.\n",
      "and the field disappears very quickly, does not really look like opacity. also, it is present in all iterations when rendering with low resolution (512, 512)\n",
      "i'll try and prepare something.\n",
      "ok, sorry to not be immediately helpful\n",
      "it's just that a lot of things could be breaking and it's hard to tell which just by your example\n",
      "no problem, thanks for your help! in a few minutes, i'll be able to tell if serial rendering gives the right result, at least\n",
      "turning off openmp threading *and* mpi parallelization for each iteration (still using `ts.piter()`) give the right result!\n",
      "i can do some more tests, but this serial rendering too 1:30 h :confused:\n",
      "well that at least narrows down where the bug might be\n",
      "wonders if the openmp code cython generates is triggering ub\n",
      "iirc cython is perfectly happy to generate incorrect openmp code in some circumstances\n",
      "e.g. <https://github.com/cython/cython/issues/2316>\n",
      "(that's just a guess though)\n",
      "ok i can try pure mpi (or pure openmp) rendering, that should at least tell us if it's openmp threading.\n",
      "<@u042fh0rb> update: pure openmp threading (in this case `omp_num_threads=4`) made the rendering 3x faster and gave the correct result\n",
      "heh, so i guess it’s mpi?\n",
      "i wonder what mpi parallelism over a single dataset is even doing in your example\n",
      "i am using it pretty much as a black box, i have to admit...\n",
      "if i've got a gizmodataset and i want to load multiple fields of the same particle type from the same region, it seems like there should be a way to have yt only do the whole selector walk once. is there? the naive thing which doesn't work is\n",
      "```region[('parttype0', ('masses', 'neutralhydrogenabundance'))]```\n",
      "(playing with `omp_num_threads` always gives the right answer, so yeah it seems to be mpi)\n",
      "<@u91855pa9> each two-element tuple is a single field, what you passed isn’t a valid field name tuple\n",
      "instead pass a list of two two-element tuples\n",
      "i think that’ll only do the selection once\n",
      "<@ut28w0ddg> has joined the channel\n",
      "<@u042fh0rb> do you mean like this?\n",
      "```region[[('parttype0', 'masses'), ('parttype0', 'neutralhydrogenfraction')]]```\n",
      "this doesn't work either, as a tuple of two 2-tuples or as a list of two 2-tuples\n",
      "huh, it should\n",
      "you’re on yt-4.0 right?\n",
      "yes\n",
      "and sorry, what does “doesn’t work” mean exactly in this context?\n",
      "```in [5]: reg[[('parttype0', 'masses'), ('parttype0', 'neutralhydrogenfraction')]]\n",
      "---------------------------------------------------------------------------\n",
      "typeerror                                 traceback (most recent call last)\n",
      "~/python3.8/lib/python3.8/site-packages/yt/mods.py in &lt;module&gt;\n",
      "----&gt; 1 reg[[('parttype0', 'masses'), ('parttype0', 'neutralhydrogenfraction')]]\n",
      "\n",
      "~/python3.8/lib/python3.8/site-packages/yt/data_objects/data_containers.py in __getitem__(self, key)\n",
      "    249             f = key\n",
      "    250         else:\n",
      "--&gt; 251             f = self._determine_fields([key])[0]\n",
      "    252         if f not in self.field_data and key not in self.field_data:\n",
      "    253             if f in self._container_fields:\n",
      "\n",
      "~/python3.8/lib/python3.8/site-packages/yt/data_objects/data_containers.py in _determine_fields(self, fields)\n",
      "   1189             else:\n",
      "   1190                 fname = field\n",
      "-&gt; 1191                 finfo = self.ds._get_field_info(\"unknown\", fname)\n",
      "   1192                 if finfo.sampling_type == \"particle\":\n",
      "   1193                     ftype = self._current_particle_type\n",
      "\n",
      "~/python3.8/lib/python3.8/site-packages/yt/data_objects/static_output.py in _get_field_info(self, ftype, fname)\n",
      "    788             if field not in self.field_info.field_aliases.values():\n",
      "    789                 return self._last_finfo\n",
      "--&gt; 790         if field in self.field_info:\n",
      "    791             self._last_freq = field\n",
      "    792             self._last_finfo = self.field_info[(ftype, fname)]\n",
      "\n",
      "~/python3.8/lib/python3.8/site-packages/yt/fields/field_info_container.py in __contains__(self, key)\n",
      "    330\n",
      "    331     def __contains__(self, key):\n",
      "--&gt; 332         if dict.__contains__(self, key): return true\n",
      "    333         if self.fallback is none: return false\n",
      "    334         return key in self.fallback\n",
      "\n",
      "typeerror: unhashable type: 'list'```\n",
      "oh oh wait, you want to batch the field accesses\n",
      "i see\n",
      "and the problem is that doing the two fields separately are spending a lot of time not doing i/o\n",
      "it's actually not i/o but yeah\n",
      "how do you know it’s spending time doing index calculations? did you profile?\n",
      "yes\n",
      "if this is going to turn into \"we need to patch yt to get this behavior\" i should be much more systematic about it. i've just been looking at what symbols show up in `perf top`\n",
      "ok, sorry just trying to understand exactly what you’re doing and what the trouble is\n",
      "doesn’t help that i got some sedation at the doctor earlier\n",
      "i want to read two (or more) attributes for a single particle type.\n",
      "the `__getitem__` calls to a yt region dominate quite a few data analysis scripts i have.\n",
      "if i do this in two `__getitem__` calls, i'm pretty sure from reading the yt code for this operation that it will look up which particles i'm trying to pick out twice.\n",
      "it would be pretty cool if i had a way to not have yt do the redundant work of figuring out which particles i'm trying to read data for.\n",
      "i think you may just be seeing yt doing i/o twice\n",
      "doing `ds.index` does all the spatial indexing\n",
      "let me put together a test script so we can be sure we're talking about the same thing\n",
      "(it always helps for these discussions to have a test script we can both run)\n",
      "`reg` is a subregion of the simulation box, right? not `ds.all_data()`?\n",
      "correct, it is a subregion\n",
      "<@u91855pa9>\n",
      "\n",
      "```in [2]: %time ds.index\n",
      "yt : [info     ] 2020-02-03 16:34:04,784 allocating for 1.191e+07 particles\n",
      "initializing coarse index : 100%|████████████████| 19/19 [00:01&lt;00:00,  9.94it/s]\n",
      "initializing refined index: 100%|████████████████| 19/19 [00:11&lt;00:00,  1.71it/s]\n",
      "cpu times: user 15.2 s, sys: 209 ms, total: 15.4 s\n",
      "wall time: 15.5 s\n",
      "out[2]: &lt;yt.frontends.sph.data_structures.sphparticleindex at 0x7f6d724c2128&gt;\n",
      "\n",
      "in [3]: %time ds.find_max(('gas', 'density'))\n",
      "yt : [info     ] 2020-02-03 16:34:37,845 max value is 4.15344e-21 at 31995.6347656250000000 31473.6640625000000000 28969.8867187500000000\n",
      "cpu times: user 378 ms, sys: 84.1 ms, total: 462 ms\n",
      "wall time: 459 ms\n",
      "out[3]:\n",
      "(unyt_quantity(4.15344071e-21, 'g/cm**3'),\n",
      " unyt_array([31995.63476562, <tel:314736640625|31473.6640625> , 28969.88671875], 'code_length'))\n",
      "\n",
      "in [4]: v, c = ds.find_max(('gas', 'density'))\n",
      "yt : [info     ] 2020-02-03 16:34:51,248 max value is 4.15344e-21 at 31995.6347656250000000 31473.6640625000000000 28969.8867187500000000\n",
      "\n",
      "in [5]: reg = ds.box(c-10*kpc, c+10*kpc)\n",
      "\n",
      "in [6]: %time reg['parttype0', 'masses']\n",
      "cpu times: user 347 ms, sys: 67.9 ms, total: 415 ms\n",
      "wall time: 411 ms\n",
      "out[6]:\n",
      "unyt_array([0.00036683, 0.00047384, 0.00034478, ..., 0.00031614,\n",
      "            0.00034297, 0.00032042], 'code_mass')\n",
      "\n",
      "in [7]: %time reg['parttype0', 'density']\n",
      "cpu times: user 340 ms, sys: 39.9 ms, total: 380 ms\n",
      "wall time: 378 ms```\n",
      "i loaded the data like this:\n",
      "\n",
      "```import yt\n",
      "from yt.units import kpc\n",
      "\n",
      "fname = 'gadgetdiskgalaxy/snapshot_200.hdf5'\n",
      "\n",
      "unit_base = {'unitlength_in_cm'         : 3.08568e+21,\n",
      "             'unitmass_in_g'            :   1.989e+43,\n",
      "             'unitvelocity_in_cm_per_s' :      100000}\n",
      "\n",
      "bbox_lim = 1e5 #kpc\n",
      "\n",
      "bbox = [[-bbox_lim,bbox_lim],\n",
      "        [-bbox_lim,bbox_lim],\n",
      "        [-bbox_lim,bbox_lim]]\n",
      "\n",
      "ds = yt.load(fname,unit_base=unit_base,bounding_box=bbox)```\n",
      "when i profile accessing a field in a region all the time is spent in i/o routines:\n",
      "```         17023 function calls (16785 primitive calls) in 0.380 seconds\n",
      "\n",
      "   ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.083    0.083    0.204    0.204 io.py:19(_count_particles_chunks)\n",
      "       18    0.081    0.005    0.174    0.010 io.py:99(_read_particle_fields)\n",
      "       85    0.056    0.001    0.077    0.001 dataset.py:476(__getitem__)\n",
      "       51    0.038    0.001    0.038    0.001 {method 'astype' of 'numpy.ndarray' objects}\n",
      "      204    0.016    0.000    0.026    0.000 group.py:253(__getitem__)\n",
      "       34    0.015    0.000    0.070    0.002 io.py:80(_get_smoothing_length)\n",
      "       72    0.010    0.000    0.011    0.000 files.py:150(make_fid)\n",
      "       72    0.010    0.000    0.013    0.000 files.py:403(close)\n",
      "      102    0.010    0.000    0.010    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "      119    0.007    0.000    0.008    0.000 dataset.py:395(__init__)\n",
      "      357    0.006    0.000    0.006    0.000 dataset.py:283(shape)\n",
      "       72    0.005    0.000    0.005    0.000 files.py:95(make_fapl)\n",
      "       34    0.003    0.000    0.005    0.000 attrs.py:56(__getitem__)\n",
      "       18    0.003    0.000    0.121    0.007 io.py:35(_read_particle_coords)```\n",
      "when you do similar stuff for your workflow do you see differences?\n",
      "how do you know that  it's not just doing i/o that takes time when you query those fields?\n",
      "the timings here might depend on the geometry and size of your data of course, not saying that what you're seeing is impossible just that i need more info to be able to understand and maybe help\n",
      "here's a stripped-down example of what i'm doing, which i believe is substantially the same as what you're doing. i'm selecting a generous box around the central galaxy in a zoom:\n",
      "```import yt\n",
      "import caesar\n",
      "\n",
      "obj = caesar.quick_load('/ufrc/narayanan/kimockb/fire2/h113_hr_sn1dy300ro100ss/groups/caesar_0172_z2.000.hdf5')\n",
      "center = obj.galaxies[0].pos\n",
      "\n",
      "ds = yt.load('/ufrc/narayanan/kimockb/fire2/h113_hr_sn1dy300ro100ss/snapshot_172.0.hdf5')\n",
      "region = ds.box(center - ds.quan(75, 'kpc'), center + ds.quan(75, 'kpc'))\n",
      "\n",
      "region['parttype0', 'masses']\n",
      "region['parttype0', 'neutralhydrogenabundance']```\n",
      "```         7551150 function calls (6910456 primitive calls) in 27.731 seconds\n",
      "\n",
      "   ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     2882    6.290    0.002    6.925    0.002 /home/kimockb/python3.8/lib/python3.8/site-packages/h5py/_hl/dataset.py:476(__getitem__)\n",
      "     2820    1.940    0.001    1.940    0.001 {method 'astype' of 'numpy.ndarray' objects}\n",
      "      194    1.805    0.009    6.070    0.031 /home/kimockb/python3.8/lib/python3.8/site-packages/yt/frontends/gadget/io.py:100(_read_particle_fields)\n",
      "        2    1.271    0.636    8.841    4.421 /home/kimockb/python3.8/lib/python3.8/site-packages/yt/frontends/sph/io.py:19(_count_particles_chunks)\n",
      "     2163    1.115    0.001    1.125    0.001 /home/kimockb/python3.8/lib/python3.8/site-packages/h5py/_hl/files.py:150(make_fid)\n",
      "478306/129    0.938    0.000    2.447    0.019 /home/kimockb/python3.8/lib/python3.8/copy.py:128(deepcopy)\n",
      "     1307    0.647    0.000    0.647    0.000 {built-in method io.open_code}\n",
      "     2158    0.545    0.000    0.623    0.000 /home/kimockb/python3.8/lib/python3.8/site-packages/h5py/_hl/files.py:403(close)\n",
      "     6856    0.503    0.000    0.503    0.000 {built-in method posix.stat}\n",
      "     7752    0.474    0.000    0.822    0.000 /home/kimockb/python3.8/lib/python3.8/site-packages/h5py/_hl/group.py:253(__getitem__)\n",
      "25515/8511    0.383    0.000    1.066    0.000 /home/kimockb/python3.8/lib/python3.8/site-packages/sympy/printing/printer.py:251(_print)\n",
      "      896    0.355    0.000    1.786    0.002 /home/kimockb/python3.8/lib/python3.8/site-packages/yt/frontends/gadget/io.py:81(_get_smoothing_length)```\n",
      "yup, it's spending almost all the time doing i/o\n",
      "(the `__getitem__` method for h5py's dataset class)\n",
      "not according to `time`\n",
      "```real\t0m28.432s\n",
      "user\t0m23.244s\n",
      "sys\t0m6.028s```\n",
      "oh ok, so there's more frames to that profile\n",
      "can you run your script with py-spy pls?\n",
      "and share a flame graph with me\n",
      "<https://github.com/benfred/py-spy>\n",
      "you need to install it with pip\n",
      "i see they too have mastered the rust manylinux packaging dance\n",
      "here's an invocation:\n",
      "```py-spy record -n -o profile.svg -- python test.py```\n",
      "you want `-n` so it'll profile cython code too\n",
      "(py-spy is great if you've never used it, best python profiler right now imo)\n",
      "```╰ ➤ py-spy record -n -o profile.svg -- python bench.py\n",
      "\n",
      "error: failed to merge native and python frames (have 64 native and 32 python)```\n",
      "fun!\n",
      "i think you might need sudo on a mac if you're on a mac?\n",
      "maybe run it without `-n`\n",
      "i'm not on a mac\n",
      "seems to work without `-n`. i own the python interpreter, but i'm doing this on an hpc node.\n",
      "not really keen on moving these massive snaps off the cluster but i could do that\n",
      "yeah no worries\n",
      "without `-n` is fine\n",
      "\n",
      "would be even cooler if this didn't profile the imports\n",
      "yeah ok if you read the profile\n",
      "first bit is imports\n",
      "second bit is `ds.index`\n",
      "yup\n",
      "third and fourth bits are the two i/o bits, which are identical\n",
      "and are (looks to me) just basically doing i/o\n",
      "this profile looks 100% expected to me\n",
      "i have a sanity check to do\n",
      "i guess `_count_particles_chunks` could be amortized over both calls\n",
      "which i think <@u042hlt7u> has an open pr for\n",
      "<https://github.com/yt-project/yt/pull/2416>\n",
      "so i guess another option would be to run a patched version of yt with matt's pull request applied\n",
      "it occurs to me another lower-level thing you could do would be to call `get_data` directly\n",
      "something like this:\n",
      "\n",
      "```reg.get_data(fields=list_of_fields_to_read)```\n",
      "and then the field data will be available in the `reg.field_data` dict\n",
      "(note i haven't actually tried this, no idea if it'll be faster)\n",
      "```import yt\n",
      "import caesar\n",
      "import h5py\n",
      "import time\n",
      "\n",
      "obj = caesar.quick_load('/ufrc/narayanan/kimockb/fire2/h113_hr_sn1dy300ro100ss/groups/caesar_0172_z2.000.hdf5')\n",
      "center = obj.galaxies[0].pos\n",
      "\n",
      "ds = yt.load('/ufrc/narayanan/kimockb/fire2/h113_hr_sn1dy300ro100ss/snapshot_172.0.hdf5')\n",
      "region = ds.box(center - ds.quan(75, 'kpc'), center + ds.quan(75, 'kpc'))\n",
      "\n",
      "start = time.time()\n",
      "region['parttype0', 'masses']\n",
      "region['parttype0', 'neutralhydrogenabundance']\n",
      "print(time.time() - start)\n",
      "\n",
      "start = time.time()\n",
      "for i in range(4):\n",
      "    with h5py.file(f'/ufrc/narayanan/kimockb/fire2/h113_hr_sn1dy300ro100ss/snapshot_172.{i}.hdf5', 'r') as f:\n",
      "        f['parttype0/masses'][:]\n",
      "        f['parttype0/neutralhydrogenabundance'][:]\n",
      "print(time.time() - start)```\n",
      "that code at the bottom with h5py is doing _just_ the i/o\n",
      "the times for the two sections in seconds are\n",
      "```18.276262521743774\n",
      "0.33153271675109863```\n",
      "looks like this is a bit faster:\n",
      "\n",
      "```in [10]: %time reg.get_data(fields=[('parttype0', 'density'), ('parttype0', 'masses')])\n",
      "cpu times: user 313 ms, sys: 72.3 ms, total: 386 ms\n",
      "wall time: 383 ms```\n",
      "this is why i'm _really_ incredulous when you say \"that's i/o\"\n",
      "yeah, you're hitting the thing matt's pr fixes\n",
      "right now yt does an i/o pass to figure out how big the arrays we need to load data into should be\n",
      "so it's really doing multiple i/o passes\n",
      "also that's not necessarily a completely fair comparison\n",
      "you'd need to compare with `ds.all_data()`\n",
      "since you're using a subregion yt needs to do multiple i/o operations at disjoint points in each file\n",
      "(not saying that yt will necessarily be as fast as doing the i/o completely manually with h5py either)\n",
      "matt's pr will get things closer though\n",
      "also you can use `reg.get_data` which answers your initial question about batching i/o for multiple fields\n",
      "hope that's all helpful! sorry we're not quite at the level of performance you're expecting\n",
      "if you're up for it applying matt's pr and giving it a test would be very helpful (especially if things are broken!), if you do please leave a comment with your results\n",
      "i'll check it out later. the script that was the cause of this conversation just finished running so it's back to dissertation for me\n",
      "lol\n",
      "<@utge6j7c5> in return for helping you out earlier pretty please can you share the movie you're making? it looks really cool!\n",
      "<@u042fh0rb> sure, it's 13 mb, should i send it to you by email? (actually, <@u043bna00> helped me for the yt rendering)\n",
      "you can put it in here if that’s ok\n",
      "email works too though if you’d prefer it stay private, <mailto:nathan12343@gmail.com|nathan12343@gmail.com>\n",
      "i know i should know this...but how do i turn off the color bar in a simple projection plot?  (not the demeshed `yt` if that makes any difference)\n",
      "p = yt.projection(blah); `p.hide_colorbar()`\n",
      "thanks!\n",
      "relatedly -- if i want to change the label axes on a proj plot i can do something like (from the email archives in a 2014 email from <@u042fh0rb>):\n",
      "\n",
      "```\n",
      "import yt\n",
      "ds = yt.load(filename)\n",
      "prj = yt.projectionplot(ds, 0, 'density')\n",
      "prj.set_colorbar_label('density', 'arbitrary label')\n",
      "```\n",
      "will that automagically change the scale too?  like if i go from g/cm^2 to msun/pc^2 or something?\n",
      "it will if you use `set_unit` to customize the unit of the colorbar\n",
      "like ```prj.set_unit('g/cm**3')``` or something?\n",
      "`prj.set_unit('density', 'g/cm**3')`\n",
      "awesome - thanks for the correction!\n",
      "(i believe this is all covered explicitly in the plotting docs)\n",
      "e.g. <http://yt-project.org/doc/visualizing/plots.html#set-image-units>\n",
      "oh thank you -- i cop to not even having thought to look.   much appreciated\n",
      "has anyone had luck in plotting particles from amrex’s tutorial cases, such as `$(amrex_home)/tutorials/amr/advection_f/exec/singlevortex`? i am mostly having difficulties loading the data effectively across the multiple levels.\n",
      "<@u043bna00> ^\n",
      "(first i’ve heard of this tutorial but i bet andrew can help, it’s pretty late though)\n",
      "this specific tutorial was recently released. i would have suggested another tutorial, but most of them seem broken. sounds good, there is always tomorrow, thank you!!\n",
      "<@ujeeuv7lh> has joined the channel\n",
      "hello all, i'm using yt to create sliceplots of boxlib-type data from an amrex-based code and am getting some weird boxes in the slices that look like this. i've been using the ds.r[] method to interrogate the fields and that's showing some artificially low values as well. however, i've talked to others who've said that they've seen the same thing with yt but not with visit. before i try and do a comparison with visit which i'm not familiar with, have any of you come across something like this, or have any idea what would cause yt to plot a field with these boxes?\n",
      "<@ujeeuv7lh> the blank boxes look like spots where yt isn't finding any data - there are a couple possible reasons. the main one is if your slice maybe is right on a grid edge it can sometime result in that. can you shift the slice by 1e-9 or so and see if a similar thing happens? if so, we should try to fix it in the slice selector in yt. alternately there might be different issue, but if you can clear up the first possibility we can proceed from there.\n",
      "hi, how can i set the x and y limits on a sliceplot? <http://yt-project.org/doc/visualizing/plots.html#slice-plots>\n",
      "they are controlled by the `center` and `width` parameters. there are some examples on the page you linked. you could either set them when creating the plot as `yt.sliceplot(.., center=..., width=...)`, or customizing them later as shown here: <http://yt-project.org/doc/visualizing/plots.html#plot-customization-recentering-resizing-colormaps-and-more>\n",
      "there are several different ways to customize, but the most direct ones are `plot.set_center` and `plot.set_width`.\n",
      "thanks\n",
      "<@u042hlt7u> well i'll be - moving the center minutely seems to have fixed the white boxes. thanks so much! this slice is directly through the center of the domain but it almost certainly lines up with the edges of some of the amr grids. i'll use the offset trick from now on, although it sounds like there may be a deeper issue here in how yt does slices in such an instance that might be worth addressing.\n",
      "i agree - it's definitely something we need to fix! and now i think i can get a test case...\n",
      "hi ! could you please provide a script that produces this error ? if possible, please reduce it to a minimal form.\n",
      "hi! i’ll try to do that, but it will take a little bit of time to make it reproducible because the derived field i’m using calls an f2py function that has to live in another file.  i’ll try getting a minimal version.\n",
      "thanks for recommending this! i now see that `numpy.apply_along_axis` is not the problem.  it probably has something to do with the f2py function i’m calling in the derived field.\n",
      "it’s okay if even the minimal script has dependencies :slightly_smiling_face:\n",
      "you could maybe try replacing the f2py function altogether with a pure python function. it does not have to reprodruce the exact behaviour of the real one, what’s important is that it returns results of the type(s) you’re expecting from it. this is not necessarily applicable here but if it is, it could help identifying the source of your problem.\n",
      "also, if the problem is still not obvious, a good practice is to try and reproduce your error with a fake dataset (try `ds = yt.testing.fake_amr_ds()` or `ds = yt.testing.fake_random_ds()`  for instance), in order to make sure it’s easy for others to reproduce it with no access to your data.\n",
      "thanks again for this advice! when i replace the f2py function with a (much simpler) python function (but still with the `apply_along_axis` call), the code worked fine.\n",
      "great ! then it’s very likely that the f2py function is at fault. i never worked with this package so i won’t be of much more help, but it seems to me you should inspect the output of this function before the point of your script that raises the error\n",
      "just a small update. without the fixes from the pull-request, i managed to get the mean to within a percent from cosmic mean by taking the smoothing length as (`3/4pi*mass/density)**(1/3)`  but decreasing the number of nearest neighbors as much as possible (i went down to 4, with only 2 the code slows down too much) for both illustris and illustristng. i guess this is because these simulations are moving mesh, not sph, so the density is actually the density of each voronoi cell (particle) and not one calculated based on a number of nearest neighbors, like in the sph case with for example eagle\n",
      "with eagle things seem to work fine now with the provided smoothing lengths and 58 nearest neighbors, although it will take a while to finish the entire box, since they don't have a low resolution version\n",
      "<@uelt6g0f6> has joined the channel\n",
      "hey britton, thanks for adding the yt frontend — that does make most of my code much faster, but unfortunately it bogs down elsewhere now (and actually ends up much much slower, though it’s possible that’s at least partially due to how i’m doing the search).  i think i can solve this via a derived field, but i’m kinda hitting a wall on how to do that…\n",
      "\n",
      "so here’s the deal — with this new interface, it’s trivial for me to do 90% of what i want to do, which is re-arrange the data arrays and copy them to new structures.  however, the remaining 10%, indexing the progenitors, is taking far far longer because there’s no progenitor finder in the yt frontend — only a descendant pointer.  so, before, i was able to do something like:\n",
      "```\n",
      "for halt_index, node in enumerate(sorted_nodes):   # list of nodes sorted by uid\n",
      "    ancestors = node.ancestors\n",
      "    # handle ancestors -- basically loop over the list and assign the most massive as the main progenitor of node and all others as co-progenitors of the next most massive progenitor\n",
      "```\n",
      "\n",
      "that `node.ancestors` operation wasn’t cheap, but it wasn’t too bad once the trees were loaded.  now, however, i unfortunately have to do something like:\n",
      "```\n",
      "for halt_index, uid in enumerate(halt['uid']):\n",
      "    ancestors = halt['desc_uid'] == uid\n",
      "    # handle ancestors in a similar way now\n",
      "```\n",
      "since `halt['desc_uid']` contains all the halos over all the trees, this search takes a fair bit of time when it’s repeated over the entire tree.\n",
      "\n",
      "i _think_ that i ought to be able to use derived fields to get around this (particularly if i can save those derived fields and then have them loaded by yt!), but i’m not sure how to access that information in the derived field methodology.  basically, i’m trying to do something like:\n",
      "```\n",
      "def mmp_uid(field, data):\n",
      "    ancestors = data.ancestors  ### this line doesn't work!\n",
      "    # define the uid of the ancestor via vmax or something like that:\n",
      "    return ancestors[np.argmax([n['vmax'] for n in ancestors])]['uid']\n",
      "```\n",
      "\n",
      "is there a relatively easy way that i can get this (or something similar) working?  otherwise, it may actually turn out to best for me to continue working with the ytree interface, which i do have working at a reasonable clip now.\n",
      "thanks!\n",
      "hi shea, unfortunately, the last code example won’t work as the `data` argument coming into the field function isn’t the node itself, but the dict-like object containing its field. you could accomplish that in the following way:\n",
      "\n",
      "```\n",
      "a.add_analysis_field('mmp_uid')\n",
      "for node in list_of_nodes:\n",
      "    node['mmp_uid'] = node.ancestors[...]\n",
      "```\n",
      "\n",
      "you could also similarly create a field that stores the uid of the root node (`node.root.uid`), which would allow you to select all the nodes from a given tree in yt by the common root_uid.\n",
      "\n",
      "however, it looks like you’re setting the progenitor of a given node as the ancestor with the greatest vmax. if that’s true, you could also accomplish this by doing:\n",
      "```\n",
      "a = ytree.load(...)\n",
      "a.set_selector(\"max_field_value\", \"vmax\")\n",
      "```\n",
      "then, a given node’s progenitor line would be:\n",
      "```\n",
      "node = a[0]\n",
      "progenitors = node['prog']\n",
      "```\n",
      "there’s more information on that here: <https://ytree.readthedocs.io/en/latest/arbor.html#customizing-the-progenitor-line>\n",
      "\n",
      "that might still be too slow for you since `node['prog']` will return the full line of most massive progenitors back to the first snapshot, and maybe you only want a given node’s immediate progenitor. if that’s the case, i can probably accommodate that with a small amount of development.\n",
      "\n",
      "of course, let me know if i’ve completely missed the boat on what you’re trying to do. i’ll be back in the us next week and we can have a lower latency conversation.\n",
      "<@u8c5gmzc3> has joined the channel\n",
      "<@uq6fdttal> has joined the channel\n",
      "<@u1tmqqb38> has joined the channel\n",
      "hi, guys, i want to use the sz package in yt to produce some mock images. my simulation gas is sph particles, i used the load_particles to read in the gas particle information, but i am not quite  sure if it is ok. does this sz package require gas information in field instead of particle?\n",
      "if so, how can i load the basic gas particle information and let yt generate these required field information of gas?\n",
      "hi, everyone, i'm using `treefarm` to compute merger-trees for all halos created by the gadget fof halo finder. halo catalogs were created in yt,\n",
      "```data_ds = yt.load('snapshot_000')\n",
      "hc = halocatalog(data_ds=data_ds, finder_method='fof',output_dir='./groups_000')\n",
      "hc.create()```\n",
      "but when i make merger tree in the next code\n",
      "```import yt\n",
      "from treefarm import treefarm\n",
      "ts = yt.datasetseries(\"/groups_*/*.0.h5\")\n",
      "my_tree = treefarm(ts)\n",
      "my_tree.trace_descendents('group',filename=\"all_halos/\")```\n",
      "i got this error\n",
      "```typeerror                                 traceback (most recent call last)\n",
      "&lt;ipython-input-17-9fa3a89e6df6&gt; in &lt;module&gt;\n",
      "      4 ts = yt.datasetseries(\"./home/groups_*/*.0.h5\")\n",
      "      5 my_tree = treefarm(ts)\n",
      "----&gt; 6 my_tree.trace_descendents('group',filename=\"all_halos/\")\n",
      "\n",
      "~/myfile/lib/treefarm/treefarm/treefarm.py in trace_descendents(self, halo_type, fields, filename)\n",
      "    367 \n",
      "    368             if ds1 is none:\n",
      "--&gt; 369                 ds1 = self._load_ds(fn1, index_ptype=halo_type)\n",
      "    370             ds2 = self._load_ds(fn2, index_ptype=halo_type)\n",
      "    371 \n",
      "\n",
      "~/myfile/lib/treefarm/treefarm/treefarm.py in _load_ds(self, filename, **kwargs)\n",
      "    186         load a catalog as a yt dataset and call setup function.\n",
      "    187         \"\"\"\n",
      "--&gt; 188         ds = yt_load(filename, **kwargs)\n",
      "    189         if self.setup_function is not none:\n",
      "    190             self.setup_function(ds)\n",
      "\n",
      "~/myfile/lib/treefarm/treefarm/utilities/io.py in yt_load(filename, **kwargs)\n",
      "     13     if level &gt; 10 and level &lt; 40:\n",
      "     14         ytlogger.setlevel(40)\n",
      "---&gt; 15     ds = _yt_load(filename, **kwargs)\n",
      "     16     ytlogger.setlevel(level)\n",
      "     17     return ds\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/yt/convenience.py in load(*args, **kwargs)\n",
      "     84     candidates = find_lowest_subclasses(candidates)\n",
      "     85     if len(candidates) == 1:\n",
      "---&gt; 86         return candidates[0](*args, **kwargs)\n",
      "     87     if len(candidates) == 0:\n",
      "     88         if ytcfg.get(\"yt\", \"enzo_db\") != '' \\\n",
      "\n",
      "typeerror: __init__() got an unexpected keyword argument 'index_ptype'```\n",
      "it's seems like there is no 'group' type in the catalogs, does anyone knows what's wrong with this?\n",
      "hi <@ubykep7ek>, i’m happy to help out with this. i’m at a workshop now, but i’ll send you a message when i have time to look at this.\n",
      "hi <@ubykep7ek>, is it that you’d like to save a 2d array of density vs. temperature profile for each halo? i don’t think that’s supported yet, but one solution may be to save the profiles with yt’s `save_as_dataset` command and then have the analysis field be the path for that file? if it’s saving the full profile in the tree file you’re keen on, then we can chat about how to possibly implement that.\n",
      "yes, i want to save density-radius and temperature-radius profiles for halos. right now i am saving these profiles in a dictionary with treenodes as keys, but it would be simpler if they are saved in the same file as the arbor.\n",
      "\n",
      "i noticed that the derived fields can be added in xyz vector forms. similarly if analytic vector fields with predefined size can be implemented, that would be great. if it's not going to be too complicated to do that, i am interested in doing that.\n",
      "<@u011r0cajmb> has joined the channel\n",
      "<@ukmhhd7ra> has joined the channel\n",
      "can we have  2d plots in volume rendering? i want to visualize the several orthogonal crosssections of 3d data.\n",
      "<@ulqt4pxbp> you have to manually composite it, but it has been done!  i think your best bet wouldbe to save the pngs and then composite in something like graphics magick or with pil/pillow\n",
      "sorry for being persistent here but i've noticed another thing. by inspecting only the fields of the rockstar_ds (again the halo_catalog object played a role in its creation) i find again something that looks like a discrepancy. for example the number of sub-halos (in a spesific halo) found by the rockstar is 1785. then if i compare the fields: particle_velocity and particle_bulkvel i find something strange. that is, 1486 items seem to match in these fields while in the rest i find differences (v - v_bulk) even at the order of 10%. these differences make it hard for me to rely on the outputs because i don't fully understand them. i hope that i'm not annoying you and that there is a meaning to my questions. i you believe that they are not relevant to this thread please tell and i will stop.\n",
      "hi there, we’re glad to help out. would you be able to post the script you’re using somewhere so i could take a look? the easiest would probably be to do `yt pastebin scriptname` and then post the link here.\n",
      "<@ueq7zhb0c> has joined the channel\n",
      "hi all. i recently did a system update on my imac and as a consequence, i am now receiving an error when attempting to create a sliceplot of an enzo output. i've already completely re-installed anaconda, yt, h5py, python, the works and i have tried multiple versions as well. here is the error: <https://pastebin.com/dbrzfrps>\n",
      "i am, however, able to load the same data file on a different computer, with the same version and build of anaconda, yt, and h5py.\n",
      "interesting\n",
      "the exact same dataset?\n",
      "can you only reproduce it on that one system?\n",
      "it might be an issue with the filesystem on the computer where it’s crashing\n",
      "i’ve never seen something like this error before\n",
      "interestingly, this is in the traceback: \"~/anaconda3/lib/python3.7/site-packages/yt/frontends/enzo/io.py in _read_obj_field(self, obj, field, fid_data)\n",
      "    159                 return data.t\n",
      "    160             raise\n",
      "--&gt; 161         dg.read(h5py.h5s.all, h5py.h5s.all, data)\n",
      "    162         # i don't know why, but on some installations of h5py this works, but\n",
      "    163         # on others, nope.  doesn't seem to be a version thing.\"\n",
      "that comment refers to a commented out line below the line that’s failing for you\n",
      "any chance you can share the dataset that’s dying for you?\n",
      "also what yt and h5py versions are you using?\n",
      "errno=14 happens when you try to fopen() a file outside of your usable address space\n",
      "possibly - also, it might actually be the dataset itself, since the computer on which i *thought* i had fixed the issue just threw this error. i'm running yt 3.4.1 and h5py 2.8.0.\n",
      "`bytes actually read = 18446744073709551615` is also pretty suspicious :slightly_smiling_face:\n",
      "that’s a lot of bytes :wink:\n",
      "you can share datasets using the `yt upload` command line helper\n",
      "`yt upload my_dataset.tar.gz`\n",
      "yes, i'm very confused by the error message. the datafiles are not gigantic, on the order of a few tens of mb. however, i am accessing them over via sshfs.\n",
      "that’ll print out a url once it’s finished uploading you can share\n",
      "it’s getting pretty late, i’ll poke at this tomorrow if you can share the file\n",
      "might be an enzo issue, especially if you produced this file with a modified version of enzo\n",
      "might be a yt issue and you’ve hit some corner case in the enzo i/o routines\n",
      "might just be a corrupt file?\n",
      "might be a filesystem issue?\n",
      "hard to say :slightly_smiling_face:\n",
      "good night\n",
      "ah yes, 18446744073709551615 is 2^64 -1 :slightly_smiling_face:\n",
      "i should try to memorize that one\n",
      "ok - i think i *may* have discovered the issue. i found a previous commit in which things were plotting nicely, which existed only on the branch for my laptop, go figure. so, yt does not play well with writeghostzones = 1 parameter in enzo, which was floating around some of my parameter files. past-rebecca likely thought about inspecting the ghost zones and then promptly forgot about it. my apologies for spamming the help channel!\n",
      "ah yes, in principle that should at least fail with a much less crappy error message though!\n",
      "i think we even have support for that, although it’s definitely not tested, so i’m not surprised it’s broken\n",
      "if you can make a bug report about this we can at least make the error message less crappy or maybe even add support for reading in the data\n",
      "<@ubju11gju> has joined the channel\n",
      "when i try to get the divergence of a field in my non-periodic dataset that i load via load_uniform_grid, i get an out of bounds regardless of what sort of region i try and specify. is this just because the streamdataset will always have to use the whole grid/there isn't any chunking?\n",
      "example script and error: <https://pastebin.com/hza70azp>, <https://pastebin.com/qxnbpnga>\n",
      "just set `ds.periodicity=(true, true, true)`\n",
      "as a hack\n",
      "what’s happening is that your field is selecting data outside the domain to fill in ghost zones\n",
      "cool that works\n",
      "i guess i'm surprised that was happening even with a smaller region though? does it compute the quantity for the whole domain even if i'm just asking for part?\n",
      "no, it should work with a sufficiently small region\n",
      "it seems like any size of region will break in my script, except one that doesn't contain any data\n",
      "but this doesn't really matter, i can just change the periodicity, i suppose it doesn't effect anything else\n",
      "that sounds vaguely like a bug? i can try to take a look i guess\n",
      "oh i think i know what’s happening\n",
      "you only have one grid\n",
      "so it’s getting the data for the entire grid\n",
      "it doesn’t matter if you only select a subset of that grid, yt still treats it as a single chunk\n",
      "okay cool thats what i figured\n",
      "hi, i have an enzo data that outputs some extra field f which is supposed to be a kind of density field. however, yt reads f as a dimensionless field. is there a simple way to add unit to my field f? thanks.\n",
      "not for the field f itself, that’s something we want to make easier in yt 4.0 though\n",
      "what you can do now is define an alias field that attaches the correct units\n",
      "for example, here’s an alias of density that we force to have units of `'erg'` (even though that’s wrong, it’s just for illustration):\n",
      "```\n",
      "def my_density_alias(field, data):\n",
      "    dens = data['gas', 'density']\n",
      "    # strip units without copying\n",
      "    dens = dens.view(np.ndarray)\n",
      "    return data.ds.arr(dens, 'erg')\n",
      "\n",
      "ds.add_field(('gas', 'density_alias'), function=my_density_alias, units='erg', sampling_type='cell')\n",
      "```\n",
      "for your field you could skip the stripping units part, since it’s already getting returned as dimensionless\n",
      "it works, thank you. <@u042fh0rb>\n",
      "in yt4.x are there any methods for plotting the voronoi mesh over (e.g.) a projection plot of arepo data.  something like:\n",
      "\n",
      "<https://yt-project.org/docs/dev/cookbook/complex_plots.html#plotting-grid-edges-over-fluids>\n",
      "no, we've talked about that for slices (where it makes more sense i think?) but i don't think it really makes sense to do that for projections\n",
      "but we don't do it for slices either\n",
      "i think john has thought about it a little\n",
      "oh yeah - a slice would make more sense you're right\n",
      "it works in projections of amr data because there's symmetries along the coordinate axes, not so much for voronoi meshes\n",
      "other quick question: is there a way to mute yt ? (avoiding the non-error messages like `[info]` or `[warnings]`\n",
      "you can set that using `yt.funcs.mylog.setlevel(50)`! the various amounts of logging can be found here <https://yt-project.org/doc/faq/index.html> .\n",
      "perfect, thank you very much !\n",
      "hi all, is it possible to use the volume rendering functionality if i interpolated particle data onto a grid, and then loaded it into yt? or has volume rendering been implemented for particle data yet?\n",
      "<@u0152dmhadp> has joined the channel\n",
      "this message was deleted.\n",
      "<@uppavuhsm>, projections and slices are fundamentally different operations, so it’s not inconceivable that they give very different views. the projection samples data all along the line of sight, so for example, you’ll get a full view of that angled disk. the slice is sampling data at a single distance along the line of sight, so in the example above, much of that disk is in the foreground or background and won’t be seen. this is also why adding the data_source doesn’t change much in the slice example. the same grids are being sampled because your sphere intersects the plane of the slice.\n",
      "\n",
      "one thing worth trying is to make slices and projections that are perfectly face on with the disk. you can do this by computing the angular momentum vector for the sphere, then using that to make off-axis slices and projections. have a look here for more info:\n",
      "<https://yt-project.org/docs/dev/visualizing/plots.html#off-axis-slices>\n",
      "<https://yt-project.org/docs/dev/visualizing/plots.html#off-axis-projection-plots>\n",
      "the necessary bits of the `cykdtree` package were pulled into the yt source at some point. from a conda environment, you should just be able to do:\n",
      "```conda install numpy cython\n",
      "cd yt\n",
      "pip install -e .```\n",
      "thanks so much, i've done this already but the issue is that particle plots can't be done off axis so i'm trying to keep it consistent between projections of the amr grid, the gas distribution and the stars,  and this off axis slice thing was becoming issue with the amr grid picture, as a projection makes no sense and neither did the slice\n",
      "is it not possible to turn on minor colorbar ticks for off axis slice plots?\n",
      "doesn’t this produce the expected result ?\n",
      "`plot.set_colorbar_minorticks(field, false)`\n",
      "\n",
      "noop\n",
      "ok, this is a bug\n",
      "which version of yt are you running ?\n",
      "3.5.1\n",
      "any reason not to upgrade to 3.6 ?\n",
      "im running on a shared cluster so i would need to ask the guy who runs it - wasn't aware there was an update available as well\n",
      "will ask him now, and let you know tomorrow if this is still a bug\n",
      "i’ll check to see if the bug is still there in 3.6 right now\n",
      "checked\n",
      "well. in 3.6.0, colorbar minorticks are actually _broken._ the class method is here for offaxissliceplot but it’s actually impossible to *have* colorbar minorticks.\n",
      "this is a bug i recently fixed on the master branch, however not in time for the 3.6.0 release\n",
      "since you don’t have the possibility to use the master branch, you’re in a position where no release has this feature in a stable state, sorry about that ! i’d recommend updating to 3.6 nonetheless because a lot of _other_ bugs were fixed in the mean time, but it’s up to you.\n",
      "no problem, thank you so much anyway\n",
      "thanks!\n",
      "\n",
      "i've reproduced my heisenbug\n",
      "this throws a notimplementederror on my end. can anyone confirm?\n",
      "yup, i can hit that too\n",
      "i bet i know what’s happening\n",
      "thanks for the report, i’ll try to take a look this afternoon\n",
      "<https://github.com/yt-project/yt/issues/2069>\n",
      "<https://i.imgur.com/i1jkgvc.png>\n",
      "fixed i think\n",
      "have some more cleanups to do though\n",
      "thanks again for the report!\n",
      "oo nice\n",
      "<@u012pg4blkx> has joined the channel\n",
      "if you need to sample the field at the location of many (or all) particles, you can instead use the mesh sampling method (see <https://yt-project.org/doc/analyzing/fields.html#mesh-sampling-particle-fields>) that should be available in yt 3.6\n",
      "thank you all.\n",
      "<@u013g0wrvu0> has joined the channel\n",
      "hey! thanks a lot for your response. i figured out the sources of the errors. `/yt/build/lib.linux-x86_64-3.6/yt/`  folder was empty.  could be a git clone error, i'm not sure why these files were missing. but i copied the files from an older version of yt-4.0 (from february) i had downloaded. and it installed properly.\n",
      "\n",
      "are there any concerns about simply copying the build files from an older version (4 months old)?\n",
      "well i can think of one: it’ll be virtually impossible to reproduce your environment if you ever need assistance with a bug/error that we can’t reproduce straighforwardly (seems like a very unlikely situation though). that being said, the actual `4.0` *release* is on its way so the days of installing from source will soon be over. i would recommend you stay tuned for when that happens :slightly_smiling_face:\n",
      "congratz for figuring it out btw !\n",
      "<@ubq983866> has joined the channel\n",
      "<@ubrh47d63> has joined the channel\n",
      "\n",
      "```\n",
      "plot = phaseplot(ds, \"density\", \"temperature\", [\"mass\"], weight_field=none)\n",
      "plot.set_log(\"density\", true)\n",
      "plot.set_log(\"temperature\", true)\n",
      "plot.save('phase.pdf', mpl_kwargs={'bbox_inches':'tight'})\n",
      "```\n",
      "this seems like a plain phaseplot, almost straight from the example in the docs, but something's going wrong with the colorbar and some other labels on the right. any ideas?\n",
      "<@u91855pa9> ahhh, recent matplotlib had a strange change\n",
      "and <@u042fh0rb> fixed it in a recent yt\n",
      "i'll update, that'll probably do it\n",
      "i get the impression that “recent matplotlib had a strange change but <@u042fh0rb> fixed it in a recent yt” is a significant fraction of yt development\n",
      "i like to think folks are also doing some fun things with data handling / indexing and whatnot, like the sph overhaul, but point taken.  :slightly_smiling_face:\n",
      "wow, you're not kidding! seriously is.\n",
      "sorry did not check in earlier…did you solve your problem…?  sounds like you have a function `f(p, rho, …)` which gives you the intersection point? i would just add this function as a new field\n",
      "<@uqw7p0t33> has joined the channel\n",
      "quick question, how does one change the unit displayed for a given field’s colormap, e.g in a sliceplot ? (frm `g/cm***3` to* `kg/m**3` for instance)\n",
      "```p = yt.sliceplot(...)\n",
      "p.set_unit(&lt;field&gt;, &lt;units&gt;)```\n",
      "i believe…\n",
      "perfect, thanks !\n",
      "i’ve never seen this before, you should file an issue\n",
      "we’d need more details about your mac setup probably\n",
      "e.g. where did gcc come from? xcode/homebrew/macports?\n",
      "also how are you building yt exactly?\n",
      "i found a way to get around this, so i closed the issue. <@u91855pa9>'s solution almost worked, though `-stdlib=libc++` only applies for osx, not macos, so you have to run `export macosx_deployment_target=10.10` first. thanks for your help!\n",
      "i'm suspicious of the fact that you had to hack around anything here. people shouldn't have to do that.\n",
      "unfortunately apple has made development on macs increasingly difficult for several years now\n",
      "probably this is something that cython upstream needs to handle\n",
      "we can't do much about it\n",
      "well, beyond dropping c++ code, but that's a big ask (ewahboolarray wraps a c++ library)\n",
      "the whole c++ stdlib situation on macs is a huge headache\n",
      "<https://github.com/cython/cython/issues/2694>\n",
      "rkingery in that issue is also using anaconda, so that might have something to do with it too\n",
      "i don't use conda on macs, partially because of headaches like this\n",
      "so it might also be an issue that needs to be fixed in conda or some conda package\n",
      "if someone can reproduce the issue and dive and debug, that would be great, it looks like plenty of other people are hitting the issue outside of yt\n",
      "hello (again) world! is it possible to make a profile plot of a subset of data?\n",
      "i've been able to make profiles with `all_data` but i get a blank screen (and profile.x is an empty array) if i try using ds.disk.\n",
      "actually, i managed to select the subregion i wanted with cut_regions :slightly_smiling_face:\n",
      "i cannot recall, is there a good way to effectively “remove” a derived field from a dataset? i can remove it from `ds.derived_field_list` and from `ds.field_info` but if i redefine a derived field with the same name and a different definition, it just remembers the old one.\n",
      "this is a sort of way of manually overriding a derived field definition.  in trident, we removed the `force_override` keyword when adding new fields because of recursion errors in field definitions that were causing all sorts of problems.\n",
      "i just figured if we manually deleted the references to an existing derived field, we could add a new one without problems, but i’m having problems actually achieving this.  any ideas on how to make this happen?\n",
      "i think `force_orverride` is the way to do it\n",
      "if that's not working then it's a yt bug\n",
      "i cannot use `force_override` and was instead trying to figure out if there was a way to just remove references to the existing field.\n",
      "i don't think there's an api to do that\n",
      "i don’t even need an api to do it.  i’m just looking to do it.\n",
      "right, well, the issues you're running into would need to be fixed, presumably by adding an api that does what you need it to do\n",
      "you may be the first person to try to do this\n",
      "nathan, i know what i’m doing is weird and novel.  i don’t need an api.  i’m just asking *where* fields are formally defined for a dataset and if it is possible for me to go into the code and remove references to that field so that i can achieve my goal here, effectively removing a derived field from a dataset.\n",
      "ok, well, i'm sorry i don't know offhand\n",
      "you could look at what `force_override` does, i think it is doing most of what you need to do\n",
      "but if that's buggy it would be nice to get a bug report about it\n",
      "yeah, i’ve been looking at it a bit.  i’ll continue poking.\n",
      "i think you might run into issues later messing with yt internals\n",
      "e.g. if we change something in yt in the future, that might break trident\n",
      "it would be less brittle if there were a public, tested api that trident could use\n",
      "it’s not buggy as far as i know.  as i said, the issue is on the trident side because ion fields are generated dependent on each other, and we were running into recursion loops when we allowed `force_override` to work.\n",
      "i think what you need it he contents of the `fieldinfocontainer` dict?\n",
      "but yt's field system is very complicated\n",
      "i’m not trying to make a hardcoded solution here yet.  i’m just looking for a way to remove a derived field from a dataset.  if i come up with a solution, i will consider adding it into the yt api or discussing it here to see if it passes muster.  just wanted to see if others had ideas of a solution before i deepdive\n",
      "so just removing an entry from that dict might not be sufficient\n",
      "right, that is what i found was true.\n",
      "removing it from the dict and from `ds.derived_field_list` doesn’t seem sufficient.  there is some memory of the field beyond that.\n",
      "i’ll see what i can find.  thanks for the discussion.\n",
      "if you can make a short script that demonstrates the issue i can try poking around\n",
      "here is a short script demonstrating the issue.\n",
      "<http://paste.yt-project.org/show/202/>\n",
      "i’ll keep digging to see if i can make headway on this manual override.\n",
      "this is with yt-4.0, right?\n",
      "yup.\n",
      "oh, i guess trident isn’t even needed for this script.\n",
      "i'm still compiling but i bet if you did `ad.field_data.clear()` you'd get the right answer in the second field access\n",
      "oh! i will give it a shot.\n",
      "yeah that does it\n",
      "the `field_data` caches on data objects are just plain old dicts\n",
      "it would probably be better if they were keyed on more than the field name, e.g. some sort of hash of the field definition itself too\n",
      "unfortunately cache invalidation is hard :confused:\n",
      "oh interesting, so the `clear()` call wipes the cache for an object.\n",
      "that’s super helpful!\n",
      "thanks, nathan!\n",
      "one wrinkle for you is that trident doesn't necessarily control all of the data objects that a user might create\n",
      "that’s true.\n",
      "i think this will work in a pinch.  but i don’t imagine there is a dataset-wide version of that `field_data.clear()` command, right?\n",
      "it’s just object by object?\n",
      "because if there were a dataset wide solution, it might be worth me making an api for this based on what i wrote in the script, for removing a derived field from a dataset.\n",
      "(for me to do this, not you)\n",
      "no, there isn't, i don't think there could be either given how it's written\n",
      "there's no way to go from a dataset to all of the data objects that depend on that dataset\n",
      "ok.\n",
      "thanks for the help, nathan.\n",
      "much appreciated!\n",
      "<@u042j5bn6> i synthesized this into <https://github.com/yt-project/yt/issues/2458>\n",
      "thanks, <@u042fh0rb>!  i’ll comment on it there.\n",
      "<@ugbuvtqs3> has joined the channel\n",
      "where are the keyword arguments for yt.load documented? i'm loading amrex data and i'd like to suppress the verbosity.  with just a simple yt.load call, i get messages like this:\n",
      " ```yt : [info     ] 2019-02-19 08:40:55,701 parameters: current_time              = 1.25\n",
      "yt : [info     ] 2019-02-19 08:40:55,701 parameters: domain_dimensions         = [ 50 200  20]\n",
      "yt : [info     ] 2019-02-19 08:40:55,702 parameters: domain_left_edge          = [ 0.   0.  -0.2]\n",
      "yt : [info     ] 2019-02-19 08:40:55,702 parameters: domain_right_edge         = [0.5 2.  0. ]```\n",
      "that doesn’t get suppressed via keyword arguments to yt.load, you can suppress it via the yt config file though\n",
      "<http://yt-project.org/doc/reference/configuration.html#the-configuration-file>\n",
      "in particular the suppressstreamlogging option\n",
      "you can also do it inside a script,\n",
      "<http://yt-project.org/doc/faq/index.html#how-can-i-change-yt-s-log-level>\n",
      "to answer your specific question, load passes keyword arguments to the `__init__` method of the `dataset` subclass that ultimately gets loaded\n",
      "<https://github.com/yt-project/yt/blob/master/yt/frontends/boxlib/data_structures.py#l1593>\n",
      "<@u7ku54sg5> has joined the channel\n",
      "actually this doesn’t work,  i tried it with other frontends, maybe there’s a typo ?\n",
      "i think you need to make an offaxissliceplot to do that at the moment\n",
      "i didn't actually try it, this has come up on the mailing list before many times, can you try searching?\n",
      "sliceplot makes a bunch of assumptions about your data and how you want it displayed, if you want to make very custom plots then you can always get the image buffer from the plot and then plot that image however you wish\n",
      "ah the reason it didn't work is because the code was wrong for what you were trying to do, you actually want:\n",
      "```\n",
      "ds.coordinates.x_axis[2] = 1\n",
      "ds.coordinates.x_axis['z'] = 1\n",
      "ds.coordinates.y_axis[2] = 0\n",
      "ds.coordinates.y_axis['z'] = 0\n",
      "```\n",
      "slices of 2d simulations are always slice through z, so the keys to the dictionary you want to change are the keys for slices through z\n",
      "i don't actually know for sure if it's possible to do what you're trying to do with offaxissliceplot, you'll need to try\n",
      "how would i do this ? (getting the image buffer)\n",
      "i’ll try\n",
      "plot.frb[('gas', 'density')]\n",
      "or whatever field you're plotting\n",
      "you can create the fixedresolutionbuffer directly without going through the plot first as well\n",
      "this is just the api if you already have a plot\n",
      "sorry i wasn’t aware there was an archive for the mailing list, will try to search there first from now on\n",
      "and thanks for the update\n",
      "<https://mail.python.org/archives/>\n",
      "err <https://mail.python.org/archives/list/yt-users@python.org/>\n",
      "there's one for yt-dev as well\n",
      "nice, i’ll subscribe to some of them then :partyparrot:\n",
      "thanks for the heads-up with this. i thought i had removed all the extern code. we don’t need it anymore since python2 support has been dropped. if you’d like to issue a pr to remove the bits that use `six` (i think it’s just `string_types`), that would be awesome.\n",
      "hi, i'm yajie.\n",
      "i'm trying to plot surface density radial profile. i found yt.create_profile() creates a radial profile but not for the surface density, and yt. projectionplot() calculates the surface density but plot in countour. is there a way to realise this? can i somehow get the integrated density value from projectionplot?\n",
      "not straightforwardly, no, although it’s straightforward to calculate from the projectionplot image\n",
      "```\n",
      "plot = projectionplot(...)\n",
      "image = plot.frb[plotted_field]\n",
      "```\n",
      "frb is a fixedresolutionbuffer object which you can create and customize separately from the projectionplot, see the yt docs for details\n",
      "that's helpful, thanks!\n",
      "<@utcacpwbz> dunno if you're still around, but what's the output of `cat /proc/cpuinfo` on your machine?\n",
      "hi, it's long but i send you the output\n",
      "ivy bridge, from 2013\n",
      "cool, thank you :slightly_smiling_face:\n",
      "one of the conda-forge maintainers was also able to reproduce it, hopefully there'll be a fix soon\n",
      "if you happen to have a server with newer cpus available it'll probably work there, you can also downgrade gmp to 6.1.2 with conda and it should work\n",
      "yes, the same version of yt properly works on a newer cluster. my machine is bit old.\n",
      "it's likely a bug in the c compiler that conda-forge uses, but also it's a bug in gmp for the compiler to produce incorrect code like this. fun bug!\n",
      "<@utbky6vrb> has joined the channel\n",
      "it gets clear! how can i downgrade gmp to 6.1.2?\n",
      "`conda install -c conda-forge gmp==6.1.2`, i think?\n",
      "thank you, i try this.\n",
      "by downgrading gmp i successfully import yt! thank you so much!\n",
      "i know i've done this before, but i can't seem to figure it out or find it in the documentation at the moment:\n",
      "does anyone know offhand how to specify the dpi of a plot you're saving?\n",
      "i have a 4000 by 4000 frb, i'd like to get a png with that resolution, i guess i need to specify the figure size and the dpi\n",
      "i think it’s `set_buff_size(4000)` or `set_buff_size((4000, 4000))`\n",
      "i think that one gets the frb to be 4000 by 4000, but it looks like the image still gets saved at lower resolution\n",
      "i think i have to pass the right figure size and dpi to matplotlib\n",
      "right, yes\n",
      "looks like `save` accepts a dict called `mpl_kwargs`\n",
      "so i think you could do `.save(mpl_kwargs={\"dpi\": 123})`\n",
      "or whatever the matplotlib kwarg to `savefig` is\n",
      "thanks <@u042s6y2g>! that helps a lot\n",
      "no prob!\n",
      "yes, that’s exactly right\n",
      "we should have a doc page explaining this point because it comes up a lot\n",
      "it’s really a matplotlib thing :confused:\n",
      "if i want to overwrite an alias field, do i simply have to remove it from the `field_info.field_aliases` and  `field_info` itself before adding a new one?  or are there other little corners that it’s stored?\n",
      "\n",
      "```\n",
      "field_info.field_aliases.pop((ftype, field))\n",
      "field_info.pop((ftype, field))\n",
      "```\n",
      "does force_override not work?\n",
      "`alias` doesn’t seem to have a force_override.\n",
      "<https://github.com/yt-project/yt/blob/master/yt/fields/field_info_container.py#l300>\n",
      "sorry, a little more context about what you’re doing\n",
      "you’re doing this in a frontend?\n",
      "force_override is an argument to add_field\n",
      "in trident, we add new field for different ion species.  sometimes we add neutral ion species, in which case we need to alias `('gas', 'h_p0_number_density')` to `('gas', 'h_number_density')`.\n",
      "but it sounds like you’re not writing a derived field\n",
      "which is all fine and good most of the time\n",
      "but we allow the user to `force_override` the native fields sometimes, e.g., h i number density.\n",
      "ah ok, i guess if the alias already exists calling alias again should replace it?\n",
      "yeah, it doesn’t seem to\n",
      "you’re probably the first person who has tried\n",
      "yeah, i realize this is a niche corner case\n",
      ":slightly_smiling_face:\n",
      "feel free to send in a pr to patch alias\n",
      "well, that’s just it.  when i patch alias, it doesn’t seem to fix my problem.  i guess i’m patching it wrong.\n",
      "if i had it working, i’d pr it.\n",
      "ah i see\n",
      "i don’t know offhand where else to look\n",
      "i’d look at how the field aliases are implemented and poke around in a debugger\n",
      "well, in the `field_info` object, are there any other locations where a field or alias field gets stored?\n",
      "besides just `field_info[field]`?\n",
      "you mean a `fieldinfocontainer`?\n",
      "yeah.\n",
      "looks at the code\n",
      "i only saw it being stored in `fieldinfocontainer[field]` and `fieldinfocontainer.field_aliases[field]`.\n",
      "yes, i think that’s right\n",
      "so i figured if i popped those, i’d be golden.\n",
      "ok, just making sure i wasn’t missing something.\n",
      "can you push what you have somewhere?\n",
      "sure.\n",
      "actually, i can’t look at this immediately\n",
      "so that won’t actually help\n",
      "that’s ok.\n",
      "this conversation has been helpful that i’m not just missing something obvious\n",
      "i’ll keep poking at it and let you know if i figure it out.\n",
      "and pr it if i do.\n",
      "could it be the other call to `alias` in add_field?\n",
      "on line 266 of field_info_container.py\n",
      "```\n",
      "       if (ftype, name) not in self:\n",
      "            tuple_name = (ftype, name)\n",
      "            self[tuple_name] = derivedfield(tuple_name, sampling_type, function,\n",
      "                                            **kwargs)\n",
      "            self.alias(name, tuple_name)\n",
      "        else:\n",
      "            self[name] = derivedfield(name, sampling_type, function, **kwargs)\n",
      "```\n",
      "oh!\n",
      "that’s possible.\n",
      "good call.\n",
      "thanks, i’ll try that.\n",
      "grumbles about getting rid of string field names\n",
      "yeah…\n",
      "i wish we’d get rid of `h_number_density` and just have `h_p0_number_density`\n",
      "that would make my life much easier\n",
      "and be less confusing to the uninitiated\n",
      "probably worth having a discussion about that\n",
      "we can at least deprecate h_number_density\n",
      "if people agree\n",
      "yeah, that would be great.\n",
      "i think the issue is the following\n",
      "when you call `ds.field_info.alias`, it doesn’t automatically add to `ds.derived_field_list`\n",
      "i saw this a long time ago and can’t remember quite why\n",
      "i think because the `derived_field_list` is created at later stage and so doesn’t necessarily exist yet\n",
      "well, wait, maybe that’s not this specific issue, but it’s a thing that does happen when you add aliases manually\n",
      "yeah, that’s fair, but we manually add the field to the derived field list\n",
      "it’s just a field name, no?\n",
      "not the actual field definition.\n",
      "yeah, i guess it’s unrelated, but maybe this is technically a bug if `alias` is to be considered a user-facing function\n",
      "it’s not\n",
      "at least imo :slightly_smiling_face:\n",
      "ok fair enough\n",
      "related to halo finding: i'm having a bit of trouble understanding from the worked halo analysis example (<http://yt-project.org/doc/cookbook/halo_analysis_example.html#halo-analysis-example>)  how to access one of the default quantities.  say, `particle_mass`\n",
      "i see:\n",
      "\n",
      "```\n",
      "in [10]: hc.quantities\n",
      "out[10]:\n",
      "['particle_identifier',\n",
      " 'particle_mass',\n",
      " 'particle_position_x',\n",
      " 'particle_position_y',\n",
      " 'particle_position_z',\n",
      " 'virial_radius']\n",
      "```\n",
      "though this appearas to be a list, not a dict of quantities\n",
      "relatedly: is it possible to find out what the particle id's are that go into a given halo?\n",
      "<@u046k2qnk>, each halo being processed by the halo catalog will have a dictionary called `quantities` with those keys\n",
      "so after ```hc.create()```  something like:\n",
      "\n",
      "```\n",
      "halo = hc.halo_list[0]\n",
      "print halo['quantities']\n",
      "```\n",
      "?\n",
      "so, if you write some halo callback function:\n",
      "```\n",
      "def some_callback(halo):\n",
      "    halo.quantities['particle_mass']\n",
      "```\n",
      "yeah, your example should also work\n",
      "the new catalog saved to disk will also save all those quantities as fields\n",
      "as for getting the member particles, they are not exposed to the user at the moment, but they are also not far under the hood\n",
      "thanks <@u042s6y2g>!  do you  know off hand where in the halo_analysis codes these are?   i might try to see if i can find these\n",
      "ideally i'd love to to use these to track halos from snapshot to snapshot (i..e follow the growth of an individual halo)...but would need to match particle ids (i think?) to make sure i'm connecting the right halos\n",
      "though maybe there's some functionality for what i just described and i'm not seeing it?\n",
      "and we’re back\n",
      "<@u046k2qnk>, the place to look is `yt/analysis_modules/halo_finding/halo_objects.py` for instances of ‘particle_index’\n",
      "another thing that could work would be to run yt’s rockstar using just the gas particles. you could then use consistent-trees as a way to track objects\n",
      "and load it with ytree (<http://ytree.readthedocs.io|ytree.readthedocs.io>)\n",
      "oh those are good ideas -- thanks!\n",
      "whoops - sorry all for posting a dm to someone else in <#c1ycpsx08|help>\n",
      "<@u043bna00> has joined the channel\n",
      "i tried to make a sliceplot of a field that is all zeros. here's what i get:\n",
      "\n",
      "i thought it used to detect this and switch to linear scaling?\n",
      "it should?\n",
      "can you point me to roughly where in the codebase that happens?\n",
      "<https://github.com/yt-project/yt/blob/master/yt/visualization/plot_window.py#l785-l807>\n",
      "ah well then:\n",
      "\n",
      "i guess we could check if the dynamic range is tiny instead of zero\n",
      "for some value of tiny that might require thinking more than you’d like about floating point math\n",
      "`np.finfo(image.dtype).eps`?\n",
      "maybe even more generous than that, we don’t want the colorbar choice to switch randomly when someone is making a movie, but if the dynamic range is like 0.1 or smaller we probably shouldn’t be using a log scale colorbar anyway\n",
      "but i guess if we make an arbitrary choice then we might produce weird behavior where the scaling changes as someone is making a movie\n",
      "but yeah, that would fix your issue at least :slightly_smiling_face:\n",
      "i’m just thinking out loud\n",
      "i also eventually want to make it so that if you’re plotting a field that goes through zero we automatically choose a diverging colormap centered at zero\n",
      "but that would require metadata in the field system to work nicely i think\n",
      "isn't the only way checking on eps will cause the colorbar to switch when making a movie is if in some of the frames the dynamic range is round-off level tiny\n",
      "yeah, i’m just spitballing that if we chose a much larger cutoff that it might lead people to on average get nicer plots\n",
      "i agree with you the movie thing isn’t an issue if you make the cutoff really tiny\n",
      "i’m not sure eps is the right answer though\n",
      "because what matters is the ratio between the min and max values in the array\n",
      "and floating point numbers don’t have even spacing across the range of floating point numbers, so the appropriate ratio to choose depends on the min and max\n",
      "np.nextafter() might be useful here\n",
      "it looks like, in this case, there actually is a representable number in between 0 and my nanmax: `np.nextafter(np.nanmin(image), np.nanmax(image))`:\n",
      "4.94065645841e-324 dimensionless\n",
      "yeah, so that argues for choosing an arbitrary but still small cutoff\n",
      "10^-6 ?\n",
      "i might even go with 0.1 but that might be controversial :slightly_smiling_face:\n",
      "i guess another way to deal with this is if either the min or max is zero, go with linear scaling\n",
      "zero or negative\n",
      "that would unconditionally avoid the error you’re hitting and we wouldn’t need to make arbitrary guesses about people’s data\n",
      "but doesn't a symlog scaling still possible make sense in that case?\n",
      "i think we only go with symlog if there are negative values?\n",
      "so i guess if the min is zero but the max is greater than zero, use log scaling\n",
      "all other cases where the min or max is &lt;= 0, use symlog scaling\n",
      "sorry\n",
      "the first one should use linear scaling\n",
      "symlog has an extra parameter that you need to care about so we avoid it unless we need to use it\n",
      "(linthresh)\n",
      "that fixes my issue, but what if the min is 0.0 and the max is 10^8? isn't that an appropriate thing to log-scale, after masking out the resulting inf pixels?\n",
      "i thought yt relied on that stuff a lot\n",
      "like in phaseplots where some of the cells have 0 stuff in them\n",
      "ah yeah that’s true\n",
      "i guess we could check if the max is greater than 1 and use a one-sided symlog scaling with the linthresh set to 1?\n",
      "and if it’s less than one use linear scaling\n",
      "or just use symlog scaling with the threshold always set to one, maybe that works in all cases\n",
      "<@u5t7v4urg> what version are you on right now? a stable release or the dev version?\n",
      "and do you use conda?\n",
      "<@u44sw7j11> has joined the channel\n",
      "<@u042fh0rb> sorry that i was away for a while. i do use condo and i'm on a dev version\n",
      "what version do you want to go back to?\n",
      "the one in use about a year ago, i don't remember the number though\n",
      "ok i guess the easiest thing to do is put you back on the last stable release\n",
      "which came out last august iirc\n",
      "```\n",
      "pip uninstall yt\n",
      "conda install yt\n",
      "```\n",
      "hmmm, any earlier version? like the one on june or july last year\n",
      "<@u5t7v4urg> you can do `conda install yt=3.4.0` for example to install any version\n",
      "<@u5t7v4urg> i think we can get you up and running, though, with the latest version.  i know you're on campus most of the time -- if you wanted to come by my office at ncsa this afternoon we could go through the issues you're having.\n",
      "thank you!\n",
      "<@u8jq8nwq7> has joined the channel\n",
      "<@ua068e10d> has joined the channel\n",
      "hi, any idea of why the `self.total_particles` is in `float` instead of `int64`? i am loading a very large (&gt;2^32) gizmo snapshot. the value of the  `self.total_particles` is correct.\n",
      "it's a python float i think\n",
      "which are 64 bits\n",
      "can you give more detail about what is going wrong exactly?\n",
      "it should probably be an int though, not sure why offhand it's coming out as a float\n",
      "i don't know how to track why the `self.total_particles` is float instead of int64\n",
      "right, i'm not asking you to do that\n",
      "i'm just saying i don't know why it's coming out that way offhand\n",
      "i think it is related to the large number of particles in the simulation. the same gizmo format with smaller particle number is totally fine...\n",
      "i'd probably need to mess with the raw output file\n",
      "what yt version is this?\n",
      "a quick hack might be to just say `morton = np.empty(int(self.total_particles), dtype=\"uint64\")`\n",
      "that'll paper over the error\n",
      "i think you're on yt 3.x\n",
      "in which case switching to the yt-4.0 branch might also be a good idea\n",
      "particularly for a very big particle dataset\n",
      "yes, i can do that. but that should be something roots in the loading part, which may cause other issues. version is '3.5.1'\n",
      "so in yt 4.0 we completely redid the particle indexing system\n",
      "specifically to better support very large datasets\n",
      "which is why i think it might be impactful here\n",
      "all right, i will try that.\n",
      "it looks like you're using python2.7 though, you'll need to use python3, we dropped support for python2.7 in the yt-4.0 branch\n",
      "anyway, good luck!\n",
      "<@udjsrnamq> has joined the channel\n",
      "<@ua9stnc76> has joined the channel\n",
      "hello, i hope this is the appropriate place. is it possible to set the background of a 3d volume rendering scene? i am trying to use the background command in saving an image like ```im = sc.render()\n",
      "im.write_png('blue_bg.png', background=[0,0,1,1], sigma_clip=3)``` yet the png still has a black background. is this the proper technique? also, in the cookbook here (<http://yt-project.org/docs/dev/cookbook/simple_plots.html#image-background-colors>) it doesn’t appear to have any effect either.\n",
      "yeah there is a bug tracking that\n",
      "it used to work in the old volume rendering infrastructure and unfortunately it wasn’t noticed when we made a big change\n",
      "i’m not sure what the ultimate fix is\n",
      "<https://github.com/yt-project/yt/issues/1579>\n",
      "hmm okay. thanks for the quick answer! it’s unfortunate that it is broken and i hope it gets fixed. i tried the modifications made in one of the posts on that thread, but it didn’t seem to fix it for me, even with grey opacity. did that require a different branch of code?\n",
      "<@ucu9e8btm> no, as far as i understood this functionality could be used to spread your grid over several processors (for some operations). so if you have a large grid that could be useful. however, the numprocs there should be (substantially) smaller than the number of grid points. if you have several grids you can process them individually which yields (naturally) the largest speedup. see <https://yt-project.org/docs/dev/analyzing/parallel_computation.html> for help on parallelization.\n",
      "something different (a problem i ran into myself). i obtain the error message `unitparseerror: could not find unit symbol 'ptest' in the provided symbols.`  (from line 669 in yt/units/unit_object.pyc) if i do something simple like this example. should i create a bug report on github?\n",
      "ptest isn’t a unit that’s know by yt’s unit system\n",
      "what is ptest?\n",
      "oh wait, sorry\n",
      "i didn’t read closely\n",
      "that should work please file a bug\n",
      "ok will do\n",
      "another question i have is how yt handles athena input when the ghost cells are also outputted. i noticed that `ds.domain_dimensions` is changed accordingly but `len(ds.all_data()['density'])` is greater than `np.product(ds.domain_dimension)` this leads to weird artefacts such as shown on the file attached.\n",
      "ok i filed #2032\n",
      "seems buggy\n",
      "i bet no one has tried to load data with ghost zones output into yt before\n",
      "is there some way we can tell by looking at the output that this is the case?\n",
      "my memory is that the athena vtk format isn’t especially self-describing\n",
      "yeah, i thought so… so i tried to fix it for a while but couldn’t figure out the structure…also because the value of `len(ds.all_data()['density'])` is very strange, i.e., doesn’t fit any expectations i had. so not sure what’s going on there…\n",
      "<@u042j7xjp> ^\n",
      "<@u042j7xjp> has joined the channel\n",
      "yeah, our problem here is that we don’t yet support this kind of data. but if there is a way from the file to tell that the ghost cells are there we can ignore them\n",
      "do you have a file i can look at for this, <@u8fuk8kcl>? is the one you give me already like that?\n",
      "no, it’s not that one. sure i can upload another one with added boundary conditions. should i file a bug report? [note that apart from the visual artefacts things seem to run fine so it is not that important]\n",
      "well if there are visual artifacts i wouldn’t trust anything else :)\n",
      "you should definitely not trust anything about this data at this point \n",
      "oh ok… :smile: thanks!\n",
      "but please do file a bug report \n",
      "will do. thanks for your help\n",
      "ok i filed #2033. thanks for your help with these two issues.\n",
      "awesome, will try to look at the first one today, i think we should get that one fixed for yt 3.5\n",
      "is there a way to calculate the angular momentum for some particular cells that satisfy some conditions? as an example, i want to calculate the angular momentum for all the dense gas within a sphere, but this way doesn’t work … thanks!\n",
      "\n",
      "sp = ds.sphere(center, (10, ‘kpc’))\n",
      "dense_sp = sp.cut_region([‘obj[“h_p0_number_density”]&gt;= 1e-2’])\n",
      "dense_sp.quantities.angular_momentum_vector()\n",
      "and another question. i’m trying to understand how yt calculates the .quantities.angular_momentum_vector(). i thought it’s a combination of the (x, y, z) specific angular momentum of all the particles, but clearly it’s not … here is how i did it. anything wrong here?\n",
      "\n",
      "what do you mean by it doesn't work? do you mean the results are not coherent with what you expect?\n",
      "also be aware that the `angular_momentum_vector` accepts some arguments to control what is actually computed (am of the particles, am of the gas) if that makes sense for your dataset.\n",
      "for my first question, i can’t do dense_sp.quantities.angular_momentum_vector(), it gives me an error like this one.\n",
      "---------------------------------------------------------------------------\n",
      "indexerror                                traceback (most recent call last)\n",
      "&lt;ipython-input-5-2000e9aaa8dc&gt; in &lt;module&gt;\n",
      "      1 sp = ds.sphere(halo_center, (10, ‘kpc’))\n",
      "      2 dense_sp = sp.cut_region([‘obj[“h_p0_number_density”]&gt;= 1e-2’])\n",
      "----&gt; 3 dense_sp.quantities.angular_momentum_vector()\n",
      "\n",
      "~/.local/lib/python3.6/site-packages/yt-3.5.dev0-py3.6-macosx-10.9-x86_64.egg/yt/data_objects/derived_quantities.py in __call__(self, *args, **kwargs)\n",
      "     65         storage = {}\n",
      "     66         for sto, ds in parallel_objects(chunks, -1, storage = storage):\n",
      "---&gt; 67             sto.result = self.process_chunk(ds, *args, **kwargs)\n",
      "     68         # now storage will have everything, and will be done via pickling, so\n",
      "     69         # the units will be preserved.  (credit to nathan for this\n",
      "\n",
      "~/.local/lib/python3.6/site-packages/yt-3.5.dev0-py3.6-macosx-10.9-x86_64.egg/yt/data_objects/derived_quantities.py in process_chunk(self, data, **kwargs)\n",
      "    462             rvals.extend([(data[“all”, “particle_specific_angular_momentum_%s” % axis] *\n",
      "    463                            data[“all”, “particle_mass”]).sum(dtype=np.float64) \\\n",
      "--&gt; 464                           for axis in “xyz”])\n",
      "    465             rvals.append(data[“all”, “particle_mass”].sum(dtype=np.float64))\n",
      "    466         return rvals\n",
      "\n",
      "~/.local/lib/python3.6/site-packages/yt-3.5.dev0-py3.6-macosx-10.9-x86_64.egg/yt/data_objects/derived_quantities.py in &lt;listcomp&gt;(.0)\n",
      "    462             rvals.extend([(data[“all”, “particle_specific_angular_momentum_%s” % axis] *\n",
      "    463                            data[“all”, “particle_mass”]).sum(dtype=np.float64) \\\n",
      "--&gt; 464                           for axis in “xyz”])\n",
      "    465             rvals.append(data[“all”, “particle_mass”].sum(dtype=np.float64))\n",
      "    466         return rvals\n",
      "\n",
      "~/.local/lib/python3.6/site-packages/yt-3.5.dev0-py3.6-macosx-10.9-x86_64.egg/yt/data_objects/data_containers.py in __getitem__(self, key)\n",
      "    280                 return self.field_data[f]\n",
      "    281             else:\n",
      "--&gt; 282                 self.get_data(f)\n",
      "    283         # fi.units is the unit expression string. we depend on the registry\n",
      "    284         # hanging off the dataset to define this unit object.\n",
      "\n",
      "~/.local/lib/python3.6/site-packages/yt-3.5.dev0-py3.6-macosx-10.9-x86_64.egg/yt/data_objects/selection_data_containers.py in get_data(self, fields)\n",
      "    824                 parent = getattr(self, “parent”, self.base_object)\n",
      "    825                 self.field_data[field] = \\\n",
      "--&gt; 826                   parent[field][self._part_ind(field[0])]\n",
      "    827             else:\n",
      "    828                 self.field_data[field] = self.base_object[field][ind]\n",
      "\n",
      "~/.local/lib/python3.6/site-packages/yt-3.5.dev0-py3.6-macosx-10.9-x86_64.egg/yt/units/yt_array.py in __getitem__(self, item)\n",
      "   1034\n",
      "   1035     def __getitem__(self, item):\n",
      "-&gt; 1036         ret = super(ytarray, self).__getitem__(item)\n",
      "   1037         if ret.shape == ():\n",
      "   1038             return ytquantity(ret, self.units, bypass_validation=true)\n",
      "\n",
      "indexerror: boolean index did not match indexed array along dimension 0; dimension is 3 but corresponding boolean dimension is 331\n",
      "for this question, i was expecting my way of calculating the angular momentum (of all particles) shows the same result as the one from .quantities.angular_momentrum_vector(), but it’s not. so i wonder what’s the argument that the angular_momentum_vector takes? could you point me to the source code of this? thanks!\n",
      "btw, i’m not using yt-4.0, maybe that’s the reason?\n",
      "there were a bunch of fixes to the angular momentum calculation done in the last six months that are in yt-4.0 but not the master branch.  but these may only affect particle-based datasets.  it could be beneficial to look these over for both the discussion as well as the location where things are getting calculated, as it may provide some insight for your particular problems, <@udxdkjcdt>:\n",
      "\n",
      "<https://github.com/yt-project/yt/pull/1902>\n",
      "<https://github.com/yt-project/yt/pull/1934>\n",
      "these look great, thanks! <@u042j5bn6>\n",
      ":slightly_smiling_face:\n",
      "i think we made those fixes on master too\n",
      "ah you’re talking about something different, nm\n",
      "this seems like a bug to me, you should be able to use the angular momentum vector derived quantity with a cut region, is there any chance you can file a bug about this?\n",
      "please include a short example script demonstrating the issue, optimally making use of one of the test datasets on <http://yt-project.org/data|yt-project.org/data> if you can trigger it with one of those\n",
      "i guess the point there is that it's still using ad rather than ds and hence, will likely be just as fast as the method that i copied above. thanks anyway :smile:\n",
      "and yes, i have been running with parallelisation on, following some of the docs on the website.\n",
      "using the tip of yt4, i’m seeing some new errors with basic functionality:\n",
      "\n",
      "```import yt\n",
      "ds = yt.load('fire_m12i_ref11/snapshot_600.hdf5')\n",
      "_, c = ds.find_max('density')```\n",
      "this script fails for both a new os x install as well as a windows install for entirely different reasons. :disappointed:\n",
      "ughhhhhh that’s not great.\n",
      "```loading particle index: 100%|████████████████████████████████████████████████████████| 10/10 [00:00&lt;00:00, 1587.25it/s]\n",
      "traceback (most recent call last):\n",
      "  file \"test.py\", line 3, in &lt;module&gt;\n",
      "    _, c = ds.find_max('density')\n",
      "  file \"/users/chummels/src/yt-conda/src/yt-git/yt/data_objects/static_output.py\", line 854, in find_max\n",
      "    source.quantities.max_location(field)\n",
      "  file \"/users/chummels/src/yt-conda/src/yt-git/yt/data_objects/derived_quantities.py\", line 622, in __call__\n",
      "    rv = super(maxlocation, self).__call__(field, sample_fields)\n",
      "  file \"/users/chummels/src/yt-conda/src/yt-git/yt/data_objects/derived_quantities.py\", line 578, in __call__\n",
      "    rv = super(sampleatmaxfieldvalues, self).__call__(field, sample_fields)\n",
      "  file \"/users/chummels/src/yt-conda/src/yt-git/yt/data_objects/derived_quantities.py\", line 65, in __call__\n",
      "    values = [self.data_source.ds.arr(values[i]) for i in range(self.num_vals)]\n",
      "  file \"/users/chummels/src/yt-conda/src/yt-git/yt/data_objects/derived_quantities.py\", line 65, in &lt;listcomp&gt;\n",
      "    values = [self.data_source.ds.arr(values[i]) for i in range(self.num_vals)]\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/site-packages/unyt/array.py\", line 528, in __new__\n",
      "    return _coerce_iterable_units(input_array, registry)\n",
      "  file \"/users/chummels/src/yt-conda/lib/python3.6/site-packages/unyt/array.py\", line 223, in _coerce_iterable_units\n",
      "    raise iterableunitcoercionerror(input_object)\n",
      "unyt.exceptions.iterableunitcoercionerror: received a list or tuple of quantities with nonuniform units: [unyt_quantity(4.36017291e-22, 'g/cm**3'), unyt_quantity(0.58903652, 'code_mass/code_length**3'), unyt_quantity(0.40852755, 'code_mas\n",
      "s/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quant\n",
      "ity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3')]```\n",
      "maybe i need to update to a newer unyt?\n",
      "what version are you using?\n",
      "2.7.1\n",
      "looks like it’s up to date.\n",
      "<@u44sw7j11> can you run the script above and see if it bails for you on yt4 tip?\n",
      "yep i’ll try\n",
      "presumably this error would be caught by the unit tests, so maybe it isn’t a global problem?\n",
      "we’re testing with 2.7.1 on travis for osx so you’re using the same version there\n",
      "ok, good.\n",
      "(i just went to check to make sure)\n",
      "oh, interesting.  it seems to work ok if you give it the full field tuple: `('gas', 'density')` but fails for just `'density'`\n",
      "at least on the os x box.\n",
      "ok that’s better then!\n",
      "but on the windows machine, i get a totally different error, even when i use the full field tuple.\n",
      "we should build in a readable error though if ‘density’ is passed\n",
      "or an ambiguous field\n",
      "i guess i can work on this issue: <https://github.com/yt-project/yt/issues/1467>\n",
      "but it seems like maybe the changes necessary to fix it will be in all sorts of places in the code.\n",
      "it doesn’t all have to be in one pr\n",
      "i wonder if we could have a validator function that checks the field is valid that’s passed to the various operations that we use in yt\n",
      "it’d make it so all the logic was in one place at least\n",
      "hmmm\n",
      "i haven’t looked into this issue at all yet so i’m not sure how big of an effort that’d be\n",
      "(sorry it’s taking me a bit on my machine. i’ve been downloading the ds you used in your example)\n",
      "i can use ‘density’ on isolatedgalaxy though\n",
      "(but i’m guessing this ds has particles or something?)\n",
      "yeah, that’s odd.  both of those datasets lack any native fields with `'density'` so there’s no interference there.\n",
      "oh weird ok so i loaded that fire dataset and i am getting other weird errors now\n",
      "but they have native `'density'` fields\n",
      "are you getting errors with `valueerror` buffer size?\n",
      "because those are the errors i’m getting in windows and another user reported in os x\n",
      "i’m going to try to reinstall 4.0 and see that this isn’t a local error before i say anything\n",
      "ok.\n",
      "oh, so if i do `_, c = ds2.find_max('density')` it works\n",
      "but `'density'` is a native field, not a derived field.\n",
      "right right\n",
      "intriguing, though.\n",
      "i haven’t followed enough of what unyt does these days to know what’s happening in there.\n",
      "yeah so my traceback with ‘density’ looks similar\n",
      "```---------------------------------------------------------------------------\n",
      "iterableunitcoercionerror                 traceback (most recent call last)\n",
      "&lt;ipython-input-4-6a57cf8f4536&gt; in &lt;module&gt;\n",
      "----&gt; 1 _, c = ds2.find_max('density')\n",
      "\n",
      "~/repos/yt/yt/data_objects/static_output.py in find_max(self, field)\n",
      "    852         source = self.all_data()\n",
      "    853         max_val, mx, my, mz = \\\n",
      "--&gt; 854             source.quantities.max_location(field)\n",
      "    855         # this is a hack to fix the fact that some non-cartesian datasets have\n",
      "    856         # dimensionless quantities, and we can't yet handle that.\n",
      "\n",
      "~/repos/yt/yt/data_objects/derived_quantities.py in __call__(self, field)\n",
      "    620         self.data_source.index\n",
      "    621         sample_fields = get_position_fields(field, self.data_source)\n",
      "--&gt; 622         rv = super(maxlocation, self).__call__(field, sample_fields)\n",
      "    623         if len(rv) == 1:\n",
      "    624             rv = rv[0]\n",
      "\n",
      "~/repos/yt/yt/data_objects/derived_quantities.py in __call__(self, field, sample_fields)\n",
      "    576\n",
      "    577     def __call__(self, field, sample_fields):\n",
      "--&gt; 578         rv = super(sampleatmaxfieldvalues, self).__call__(field, sample_fields)\n",
      "    579         if len(rv) == 1: rv = rv[0]\n",
      "    580         return rv\n",
      "\n",
      "~/repos/yt/yt/data_objects/derived_quantities.py in __call__(self, *args, **kwargs)\n",
      "     63                 values[i].append(storage[key][i])\n",
      "     64         # these will be ytarrays\n",
      "---&gt; 65         values = [self.data_source.ds.arr(values[i]) for i in range(self.num_vals)]\n",
      "     66         values = self.reduce_intermediate(values)\n",
      "     67         return values\n",
      "\n",
      "~/repos/yt/yt/data_objects/derived_quantities.py in &lt;listcomp&gt;(.0)\n",
      "     63                 values[i].append(storage[key][i])\n",
      "     64         # these will be ytarrays\n",
      "---&gt; 65         values = [self.data_source.ds.arr(values[i]) for i in range(self.num_vals)]\n",
      "     66         values = self.reduce_intermediate(values)\n",
      "     67         return values\n",
      "\n",
      "~/python/anaconda/envs/ytdev37/lib/python3.7/site-packages/unyt/array.py in __new__(cls, input_array, units, registry, dtype, bypass_validation, input_units, name)\n",
      "    526         elif _iterable(input_array) and input_array:\n",
      "    527             if isinstance(input_array[0], unyt_array):\n",
      "--&gt; 528                 return _coerce_iterable_units(input_array, registry)\n",
      "    529\n",
      "    530         # input array is an already formed ndarray instance\n",
      "\n",
      "~/python/anaconda/envs/ytdev37/lib/python3.7/site-packages/unyt/array.py in _coerce_iterable_units(input_object, registry)\n",
      "    221             ff = getattr(input_object[0], \"units\", null_unit)\n",
      "    222             if any([ff != getattr(_, \"units\", null_unit) for _ in input_object]):\n",
      "--&gt; 223                 raise iterableunitcoercionerror(input_object)\n",
      "    224             # this will create a copy of the data in the iterable.\n",
      "    225             return unyt_array(np.array(input_object), ff, registry=registry)\n",
      "\n",
      "iterableunitcoercionerror: received a list or tuple of quantities with nonuniform units: [unyt_quantity(4.36017291e-22, 'g/cm**3'), unyt_quantity(0.58903652, 'code_mass/code_length**3'), unyt_quantity(0.40852755, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3'), unyt_quantity(-1.e+90, 'code_mass/code_length**3')]```\n",
      "i can’t find any other fields that generate this problem than density.\n",
      "yeahhhh hmmm\n",
      "ok, well i’m stopping for now, but i’m happy to help figure this out tomorrow\n",
      "ok.\n",
      "i think this might be a separate issue outside of the ambiguous field thing\n",
      "yeah.\n",
      "it might be worth checking to see if another frontend has the same issue\n",
      "this is definitely a new error, as i’ve run this code under yt4 without issue.  this code is actually in the trident tests based on yt4.\n",
      "i’ll do some more digging.\n",
      "it’s interesting that isolatedgalaxy worked, so i wonder if it’s just gizmo or it’s all particle frontends\n",
      "yeah thanks for reporting it.\n",
      "i think we can track it down together\n",
      "(or maybe you’ll figure it out in the remaining hours of your day!)\n",
      "i’ll give it a shot.\n",
      "it looks like for whatever reason, the gizmo dataset is reporting it’s `density` values sometimes in cgs units and sometimes in code units, and unyt is having a problem including numbers with different units in the same array.  but it’s weird because (1) i don’t understand why gizmo would be reporting density values sometimes in code units and sometimes in cgs; and (2) unyt should be smart enough to use its conversion to put the code_length units into cgs in the array.  but i imagine (1) is the source of the problem.\n",
      "so, after some more digging here is what i find:\n",
      "\n",
      "--the problem persists at least as far back as a year ago in the yt4 branch.\n",
      "--the problem occurs for any of the particle-based datasets that i’ve tested, both from gizmo and gadget frontends.\n",
      "--presumably this is due to an ambiguous fieldname access, since gadget and gizmo datasets both have derived fields for `('gas', 'density')` as well as `('parttype0', 'density')` .  my guess is that when the derivedquantity is being called on this data, some of the chunks are being processed on the `('gas', 'density')` field, which returns in cgs and some of the chunks are being called on the `('parttype0', 'density')` field which returns in code_units.  then `ds.arr` doesn’t know how to combine the quantities with different units.  presumably this will occur for all derived quantities.\n",
      "--notably, this problem goes away if you first calculate a derivedquantity on *one* of the gas density fields using the full tuple first: e.g.,\n",
      "```_, c = ds.find_max(('gas', 'density'))\n",
      "_, c = ds.find_max('density')```\n",
      "presumably because it already has `('gas', 'density')` in its cache from the first function call before trying to do the second function call.\n",
      "\n",
      "so in the end, it looks like the problem could be resolved with code to address this issue: <https://github.com/yt-project/yt/issues/1467> .  i’m happy to work on this.  i guess maybe whenever a field access is called, it should check the derived_field_list if there are any ambiguities and error out if there are?  this would be done in the `fieldinfocontainer`?\n",
      "the data we have suppose the enzo_cosmology_plus.tar.gz, i wish to create a field and calculate the value(faraday rotation measure) it over a length in the dataset, i have a term of length in the formula which has units of kpc. so how do calculate over some length i.e which field should i use in my formula for specifying the length parameter?\n",
      "you can also do `region.argmax(&lt;field&gt;)` to get the position or other field values at the max\n",
      "as a note, `min` and `max` do both in a single pass and cache the results, so if you do one, doing the other should be fast.\n",
      "indeed ! additionally, i note that `argmin` and `argmax` generate heterogeneous-unit coordinates in non-cartesian ! as we discussed in many occasions, this is the correct, though not widespread behaviour for non angular coordinates :slightly_smiling_face:\n",
      "thanks every one\n",
      "hi <@u012n308pch>, i think it would help if you posted the code for the definition of your luminosity field and for how you are creating the sphere. could you do that?\n",
      "hi britton, this is my code:\n",
      "import yt\n",
      "import math\n",
      "import numpy as np\n",
      "\n",
      "from yt import derived_field\n",
      "fn = \"/tigress/cen/chris_choo/redshift0045\"\n",
      "ds = yt.load(fn, file_style=\"%s.grid.cpu%%04i\", unit_system=\"cgs\")\n",
      "filename = \"/tigress/cen/chris_choo/c31.z3000.finegalaxiesstsa\"\n",
      "f = open(filename, \"r\")\n",
      "from yt.units import gram, centimeter, erg, s, kg, k, g\n",
      "\n",
      "#defining constants\n",
      "clight=yt.physical_constants.clight\n",
      "planck=yt.physical_constants.hcgs\n",
      "b_constant=yt.physical_constants.kboltz\n",
      "proton_mass=yt.physical_<http://constants.mp|constants.mp>\n",
      "#defining additional necessary parameters\n",
      "e_transition = 2.17896*10**-11.*erg\n",
      "phi=1.*s\n",
      "e_mass = 9.109*10**-28.*g\n",
      "n_one=31\n",
      "n_two = 30\n",
      "delta_n = 1\n",
      "z = 1\n",
      "m_delta_n = 0.190775\n",
      "\n",
      "#defining necessary formulas\n",
      "frequency = e_transition*(1/(n_two**2)-1/(n_one**2))/planck\n",
      "f_n1_n2=n_one*m_delta_n*(1+1.5*delta_n/n_one)\n",
      "\n",
      "print(\"clight = \", clight)\n",
      "print(\"planck = \", planck)\n",
      "print(\"b_constant = \", b_constant)\n",
      "print(\"proton_mass = \", proton_mass)\n",
      "print(\"frequency = \", frequency)\n",
      "\n",
      "def _emissivity(field, data):\n",
      "    return 3.469*10**-12*phi*z*(f_n1_n2/n_one)*(1-3*delta_n/(2*n_one))*data[\"h_number_density\"].in_units(\"1/cm**3\")*(data[\"electron_density\"].in_units(\"g*1/cm**3\")/e_mass)/(data[\"temperature\"].in_units(\"k\")**(5/2))*np.exp(-1.579*10**5/(n_one**2)/data[\"temperature\"].in_units(\"k\"))*2*b_constant*data[\"temperature\"].in_units(\"k\")*(frequency**2)/(clight**2)\n",
      "yt.add_field(\"emissivity\", sampling_type = \"cell\", function =_emissivity, units = \"erg*s/(k**(5/2)*cm**8)\")\n",
      "\n",
      "def _luminosity_field(field, data):\n",
      "    return data[\"emissivity\"].in_units(\"erg*s/(k**(5/2)*cm**8)\")*data[\"cell_volume\"].in_units(\"cm**3\")\n",
      "yt.add_field(\"luminosity_field\", sampling_type = \"cell\", function = _luminosity_field, units = \"erg*s/(k**(5/2)*cm**5)\")\n",
      "the part with the sphere:\n",
      "x = []; y = []; z = [];\n",
      "for line in f:\n",
      "    words = line.split()\n",
      "    x.append(float(words[13]))\n",
      "    y.append(float(words[14]))\n",
      "    z.append(float(words[15]))\n",
      "    inc+=1\n",
      "for some reason, if i try running:\n",
      "sp_2 = ds.sphere([0.4326766, 0.5219305, 0.3977057], (50, 'kpc'))\n",
      "luminosity_sum_2 = sp_2.sum(\"luminosity_field\")\n",
      "before defining the sphere, the code runs. however, if i try running it after defining the sphere, i see the error that i posted\n",
      "^ i meant before/after getting the values for x[], y[] z[]\n",
      "this doesn't actually seem to be explicitly related to hard-coding, more something about how getting the values for x[] y[] z[] interferes with my ability to sum luminosity for a sphere\n",
      "thank you!\n",
      "hi shai, i haven’t tried to run this yet, but i noticed that your code has a line `z = 1`, so you are clobbering the list of z positions for the spheres, or the other way around. one of those two variable names needs to be changed. i suspect this is the issue.\n",
      "you're absolutely right\n",
      "thank you!!!\n",
      "excellent, glad i could help.\n",
      "thanks <@u042fh0rb> the demeshened yt-4.0 version works beautifully with ('gas', 'density') and temperature. does it also work for doing a projectionplot of density of stars (parttype4) and high-res dm (parttype1) for the example fire sim used in that notebook?\n",
      "<@uatlrva65> has joined the channel\n",
      "<@uau5l1r6g> has joined the channel\n",
      "hey! i think i've already asked this question in the past, but is there an efficient way in yt to get the value of a given field at a list of position?\n",
      "say i have an array of shape `(npt, ndim)`, is there a way to query e.g. the pressure at each location?\n",
      "i think `ds.find_field_values_at_points` is what you’re looking for.\n",
      "<@u01046wns02> has joined the channel\n",
      "hi ! i tried installing yt-4.0 in a separate conda environement but neither `conda develop -b .` nor `pip install -e .` worked.\n",
      "the first fails with\n",
      "```modulenotfounderror: no module named 'cython'```\n",
      "though i did install it, while `pip install` just runs forever, printing nothing.\n",
      "any clue ?\n",
      "i'd strace the latter to see what it's doing\n",
      "and i avoid conda so i won't help much with the former :slightly_smiling_face:\n",
      "strace ?\n",
      "<https://github.com/strace/strace> ?\n",
      "i’m on mac, i don’t think i can use this :confused: (tried getting it with homebrew, to no avail)\n",
      "i use conda on a mac\n",
      "is your cython in your base env or the new env you created?\n",
      "i installed it on the target env\n",
      "hmmm\n",
      "if you switch to master does it build with `pip install -e . ` is this just an issue with 4.0?\n",
      "well that’s what i did in my usual (not base) env and yes it worked\n",
      "i actually did it multiple times on this machine and it always worked with master, but never with yt4.0\n",
      "is it always an issue detecting cython? or do other things happen sometimes?\n",
      "i don’t remember, last time i tried i did not keep trace of what exactly had gone wrong :confused:\n",
      "no worries\n",
      "can you do `conda list`  in the env?\n",
      "```# packages in environment at /users/clm/miniconda3/envs/yt4:\n",
      "#\n",
      "# name                    version                   build  channel\n",
      "appnope                   0.1.0           py38h32f6830_1001    conda-forge\n",
      "backcall                  0.1.0                      py_0    conda-forge\n",
      "bzip2                     1.0.8                h0b31af3_2    conda-forge\n",
      "ca-certificates           2019.11.28           hecc5488_0    conda-forge\n",
      "certifi                   2019.11.28       py38h32f6830_1    conda-forge\n",
      "cftime                    1.1.1            py38h65ad66c_0    conda-forge\n",
      "curl                      7.68.0               h8754def_0    conda-forge\n",
      "cycler                    0.10.0                     py_2    conda-forge\n",
      "cython                    0.29.15          py38hc84c608_1    conda-forge\n",
      "decorator                 4.4.2                      py_0    conda-forge\n",
      "expat                     2.2.9                h4a8c4bd_2    conda-forge\n",
      "freetype                  2.10.1               h8da9a1a_0    conda-forge\n",
      "gettext                   0.19.8.1          h46ab8bc_1002    conda-forge\n",
      "git                       2.25.0          pl526hdc91d69_0    conda-forge\n",
      "hdf4                      4.2.13            h84186c3_1003    conda-forge\n",
      "hdf5                      1.10.5          nompi_h3e39495_1104    conda-forge\n",
      "ipython                   7.13.0           py38h32f6830_2    conda-forge\n",
      "ipython_genutils          0.2.0                      py_1    conda-forge\n",
      "jedi                      0.16.0           py38h32f6830_1    conda-forge\n",
      "jpeg                      9c                h1de35cc_1001    conda-forge\n",
      "kiwisolver                1.1.0            py38ha0d09dd_1    conda-forge\n",
      "krb5                      1.16.4               h1752a42_0    conda-forge\n",
      "libblas                   3.8.0               16_openblas    conda-forge\n",
      "libcblas                  3.8.0               16_openblas    conda-forge\n",
      "libcurl                   7.68.0               h709d2b2_0    conda-forge\n",
      "libcxx                    9.0.1                         1    conda-forge\n",
      "libedit                   3.1.20170329      hcfe32e1_1001    conda-forge\n",
      "libffi                    3.2.1             h4a8c4bd_1007    conda-forge\n",
      "libgfortran               4.0.0                         2    conda-forge\n",
      "libiconv                  1.15              h0b31af3_1006    conda-forge\n",
      "liblapack                 3.8.0               16_openblas    conda-forge\n",
      "libnetcdf                 4.7.3           nompi_hda4e5f1_101    conda-forge\n",
      "libopenblas               0.3.9                h3d69b6c_0    conda-forge\n",
      "libpng                    1.6.37               hbbe82c9_1    conda-forge\n",
      "libssh2                   1.8.2                hcdc9a53_2    conda-forge\n",
      "llvm-openmp               9.0.1                h28b9765_2    conda-forge\n",
      "matplotlib                3.2.1                         0    conda-forge\n",
      "matplotlib-base           3.2.1            py38h1300a51_0    conda-forge\n",
      "mpi                       1.0                     openmpi    conda-forge\n",
      "mpi4py                    3.0.3            py38h5dec745_1    conda-forge\n",
      "mpmath                    1.1.0                      py_0    conda-forge\n",
      "ncurses                   6.1               h0a44026_1002    conda-forge\n",
      "netcdf4                   1.5.3           nompi_py38hcf2264a_102    conda-forge\n",
      "numpy                     1.18.1           py38hde6bac1_0    conda-forge\n",
      "openmpi                   4.0.2                hed52333_4    conda-forge\n",
      "openssl                   1.1.1e               h0b31af3_0    conda-forge\n",
      "parso                     0.6.2                      py_0    conda-forge\n",
      "pcre                      8.44                 h4a8c4bd_0    conda-forge\n",
      "perl                      5.26.2            haec8ef5_1006    conda-forge\n",
      "pexpect                   4.8.0            py38h32f6830_1    conda-forge\n",
      "pickleshare               0.7.5           py38h32f6830_1001    conda-forge\n",
      "pip                       20.0.2                     py_2    conda-forge\n",
      "prompt-toolkit            3.0.4                      py_0    conda-forge\n",
      "ptyprocess                0.6.0                   py_1001    conda-forge\n",
      "pygments                  2.6.1                      py_0    conda-forge\n",
      "pyparsing                 2.4.6                      py_0    conda-forge\n",
      "python                    3.8.2           hdc38147_4_cpython    conda-forge\n",
      "python-dateutil           2.8.1                      py_0    conda-forge\n",
      "python_abi                3.8                      1_cp38    conda-forge\n",
      "readline                  8.0                  hcfe32e1_0    conda-forge\n",
      "scipy                     1.3.3            py38h82752d6_0    conda-forge\n",
      "setuptools                46.0.0           py38h32f6830_2    conda-forge\n",
      "six                       1.14.0                     py_1    conda-forge\n",
      "sqlite                    3.30.1               h93121df_0    conda-forge\n",
      "sympy                     1.5.1            py38h32f6830_2    conda-forge\n",
      "tk                        8.6.10               hbbe82c9_0    conda-forge\n",
      "tornado                   6.0.4            py38h64e0658_1    conda-forge\n",
      "traitlets                 4.3.3            py38h32f6830_1    conda-forge\n",
      "wcwidth                   0.1.8                      py_0    conda-forge\n",
      "wheel                     0.34.2                     py_1    conda-forge\n",
      "xz                        5.2.4             h0b31af3_1002    conda-forge\n",
      "zlib                      1.2.11            h0b31af3_1006    conda-forge```\n",
      "ok i’m trying out something on my machine\n",
      "i also tested <@ud9l1d44t> on my machine (mac) and yt-4.0 pip install -e is also running forever.\n",
      "<@ud9l1d44t>, sorry i lied. it works for me :disappointed:.\n",
      "damn it\n",
      "i mean... “good for you” :sweat_smile:\n",
      "i don’t need it, just was testing to see if it was also the case for my conda to better pinpoint the problem :slightly_smiling_face:.\n",
      "and thank you for that !\n",
      "hope you find the problem soon :slightly_smiling_face:.\n",
      "<@usj3smag2> what version of python are you using in your build?\n",
      "ok i just made a fresh py38 env and tried to build 4.0 and had an issue too\n",
      "or, i think i did\n",
      "i walked away from my computer and it hung\n",
      "i didn’t have any issue picking up cython though\n",
      "anyways, i will do my best to figure this out later in the week, but for now i would try building in a different python version. i didn’t have issues building with 3.6 for the yt-4.0 branch\n",
      "<@uaurgnee6> has joined the channel\n",
      "i’m trying to generate some multi-panel images for a fire paper using this cookbook recipe as a template: <http://yt-project.org/docs/dev/cookbook/complex_plots.html#multipanel-with-phaseplot> .  these are great templates on how to use the mpl `axesgrid` object, but i had a quick question i couldn’t seem to figure out.  i need to put the colorbar on the *top* of the image, not on the side, since i’m putting several panes next to each other.  but if i use the method described in these cookbooks, which just grabs the yt-generated colorbar and sticks it in the `axesgrid` colorbar position, it looks all smooshed because yt generates one for a vertical placement, and mpl wants one for horizontal.  is there any easy way around this? example image:\n",
      "\n",
      "<https://i.imgur.com/nlrnafi.png>\n",
      "no idea\n",
      "it might be easier to do it “fully manually” using the frb interface\n",
      "yt plots in general don’t know how to draw themselves with horizontal colorbars\n",
      "so you’d be fighting against that\n",
      "yeah, that’s what i thought.  i dug into the code a bit, but seemed problematic.\n",
      "is there a good example of doing it fully manually using the frb interface?  presumably, the frb interface won’t account for the annotations on the plot window, right?\n",
      "yeah, you’d need to draw those manually too :confused:\n",
      "gotcha.\n",
      "well, the plot axes themselves look good.  i wonder if there is a way to use them as-is, and then just create the colorbar manually by doing an `imshow(frb)` or something?\n",
      "even if i don’t use the imshow’d frb\n",
      "<http://yt-project.org/doc/cookbook/complex_plots.html#advanced-multi-plot-multi-panel>\n",
      "but that doesn’t use axesgrid\n",
      "it would be similar though\n",
      "maybe\n",
      "i guess you’d need to draw the colorbar manually?\n",
      "and not rely on yt’s plotting code to do it\n",
      "right--i guess i’m not sure how to do that.\n",
      "i don’t know either\n",
      "it’s hard for me to say without looking at your code\n",
      "yeah, that’s fair.\n",
      "<@ufmrug2ka> has joined the channel\n",
      "yes, take a look at this pull request which added that capability: <https://github.com/yt-project/yt/pull/2186>\n",
      "<@up180lpev> has joined the channel\n",
      "<@u7puv7h5j> has joined the channel\n",
      "hi all, a dumb question - when creating an index from an sph dataset, does this ignore the bounding box and create the index for all particles?\n",
      "\n",
      "```\n",
      "x = 10\n",
      "ds = yt.load(/some/dummy/data.h5', bounding_box=[[-x,x],[-x,x],[-x,x]])\n",
      "ds.index  # does this ignore bounding_box?\n",
      "```\n",
      "ok…. ignorant question time. i’ve got yt installed via conda and i’m struggling to figure out howto  upgrade from `3.4.1` to `3.5` … ```$ conda update yt\n",
      "collecting package metadata (repodata.json): done\n",
      "solving environment: done\n",
      "\n",
      "# all requested packages already installed.\n",
      "\n",
      "```\n",
      "thinks for a _long_ time before doing this, and still the 3.4.1. :shrug:\n",
      "is there a way to specify 3.5.1 explicitly? (<https://yt-project.org/doc/installing.html> is unclear on what i should be doing here and `conda search  yt --info` only has stuff about 3.4.1)\n",
      "<@u4esetp43> i’m not exactly sure. if i do `conda search yt` i can see 3.5.1 appearing in my search. what channels are you looking in? and what version of python are you using?\n",
      "i’ve got python 3.5. i just explicitly did `yt=3.5.1` and it took a while  to think but found this\n",
      "the `- yt=3.5.1 -&gt; python[version='&lt;=3.3']` is surprising to me\n",
      "i don't think there are builds of yt for python 3.5 on conda-forge\n",
      "at least the latest version of yt\n",
      "only python 3.6 and 3.7 at the moment i believe\n",
      "the default channel is maintained by anaconda inc, we have no way to update the yt package in defaults\n",
      "<@u7puv7h5j> what yt version are you using?\n",
      "<@u4esetp43> for your question see above ^\n",
      "ah, no, 2.7, 3.6, and 3.7, you can see all the builds that are available here: <https://anaconda.org/conda-forge/yt/files>\n",
      "so the ` python[version='&lt;=3.3']` is  just misleading?\n",
      "i'm not sure what that string means exactly\n",
      "but there definitely aren't builds of yt 3.5.1 on conda-forge for python3.5 so i'd guess that's the issue\n",
      "heh\n",
      "ok, will try updating python to 3.6 and see what rabbit hole that leads me down … thanks!\n",
      "i'd do 3.7\n",
      "3.8 is about to come out so you'll just have to deal with this again soon\n",
      "updating to 3.7 now delays that pain for a year or so :slightly_smiling_face:\n",
      ":see_no_evil:\n",
      "oh lord\n",
      "so 3.6 looked like it’d work, but i was going to take your advice and  do 3.7\n",
      "```$ conda install python=3.7\n",
      "collecting package metadata (repodata.json): done\n",
      "solving environment: failed with initial frozen solve. retrying with flexible solve.\n",
      "solving environment: \\ \n",
      "found conflicts! looking for incompatible packages.\n",
      "```\n",
      ":partyparrot:\n",
      "conda install -c conda-forge python=3.7\n",
      "basically you always want to use conda-forge\n",
      "except i don’t\n",
      "*shrug* ok\n",
      "i use astroconda for everything for many reasons\n",
      "really defaults doesn't have 3.7 yet?\n",
      "lol\n",
      "¯\\_(ツ)_/¯\n",
      "hopefully the 3.6 will fix the problem\n",
      "(this  is all to get the  latest version of trident working …)\n",
      "thanks <@u042fh0rb>! i saw that the builds were up for 3.6 and 3.7 but not 3.5. :slightly_smiling_face:\n",
      "conda-forge in general only supports builds for the two most recent python3 releases\n",
      "oh interesting. i didn’t realize that.\n",
      "yeah, otherwise builds would take too long, they're using free ci resources to do almost all of the builds\n",
      "ahhhhh yeah, that makes sense. still, thanks for that. i’m sure it’ll be helpful to know when 3.8 comes out. :slightly_smiling_face:\n",
      "hi nathan, it's 3.5.1\n",
      "and is this an sph dataset?\n",
      "it's a gizmo output, so for the purpose of yt it's particles yeah (i think?)\n",
      "yes, it is\n",
      "i'd suggest running the in-development yt-4.0 branch, see <https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb> for installation instructions\n",
      "i don't know offhand why it's using all of the particles to do the indexing, i don't think it should, but also the yt-3 sph support is in maintainance mode at the moment as ongoing development goes towards releasing yt-4.0\n",
      "lots of things are faster in yt-4, you might find you don't need to use the bbox keyword at all anymore\n",
      "alright, i assume this has enhancements for building the grid on subsets?\n",
      "ah, gotcha\n",
      "there's no mesh in yt-4 for sph data\n",
      "we index particles using a ewah-compressed bitmaps instead of using an octree\n",
      "this saves a ton of ram and has the side benefit that many operations will be much faster since we just process the raw particle data instead of smoothing data onto the octree\n",
      "that's fair, i'll go and check it out. cheers for the help :duck:\n",
      "yeah i was hitting ram issues loading a big cosmo box\n",
      "i only needed a small subset of particles around a given galaxy, but bbox didn't seem to limit the particles used for indexing\n",
      "if it's a zoom-in, passing `index_ptype='parttype0'` will often be a lot faster\n",
      "that will use the sph particles to build the index instead of all the particles\n",
      "(for yt-3.x, again, not really an issue in yt-4)\n",
      "this particular one wasn't a zoom, just a single galaxy selected from a periodic sim. so the majority of gas particles (billions!) were outside the required region\n",
      "<@ujt0fc0mr> has joined the channel\n",
      "hello, running into installation issues:\n",
      "i have been attempting to install on a machine running windows with linux/unix enabled, and have also attempted to do so on a virtual machine running ubuntu, but both fail at this same point.\n",
      "<@ujt0fc0mr> could you paste the command that you've used to install?\n",
      "<@u042f73r7>\n",
      "bash install_script.sh\n",
      "using wget <https://raw.githubusercontent.com/yt-project/yt/master/doc/install_script.sh> as the script with everything set to 1\n",
      "do you need rockstar?\n",
      "and embree?\n",
      "rockstar yes embree probably not, but it fails w/o embree anyway\n",
      "<@uk6r99v7y> has joined the channel\n",
      "hi. trying to get the intro import data tutorial working. <http://ytini.com/tutorials/tutorial_pythonsop.html>\n",
      "\n",
      "yt seems to be installed correctly. but get the following error. is it possible i'm missing a package or?\n",
      "yt : [error    ] 2019-05-30 13:01:52,957 couldn't figure out output type for /foo/data/isolatedgalaxy.tar.gz\n",
      ".hip and custom python asset attached\n",
      "\n",
      "apologies, different error now\n",
      "\n",
      "python error: traceback (most recent call last):\n",
      "file \"\", line 19, in\n",
      "file \"/applications/houdini/houdini17.5.173/frameworks/python.framework/versions/current/lib/python2.7/site-packages/yt/convenience.py\", line 98, in load\n",
      "raise ytoutputnotidentified(args, kwargs)\n",
      "ytoutputnotidentified: supplied ('/volumes/sd/03_projects/2019.05.27_visualizingastronomy/03_production/01_houdini/data/isolatedgalaxy.tar.gz',) {}, but could not load!\n",
      "also, the reference .hip file here <http://ytini.com/downloads/withpythonsop.hipnc>  is supposed to have a custom python sop but the .hip file appears empty. could someone confirm this is the case?\n",
      "<@ujyb2mz5x> has joined the channel\n",
      "<@uk6r99v7y> you need to unpack isolatedgalaxy.tgz first (`tar xf isolatedgalaxy.tgz`). it will create a directory `isolatedgalaxy` and you can load the dataset via `yt.load('isolatedgalaxy/galaxy0030/galaxy0030')`\n",
      "<@uk6r99v7y>  <http://ytini.com/downloads/withpythonsop.hipnc> has content when i download it locally:\n",
      "```$ curl -jo <http://ytini.com/downloads/withpythonsop.hipnc>\n",
      "  % total    % received % xferd  average speed   time    time     time  current\n",
      "                                 dload  upload   total   spent    left  speed\n",
      "100 32062  100 32062    0     0  4472k      0 --:--:-- --:--:-- --:--:-- 6262k\n",
      "$ head withpythonsop.hipnc \n",
      "hounc1033600baa057ec6cd709a7e5953.startfplayback -i on -r off -f 1 -e on -h on -t on -a on -k on -s 1\n",
      "tcur 0\n",
      "fps 24\n",
      "tset 0 10\n",
      "frange 1 240\n",
      "unitlength 1\n",
      "unitmass 1\n",
      "prompt '`strcat(oppwf(), \" -&gt; \")`'\n",
      "hounc1033600ba6057ec6cd7043eeba26.variablesset -g activetake = 'main'\n",
      "set -g e = '2.7182818284590452354'```\n",
      "hey...thanks a million!\n",
      "hey, forgive me if this is the wrong place or way to ask, but i'm having some difficulty with actually saving plots. they're all named slice plots despite being other kinds of plots\n",
      "how do i fix this?\n",
      "changing the contents of .save(name) only causes the beginig of the file name to change, it's still always called slice\n",
      "<@ujyb2mz5x> make sure you're passing a proper file name with suffix e.g. \"my_plot.png\"\n",
      "that did it\n",
      "thanks\n",
      "one question regarding ytini framework. can it be used to load a csv subset of the gaia data source here? <http://cdn.gea.esac.esa.int/gaia/gdr2/gaia_source/csv/>\n",
      "my goal is to create a simulation of our own milkway by using a galaxy that is similar to ours... obviously i don't want to load the entire gaia source. i thought if i knew exactly which csv files contained the andromeda or other isolated galaxy i could load just those...\n",
      "hi - trying to make a profile:\n",
      "```mass_gas_profile = yt.create_profile(sphere, \"radius\", (\"gas\", \"cell_mass\"), n_bins=60, weight_field=none, accumulation=true)```\n",
      "raises this error:\n",
      "```valueerror: bins must be monotonically increasing or decreasing```\n",
      "what to do?\n",
      "im on the latest version 3.6 currently\n",
      "<@uppavuhsm> could you share your entire script? what's the location of sphere, what's the size of the computational domain?\n",
      "```&gt;ds = yt.load(\"/mnt/zfsbergusers/jeg/newhorizon/output_dir/output_00448/info_00448.txt\", extra_particle_fields=[(\"particle_birth_time\", \"d\"), (\"particle_metallicity\", \"d\")])\n",
      "&gt;sphere = ds.sphere(center=[0.450756, 0.521550, 0.539467], radius=(10, \"kpc\"))\n",
      "%this is a sphere located on centre coordinates of a galaxy identified by adaptahop\n",
      "&gt;profile = yt.create_profile(sphere, \"radius\", (\"gas\", \"velocity_cylindrical_theta\"), weight_field=\"cell_mass\", accumulation=false)```\n",
      "this was working totally fine in the previous version\n",
      "also sorry - this new code is when i didn't specify any bins at all and it still raised the same error as\n",
      "```mass_gas_profile = yt.create_profile(sphere, \"radius\", (\"gas\", \"cell_mass\"), n_bins=60, weight_field=none, accumulation=true)```\n",
      "did something change in the way yt creates profiles between versions 5.4.1 and 6?\n",
      "can you do `print(sphere)` after it's created and see if coordinates in the same units still make sense?\n",
      "and also `print(ds.domain_left_edge, ds.domain_right_edge)`\n",
      "needs to run, but will be back later\n",
      "```print(sphere)\n",
      "&gt;ytsphere (info_00448): , center=[9.87174778e+25 1.14221664e+26 1.18145563e+26] cm, radius=3.0856775809623245e+22 cm\n",
      "print(ds.domain_left_edge, ds.domain_right_edge)\n",
      "&gt;[0. 0. 0.] code_length [1. 1. 1.] code_length```\n",
      "<@uppavuhsm> i can't reproduce it on datasets i have. how big is your `info_00448` ? would it be possible to tar it and upload to <https://use.yt/upload/> ?\n",
      "i don’t have permissions to upload it because i’m working on a shared computing cluster \n",
      "it’s a ramses dataset if that helps? \n",
      "how many cells does it actually select in that sphere?\n",
      "<@uppavuhsm> can you show me the output of `print(sphere[\"index\", \"ones\"].sum())` ?\n",
      "hi, i have a question regarding how we can access the data values after we have done some yt plotting functions. for example, i have a variable `a = yt.projectionplot(ds, \"z\", \"density\", method=\"sum\"` which add the density values along z-axis. when i try to print out the summed values in `a`, python returns an image object with the message `&lt;yt.visualization.plot_window.projectionplot at 0x7f40cf738110&gt;`. is there a way to access the changed values/arrays? thanks for the help!!!\n",
      "you should be able to get an nxn array of the image plane from your summed projection if you look at `a.frb['density']`.  frb stands for `fixedresolutionbuffer`.\n",
      "<@uf4561yh4> has joined the channel\n",
      "<@u7ev2lfat> is 3mpc/h a reasonable smoothing length within eagle? i know typically iteratively solves that equation for nearest neighbours and density. how does that map to the hsml used by a cubic spline?\n",
      "3 mpc/h may indeed be a reasonable maximal smoothing length. however note that this is the kernel cut-off for a wendland-c2 kernel, so if you are using the cubic spline for visualisation you will need to multiply all of your smoothing lengths by 1.825742 / 1.936492 (a pretty small change tbh) to have it exactly right.\n",
      "hey, apologies couldn't get back to you yesterday. yes those fields exist in `ds.derived_field_list`  along with fields for other ions in self._ions.\n",
      "i would note that your point about the mean baryon density being off is definitely not the case. there are, in all of these codes, consistency checks to ensure that the mean density is within something like 1e-5 of the 'true' value\n",
      "can i ask what happens if you double / half the grid resolution?\n",
      "i actually don't know if my comment about the smoothing lengths above is correct. from your code that you use to produce the illustris images, it appears that the smoothing length you're giving yt is the fwhm of the kernel, whereas in the eagle situations it is giving you the cut-off radius. perhaps ash can clarify which one of these that yt expects.\n",
      "it's the cut off radius, i believe. the yt deposition follows the eq. 3 of the splash method paper, whereas most codes follow eq. 9. i think our choice of eq. 3 means that the method is very sensitive to the smoothing length and the calculated density being consistent. essentially we demand that m/h^3 == rho. eq. 9 does not require this!\n",
      "<https://www.cambridge.org/core/journals/publications-of-the-astronomical-society-of-australia/article/splash-an-interactive-visualisation-tool-for-smoothed-particle-hydrodynamics-simulations/20c22a93ab1cd3adc1f5b8439b639a24>\n",
      "in the case that m/h^3 differs from rho then we will end up with the complete wrong density. it will look right, but the normalization will be incorrect - which sounds like the problem we are running into here\n",
      "that does explain why it works better when using the mass and densities to estimate a smoothing length\n",
      "ok, i see now. setting `x_nuclei_density` and `x_number_density` was an active decision made back in [this pr](<https://github.com/yt-project/yt/pull/2336>), so `x_number_density` is no longer an alias of `x_p0_number_density`. i was the one who sent this conversation over here from trident, but now i’m pretty sure that yt is doing the right thing. if trident is still assuming that `p0` and `number_density` are the same, then we need to go there.\n",
      "alright thanks !\n",
      "<@u012v1cj6jv> has joined the channel\n",
      "hey! i have a gadget dm only dataset and i was wondering if there was a way to make some volume rendering of the dm density?\n",
      "and if not, could i do some kind of sph-like projection of the data onto a regular mesh?\n",
      "this is a script for a volume rendering for gizmo (so...related) simulations that include baryons that <@u042fh0rb> wrote a while ago.  this is what i modify to make my own vr\n",
      "\n",
      "```(base) [desika.narayanan@login4 yt_tools]$ more vr.py\n",
      "#written by nathan golbaum\n",
      "import yt\n",
      "import numpy as np\n",
      "from yt.units import kpc\n",
      "ds = yt.load('gadgetdiskgalaxy/snapshot_200.hdf5')\n",
      "m, c = ds.find_max(('gas', 'density'))\n",
      "le = c - 25*kpc\n",
      "re = c + 25*kpc\n",
      "ag = ds.arbitrary_grid(le, re, [256]*3)\n",
      "fields = ['density', 'temperature', 'velocity_x', 'velocity_y', 'velocity_z']\n",
      "data = {}\n",
      "for f in fields:\n",
      "    print(f)\n",
      "    data[f] = ag[('gas', f)]\n",
      "    print(data[f].shape)\n",
      "bbox = np.array(list(zip(le, re)))\n",
      "ds2 = yt.load_uniform_grid(data, [256]*3, length_unit=ds.length_<http://unit.to|unit.to>('kpc'),\n",
      "bbox=bbox)\n",
      "sc = yt.create_scene(ds2)\n",
      "sc.camera.zoom(2)\n",
      "sc.save()```\n",
      "thanks!\n",
      "and do you know whether it is possible to trick yt into thinking dm particles are like sph?\n",
      "hmmm...that i don't know\n",
      "didn’t someone work on adding smoothing lengths to particles that don’t have them stored? that’s all you’d need, right?\n",
      "i think so, but it's the first time i work with sph data. i'll try to dig up the pr\n",
      "are you referring to this pr <https://github.com/yt-project/yt/pull/2186>?\n",
      "yes, i think so. it was a fragment of a memory.\n",
      "yeah, that’s the one.\n",
      "unsure if this is a bug or i'm just passing it incorrectly but:\n",
      "`yt.profileplot(ad_hl_r, 'y','density', n_bins=500, x_log=none)`\n",
      "`yt.profileplot(ad_hl_r, 'y','density', n_bins=500, x_log=0)`\n",
      "`yt.profileplot(ad_hl_r, 'y','density', n_bins=500, x_log=false)`\n",
      "all return a plot in which the xaxis is scaled logarithmically.\n",
      "setting `.set_log('y',0)` does correct the issue.\n",
      "looks like a bug to me\n",
      "... although i was not able to reproduce it (on the master branch). i’m getting a linearly spaced xaxis with either  `x_log=0`  or `x_log=false`\n",
      "however it’s true that passing  `x_log=none` and getting a logspaced xaxis feels counter intuitive. i think the api can be improved to solve this pretty easily.\n",
      "<@u015r1bhqgl> i opened a pr to solve this here <https://github.com/yt-project/yt/pull/2650>\n",
      "but i’m still not sure i’m capturing your whole issue.\n",
      "in which case, it's likely related more to the fact my yt repository is the amrvac frontend version rather than the official yt version. i think the amrvac frontend was due to be incorporated in yt 4.0 but maybe i should ask my local dev team about it instead. thanks for the info!\n",
      "note that the amrvac frontend is part of the latest yt release 3.6\n",
      "and indeed it will soon be part of the 4.0 “branch”\n",
      "if you have issues with the frontend though, i’m here to help too. i wrote the frontend with <@ufggcp9r6> and i’m interested in any form of feedback\n",
      "hi all,\n",
      "\n",
      "i'm trying to plot multiple line plots on the same plot but of two different quantities with wildly differing orders of magnitude. currently, i guess yt detects this and plots them on two different axes but, as can be done in matplotlib, is there a way to specify which quantity is plotted on which axis? i.e., same x range, two y axes, one quantity to each?\n",
      "cool! thanks for the clarification. indeed, <@ufggcp9r6> and i have been in near constant communication as i learn both yt and python in general :slightly_smiling_face:\n",
      "is there any way to access cells \"within a certain distance\" of other cells? i have a binary field where gas is in state 0 or 1, and i want to know what fraction of state 0 gas cells are within 2 kpc of a state 1 gas cell but i have no idea how to ask about \"nearby\".\n",
      "\n",
      "almost as good would be cells which are \"adjacent\". is there a way to access a cell's immediate neighbors? (i.e. how many state 0 cells are adjacent to a state 1 cell). i'm using art, but i don't know how to do this in any mesh code, though it seems like it would be possible\n",
      "i think you’d have to write up code to do this explicitly, as i am not aware of this functionality existing already within yt.\n",
      "i could try to do it by using the cells physical distance. is the x,y,z position of a cell available explicitly? though just imagining it having to scan through all the state 1 cells for each state 0 cell might be really slow. but maybe not horrible? idk\n",
      "<@ue8nzm52l> has joined the channel\n",
      "<@uc6l85lbb> i think that's unrelated to the issue you linked to\n",
      "if you're dealing with amrex anyway\n",
      "if you're getting a seg fault trying to volume render amr data then it's a different issue\n",
      "how big is the dataset you're dealing with?\n",
      "any chance you can generate the segfault while running with the python faulthandler?\n",
      "you could also get a c-level traceback with gdb\n",
      "feel free to open an issue on github about your specific problem, if you can somehow create an example one of us can run to trigger the issue that would be a huge help, but i understand how that can sometimes be difficult for real-world issues\n",
      "hmmm okay. the plt folders are fairly large, on the order of 10-100 gb. the thing that seems odd about the issue is that i get the `failed to split grids` but sometimes it will still output an image that feels correct.\n",
      "maybe you can just output the amr hierarchy?\n",
      "e.g. all the grid left and right edges?\n",
      "what do you mean exactly?\n",
      "sometimes these issues are caused by how yt handles the grid geometries, not the underlying actual dataset\n",
      "usually for amr data storing the grid hierarchy takes a lot less ram\n",
      "anyway, 10 gb isn't so big actually\n",
      "if you can reproduce it with one that small and upload it somewhere i can try to reproduce\n",
      "another thing to check would be to turn on bounds checking in the volume renderer\n",
      "we have it turned off for speed but that assumes there are no bugs :slightly_smiling_face:\n",
      "that has to happen at compile time though\n",
      "yep. that’s what i am going to do with this and the `ds.r` issue i mentioned earlier. i just haven’t had time to pair down and simplify everything to completely narrow down the problem to make it easier to sift through.\n",
      "i will give that a try though!\n",
      "do you guys have access to an hpc system to test problems scaling up?\n",
      "yes we do, but we don't run the unit tests on huge datasets like that\n",
      "if you'd like to help us set that up that would be awesome\n",
      "there's probably a way to test these sorts of scaling issues in a more sustainable way, right now it's more or less one-off\n",
      "anyway, looking forward to hearing more, i'd be happy to also walk you through the debugging suggestions i've made if you don't know how to do it (getting a faulthandler traceback from python, getting a gdb traceback, and turning on bounds checking)\n",
      "if i had to guess, there's some overflow issue or grid geometry bookkeeping issue in the volume renderer that causes an oob access which sometimes causes a crash\n",
      "it's all written in cython and there's a compile-time toggle to turn on bounds checking on a function by function level\n",
      "we have it turned off because it's a large performance overhead\n",
      "but you can turn it back on if you want\n",
      "that would at least give you a better traceback\n",
      "okay cool. that sounds good! i will give you a shout when i try it again because i am not entirely sure how to do those debug tests.\n",
      "what would setting up a scaling test entail?\n",
      "and what do you mean run unit tests?\n",
      "so we have a full automated test suite that runs on travis ci, appvveyor, and a jenkins instance running on a server at ncsa, every change to the codebase needs to pass the tests\n",
      "in principle someone could add some sort of automated test run with big datasets on an hpc system to our testing configuration, it would be a big job though\n",
      "but it would be valuable to have scaling tests and to make sure we're not introducing issues that only trigger for large datasets (e.g. memory or performance issues)\n",
      "not saying you need to do this, just saying we have limited resources and was trying to fully answer your question\n",
      "anyway, i'm trying to take the day off here, just wanted to quickly answer your question :slightly_smiling_face:\n",
      "have a nice day and looking forward to hearing more about your issues\n",
      "yes of course. i couldn’t do it at the moment but in the future i would definitely like to help because i think these kinds of problems are fun to tackle (and i should probably be on my own hpc allocation). thanks for the answers, enjoy the day!\n",
      "what sort of data are you working with?\n",
      "like how are the data structured in space?\n",
      "if you have x,y,z positions i guess not a uniform resolution grid?\n",
      "hello, and thank you very much. my data is on a uniform resolution grid---if i plotted just the x,y,z i would have a uniform cubic grid with nx, ny, nz points in direction\n",
      "ah, then you’re looking for yt.load_uniform_grid\n",
      "<https://yt-project.org/doc/examining/generic_array_data.html>\n",
      "thank you! i figured it out. the plots look incredible---this is an awesome package. i have pasted my code (mostly based on the tutorials) below, in case it is useful to others.\n",
      "one small question: what does `nprocs` do? does it need to equal the number of grid points?\n",
      "```\n",
      "import yt\n",
      "import numpy as np\n",
      "import matplotlib as plt\n",
      "from yt.visualization.api import streamlines\n",
      "from yt.units import mpc\n",
      "from mpl_toolkits.mplot3d import axes3d\n",
      "\n",
      "wspan = 2\n",
      "grid_pts = 64\n",
      "v_x = wspan*(2*np.random.random(size=(grid_pts, grid_pts, grid_pts))-1)\n",
      "v_y = wspan*(2*np.random.random(size=(grid_pts, grid_pts, grid_pts))-1)\n",
      "v_z = wspan*(2*np.random.random(size=(grid_pts, grid_pts, grid_pts))-1)\n",
      "\n",
      "\n",
      "data = dict(velocity_x = (v_x, \"cm/s\"),\n",
      "           velocity_y = (v_y, \"cm/s\"),\n",
      "           velocity_z = (v_z, \"cm/s\"))\n",
      "bbox = np.array([[-wspan, wspan], [-wspan, wspan], [-wspan, wspan]])\n",
      "ds = yt.load_uniform_grid(data, v_x.shape, length_unit=\"mpc\", bbox=bbox, nprocs=grid_pts)\n",
      "\n",
      "\n",
      "\n",
      "c = ds.domain_center\n",
      "n = 100\n",
      "scale = ds.domain_width[0]\n",
      "pos_dx = np.random.random((n,3))*scale-scale/2.\n",
      "pos = c+pos_dx\n",
      "pos = pos[norm(pos,axis=1)&gt;1]\n",
      "\n",
      "# create streamlines of the 3d vector velocity and integrate them through\n",
      "# the box defined above\n",
      "streamlines = streamlines(ds, pos, 'velocity_x', 'velocity_y', 'velocity_z',length=10.0*mpc)\n",
      "streamlines.integrate_through_volume()\n",
      "\n",
      "# create a 3d plot, trace the streamlines through the 3d volume of the plot\n",
      "fig=plt.figure(figsize=(10,10))\n",
      "ax = axes3d(fig)\n",
      "\n",
      "for stream in streamlines.streamlines:\n",
      "    stream = stream[np.all(stream != 0.0, axis=1)]\n",
      "    stream = stream[norm(stream,axis=1)&gt;1] # remove points in forbidden region\n",
      "    ax.plot3d(stream[:,0], stream[:,1], stream[:,2], alpha=.8, color='k',linewidth=0.2)\n",
      "\n",
      "\n",
      "```\n",
      "hi all, i’m looking into ways to distort a 3d volume rendering by modifying the ray tracing. i have an equation that describes how the ray passes through the scene based on the field data at each point. can someone point me to where would be a good place to start building this in yt?\n",
      "<@uf1uc363z> yes!  the trickiest bit will be if the scattering/distortion changes the order in which the grids are traversed.  but, this can be addressed either by using a modification of the streamlines code, or you could try out some new stuff that <@u2p0bg9q9> has been implementing for path traversal.  if theo rder of the grids does not change, you can change the lens objects in `grid_traversal.pyx`.\n",
      "<@uj8dhcnp7> has joined the channel\n",
      "hey all - we have a version of arepo where we have some custom fields in parttype3 (dust density).   i’d like to make a projection plot of that, but find that the projection plots work only on [e.g.] (`gas’,‘’density’) like fields.\n",
      "\n",
      "i’m not really sure how to convert my (`parttype3',dust_density')`  field to one like (’`gas,dust_density`  in order to make a projection plot.  is this straight forward to do?\n",
      "does `parttype3`  have smoothing lengths defined?\n",
      "\n",
      "if not,\n",
      "\n",
      "```# reload pt3 particles into a stream dataset\n",
      "ad = ds.all_data()\n",
      "\n",
      "pt = 'parttype3'\n",
      "fields = ['particle_mass'] + [f'particle_position_{ax}' for ax in 'xyz'] + ['dust_density']\n",
      "data = {field: ad[pt, field] for field in fields}\n",
      "ds_pt3 = yt.load_particles(data, data_source=ad)\n",
      "\n",
      "# generate the missing sph fields\n",
      "ds_pt3.add_sph_fields()```\n",
      "if it does have smoothing lengths defined like `parttype0`  then maybe you can just try:\n",
      "\n",
      "```ds._sph_ptypes = (\"parttype0\", \"parttype3\")```\n",
      " and then try to project\n",
      "```(parttype3',dust_density')```\n",
      "thanks <@u9ce5d9lz>!  we do have the field:\n",
      "\n",
      "```('parttype3', 'dust_dusthsml')```\n",
      "though this is different than how it’s defined for parttpye0:\n",
      "\n",
      "``` ('parttype0', 'smoothing_length'),```\n",
      "so it’s possible yt won’t know it exists.\n",
      "\n",
      "the stream idea is creative!   i tried it and the only thing is that it stuffs the `dustdensity` into both\n",
      "```('all','dustdensity')```\n",
      "and\n",
      "```('io','dustdensity')```\n",
      "is there an obvious way to get it into:\n",
      "\n",
      "```('gas','dustdensity')```\n",
      "? i think that might be the ticket if so\n",
      "hello all, i'm trying to plot vorticity magnitude fields in gaseous bluff-body flow. this was working fine with a native yt install on the hpc resources i was using before, but i switched to a new computer, where i did my own fresh install of yt last week (using the bash install script, miniconda version), and since then when i try and plot vorticity magnitude, the fields are wrong and blocky. other fields (e.g. temperature, velocity magnitude) seem to be working fine. this is amr boxlib-type data and the fields look to be polluted level-wide, with distinct level-to-level chunkiness. any idea what could be causing this, and/or what i can do to fix it?\n",
      "do you know which yt version you were using before? it might be a regression which if so would help us to fix it, if you can file an issue about this including a version of the image where things are “correct”, the yt version used to create the image, and a version where things are incorrect and that yt version, that would help us track down why it’s wrong. if you can also include a test script we can run locally using either a dataset you uplpad somewhere or one of the datasets at <http://yt-project.org/data|yt-project.org/data>, that would also help\n",
      "<@u042fh0rb> <@ujeeuv7lh> fyi with the dataset `maestro_subch_plt00248/` and yt 4 i seem to reproduce the issue\n",
      "```import yt\n",
      "ds = yt.load('maestro_subch_plt00248/')\n",
      "p = yt.sliceplot(ds, 'x', 'vorticity_magnitude')\n",
      "p.save()```\n",
      "\n",
      "\n",
      "thanks, yes - before i was using version `3.4.0` of yt, whereas now i am using `3.5.1`. my datasets are too large to share conveniently, but i was able to reproduce this issue on my laptop with the \"turbboxlowres\" orion 2 dataset at the link you provided. i created a new conda environment with yt `3.4.0` and produced the non-choppy attached image, whereas the default conda environment with yt `3.5.1` produces the choppy version. the simplified code to reproduce this issue is also attached:\n",
      "\n",
      "great, can you file an issue on github so we don’t lose track? if you’re feeling ambitious you could also try running git bisect to find the precise commit that broke things\n",
      "also maybe ping <@u043bna00> (same github username)\n",
      "(i’m on phone right now so it’s not so easy to do it myself)\n",
      "<@u9ce5d9lz> has joined the channel\n",
      "<@uatntqy9l> has joined the channel\n",
      "hey, is there something in yt where i can re-find particles in “halos” within e.g. 1.2x r_vir, instead of 1x r_vir?\n",
      "i would just try to write something myself but i imagine there are a _lot_ of optimizations that can be made!\n",
      "there's a project that builds off of yt's capabilities called `caesar` (by <@u050ek2tc>) -- \n",
      "\n",
      "<https://caesar.readthedocs.io/en/latest/>\n",
      "it can do what you're asking for maybe with a tiny bit of modification\n",
      "and works for gadget-oids\n",
      "ah yeah i am already using caesar — i’d be interested to figure out how to get that. i’ve been chatting with romeel about it but i think i need to escalate to a higher support level :stuck_out_tongue:\n",
      "it uses fof to find halo and galaxy groups, and then dumps them into an object.  to use it in it's default form you can do something like:\n",
      "\n",
      "```\n",
      "import caesar\n",
      "obj = caesar.load(snapshot_name)\n",
      "obj.member_search()\n",
      "obj.save(caesarfile)  #for future usage of the caesar file -- in the future you can just caesar.load the caesarfile i think\n",
      "\n",
      "```\n",
      "\n",
      "then the `obj` is an object that holds all the galaxy and halo info.  you could access the particle *indices* from the most massive halo by:\n",
      "\n",
      "```\n",
      "glist = obj.halos[0].glist #get the gas particles in the halo; slist would do star particles\n",
      "ds = yt.load(snapshotname)\n",
      "obj.yt_dataset = ds\n",
      "ad = ds.all_data()\n",
      "gas_masses = ad[('parttype0','masses')][glist]\n",
      "\n",
      "```\n",
      "or something like that anyways...might be some slight syntax errors\n",
      "thanks :slightly_smiling_face:. i also still need to figure out how to properly get my ahf stuff into caesar… this project has been a bit fast-paced so i’ve just been running around like a headless chicken\n",
      "do you collaborate with romeel at all? i know mika rafierferantosa was trying to get `velociraptor` to play nice with `caesar` and had it working, though i could never get it to produce reasonable results\n",
      "if you do get ahf to play nice with `caesar` let me know!  <@u91855pa9> was working on this for a bit as well on our end, though we decided to stay with the built in `caesar` fof for now\n",
      "i have a thing that emulates part of the api for caesar for ahf (because we didn’t trust the fof — rightly so, actually, in our case). it’s in <https://github.com/jborrow/lagrangian-transfer> but of course it’s a piece of crap and isn’t ready for prime-time. i’ll probably be doing a little touch-up on caesar soon to make it work properly with py3 as well, so i’ll try to let you know when that’s sorted\n",
      "yeah — i’m working with romeel on the project i’d like this for actually. we still haven’t got velociraptor working yet, unfortunately. i would like to work on that because we have velociraptor running on-the-fly in swift (i.e. during the simulation we output halo catalogues at a higher frquency than particle data).\n",
      "oh cool! i'll point <@u91855pa9> at the above github page!\n",
      "it’s not particularly helpful, but there are some conversion scripts in there that stick things in a fakecaesar object if you really need something to work asap\n",
      "i think the way to do this in the halo objects api in yt is to create a callback on the halo finding process that attaches a sphere data object at 1.2 rvir to the halo object during processing.\n",
      "it might be easier/nicer in caesar. tbh the halo objects api in yt needs some love, could be a lot nicer and easier to use. however we tend to think things like that should be outside core yt going forward.\n",
      "thanks for your help, both. i’ll try to contact mika and we can chat about how to do this within caesar, failing that i’ll look into the callback in yt.\n",
      "hey! i was trying to save and load a geometric data container (a sphere) based on a ramses dataset. however, when i reload it, it seems that i can't do plot anymore\n",
      "i get the error\n",
      "```\n",
      "in [49]: p = yt.projectionplot(dss, 'x', [('gas', 'density'), ('deposit', 'gas_tracer_density')], center=center, width=(rvir, 'mpc'))\n",
      "yt : [debug    ] 2018-07-24 19:17:50,821 appending object to info_00143.hdf5.h5 (type: &lt;class 'yt.data_objects.selection_data_containers.ytregion'&gt;)\n",
      "---------------------------------------------------------------------------\n",
      "ytinvalidfieldtype                        traceback (most recent call last)\n",
      "~/codes/yt/scripts/iyt in &lt;module&gt;()\n",
      "----&gt; 1 p = yt.projectionplot(dss, 'x', [('gas', 'density'), ('deposit', 'gas_tracer_density')], center=center, width=(rvir, 'mpc'))\n",
      "\n",
      "~/codes/yt/yt/visualization/plot_window.py in __init__(self, ds, axis, fields, center, width, axes_unit, weight_field, max_level, origin, right_handed, fontsize, field_parameters, data_source, method, proj_style, window_size, aspect)\n",
      "   1452         # plotting classes to avoid an exception\n",
      "   1453         test_data_source = ds.all_data()\n",
      "-&gt; 1454         validate_mesh_fields(test_data_source, fields)\n",
      "   1455 \n",
      "   1456         if isinstance(ds, ytspatialplotdataset):\n",
      "\n",
      "~/codes/yt/yt/visualization/plot_window.py in validate_mesh_fields(data_source, fields)\n",
      "    137 \n",
      "    138     if len(invalid_fields) &gt; 0:\n",
      "--&gt; 139         raise ytinvalidfieldtype(invalid_fields)\n",
      "    140 \n",
      "    141 \n",
      "\n",
      "ytinvalidfieldtype: \n",
      "sliceplot, projectionplot, and offaxisprojectionplot can only plot fields that\n",
      "are defined on a mesh, but received the following particle fields:\n",
      "\n",
      "    [('gas', 'density')]\n",
      "\n",
      "did you mean to use particleplot or plot a deposited particle field instead?\n",
      "```\n",
      "is there a way to \"directly\" plot, e.g. the gas density? or do i have to do a deposition (but then some information would be lost)\n",
      "i don’t think it’s possible to do that with a sphere data object, no\n",
      "you can make a `sliceplot` with a `sphere` data source and then save the slice object\n",
      "you’ll be able to make a `sliceplot` from that\n",
      "the basic issue is that we don’t save the amr index information when you save a 3d data container\n",
      "so it can’t reconstruct the original amr hierarchy\n",
      "i think we treat everything as particle data when we reload it from a data object\n",
      "for the slice we can save a 2d version of the amr hierarchy information so we can reconstruct the plot\n",
      "ok ; and in general is it possible to somehow convert the region to a sph-like dataset and then reload it?\n",
      "the issue i have is that i have a (massive) simulation, that takes ages to load and i would like to be able to extract the relevant information once and for all to speed up the analysis\n",
      "even with `bbox`?\n",
      "i'm doing a zoom simulation for which ~50% of the cpus are in the central region, so i don't gain much using bbox\n",
      "the converting to an sph-like dataset is basically what’s happening already, except support for sph data isn’t so great right now\n",
      "so we treat it as pure n-body\n",
      "to treat it like sph we’d need a way of generating smoothing lengths\n",
      "which we don’t have until the yt-4.0 branch\n",
      "ok, that makes sense\n",
      "i'll stick to my good ol' amr then\n",
      "are there any special instructions for installing yt from source with intel compilers loaded?\n",
      "\n",
      "```\n",
      "[desika.narayanan@login3] $ module list\n",
      "\n",
      "currently loaded modules:\n",
      "  1) intel/2018   2) hdf5/1.10.1   3) openmpi/3.1.0   4) git/2.14.1\n",
      "\n",
      "```\n",
      "\n",
      "after what seems like a successful `yt` installation from source:\n",
      "\n",
      "```\n",
      "in [1]: import yt\n",
      "---------------------------------------------------------------------------\n",
      "importerror                               traceback (most recent call last)\n",
      "&lt;ipython-input-1-2d2292a375dc&gt; in &lt;module&gt;()\n",
      "----&gt; 1 import yt\n",
      "\n",
      "~/yt/yt/__init__.py in &lt;module&gt;()\n",
      "     23 import numpy # in case anyone wishes to use it by name\n",
      "     24\n",
      "---&gt; 25 from yt.funcs import \\\n",
      "     26     iterable, \\\n",
      "     27     get_memory_usage, \\\n",
      "\n",
      "~/yt/yt/funcs.py in &lt;module&gt;()\n",
      "     45     ytequivalentdimserror\n",
      "     46 from yt.extern.tqdm import tqdm\n",
      "---&gt; 47 from yt.units.yt_array import ytarray, ytquantity\n",
      "     48 from functools import wraps\n",
      "     49\n",
      "\n",
      "~/yt/yt/units/__init__.py in &lt;module&gt;()\n",
      "----&gt; 1 from yt.units import unit_symbols\n",
      "      2 from yt.utilities import physical_constants\n",
      "      3\n",
      "      4 from yt.units.yt_array import ytquantity\n",
      "      5\n",
      "\n",
      "~/yt/yt/units/unit_symbols.py in &lt;module&gt;()\n",
      "     12 # the full license is in the file copying.txt, distributed with this software.\n",
      "     13 #-----------------------------------------------------------------------------\n",
      "---&gt; 14 from yt.units.yt_array import ytquantity as quan\n",
      "     15\n",
      "     16 #\n",
      "\n",
      "~/yt/yt/units/yt_array.py in &lt;module&gt;()\n",
      "     37     positive, divmod_, isnat, heaviside = (none,)*4\n",
      "     38\n",
      "---&gt; 39 from yt.units.unit_object import unit, unitparseerror\n",
      "     40 from yt.units.unit_registry import unitregistry\n",
      "     41 from yt.units.dimensions import \\\n",
      "\n",
      "~/yt/yt/units/unit_object.py in &lt;module&gt;()\n",
      "     32     unit_prefixes, prefixable_units, latex_prefixes, \\\n",
      "     33     default_base_units\n",
      "---&gt; 34 from yt.units.unit_registry import \\\n",
      "     35     unitregistry, \\\n",
      "     36     unitparseerror\n",
      "\n",
      "~/yt/yt/units/unit_registry.py in &lt;module&gt;()\n",
      "     19 from yt.units.unit_lookup_table import \\\n",
      "     20     default_unit_symbol_lut\n",
      "---&gt; 21 from yt.utilities.lib.fnv_hash import fnv_hash\n",
      "     22 from yt.extern import six\n",
      "     23 from sympy import \\\n",
      "\n",
      "importerror: /home/desika.narayanan/yt/yt/utilities/lib/fnv_hash.cpython-36m-x86_64-linux-gnu.so: undefined symbol: __intel_sse2_strchr\n",
      "```\n",
      "also:\n",
      "\n",
      "```\n",
      "[desika.narayanan@login3 ~]$ cd yt\n",
      "[desika.narayanan@login3 yt]$ git rev-parse head\n",
      "f3ee5f35fbcf43bfde322be0e56465e2dbe0eb8f\n",
      "```\n",
      "we don’t test compiling with the intel compilers and i’ve never tried doing it\n",
      "is there no gnu toolchain on this system?\n",
      "if i had to guess what the issue is, you’re not linking against the intel-provided standard library\n",
      "ah yeah - there are gnu compilers...i'll probably just do that\n",
      "pretty sure that's how i've done it before...sysadmins get fussy when you report a problem and find out that we compiled without intel so i just thought i'd reinstall\n",
      "yeah that worked:\n",
      "\n",
      "```\n",
      "[desika.narayanan@login3 ~]$ module list\n",
      "\n",
      "currently loaded modules:\n",
      "  1) git/2.14.1   2) gcc/5.2.0   3) hdf5/1.8.16\n",
      "\n",
      "inactive modules:\n",
      "  1) openmpi\n",
      "\n",
      "\n",
      "\n",
      "[desika.narayanan@login3 ~]$ ipython\n",
      "imporpython 3.6.5 |anaconda, inc.| (default, apr 29 2018, 16:14:56)\n",
      "type 'copyright', 'credits' or 'license' for more information\n",
      "ipython 6.4.0 -- an enhanced interactive python. type '?' for help.\n",
      "\n",
      "in [1]: import yt\n",
      "\n",
      "in [2]:\n",
      "```\n",
      "in principle it’s something that would be nice to support, in practice it’s a pita because we need to ask intel special permission to get binaries\n",
      "i have thought about trying to have save_as_dataset save an amr format that would be a subset of the full dataset. i think this would be great to have for this type of case, but i haven’t gotten far with it yet\n",
      "awesome, ideally we could simultaneously support block structured and octree amr\n",
      "hahaha, i tried really hard to figure out how that was related to our conversation\n",
      "<@uqddmqeea> has joined the channel\n",
      "<@uqddmqeea> can you share the code you have right now? makes it easier for me to help if i can build off a runnable example, bonus points if your example uses one of the public data files on <http://yt-project.org/data|yt-project.org/data> so i can run the full script myself\n",
      "halos_ds = yt.load(‘halo_catalogs/catalog/catalog.0.h5’)\n",
      "hc = halocatalog(data_ds=ds, halos_ds=halos_ds,output_dir=‘halocatalog’)\n",
      "hc.load()\n",
      "hi. the above is what i do to load the catalog, which was generated with the following::\n",
      "import yt\n",
      "ds = yt.load(path)\n",
      "hc = halocatalog(data_ds=ds, finder_method=‘hop’)\n",
      "hc.create()\n",
      "…and then i’m stuck as to what to do with the catalog. i can overplot it on a projection of the dm field, but i can’t easily access the halo properties\n",
      "it’s unfortunately not from the public data files. this is from a ramses pure-dm simulation\n",
      "sure but the workflow with one of the public datasets should hopefully match the workflow with yours, one sec\n",
      "actually, is there any chance you can share that data file? i don't have one produced by hop\n",
      "you can do `yt upload my_file.tar.gz` on the bash command line and then share the link it prints\n",
      "<@uqddmqeea> ^\n",
      "ah nm, i think i can do it with `rockstar_halos`\n",
      "ah i see\n",
      "i don't think you need the halocatalog at all if you have the halos dataset:\n",
      "```halos_ds = yt.load('rockstar_halos/halos_0.0.bin')\n",
      "ad = halos_ds.all_data()\n",
      "ad['halos', 'particle_mass']```\n",
      "each \"particle\" is an individual halo\n",
      "so the \"particle_mass\" field is the mass of the halo\n",
      "yes, this works, thanks!\n",
      "i’m uploading the ramses snapshot, just in case it’s useful\n",
      "if you have more questions it will be :slightly_smiling_face:\n",
      "take a look at `halos_ds.field_list` and `halos_ds.derived_field_list` for more fields that are available\n",
      "one more stupid question…how do i access the box width?\n",
      "ds.domain_width\n",
      "there's also ds.domain_left_edge and ds.domain_right_edge\n",
      "awesome, thanks!\n",
      "hello everyone,\n",
      "i'm working on cosmological simulation using gadget2, but when i load the snapshot file, i got this error:\n",
      "```yt.utilities.exceptions.ytdomainoverflow: particle bounds (-55.70845, -101.51052, -175.99706) and (11999.999, 11999.999, 11999.999) exceed domain bounds [0. 0. 0.] code_length and [12000. 12000. 12000.] ```\n",
      "i don't know how to fix it , and it is strange that just only one snapshot file has this error, there is no problem when i load another snapshot file.\n",
      "does anyone know what's wrong with this ?\n",
      "<@uruuwnsr3> has joined the channel\n",
      "<@u042fh0rb> -- following up on yesterday, i think that the `region` isn't actually doing exactly what i want.   what i'm hoping for is to create a new ds that is a subvolume of the originally loaded ds.  that is, if i run the following:\n",
      "\n",
      "```import yt\n",
      "import numpy as np\n",
      "\n",
      "snapshot = '/users/desika.narayanan/dropbox/yt_datasets/enzo_iso_galaxy/galaxy0030/galaxy0030'\n",
      "\n",
      "ds = yt.load(snapshot)\n",
      "center = ds.arr([0.5,0.5,0.5],'code_length')\n",
      "len = ds.quan(0.25,'code_length')\n",
      "\n",
      "\n",
      "region = ds.region(center,center-len,center+len)\n",
      "\n",
      "print(ds.domain_right_edge)\n",
      "print(region.ds.domain_right_edge)\n",
      "print(region.right_edge)```\n",
      "the return output is:\n",
      "\n",
      "```[1. 1. 1.] code_length\n",
      "[1. 1. 1.] code_length\n",
      "[0.75 0.75 0.75] code_length```\n",
      "in other words, what i want is that `region.ds.domain_right_edge` also = [0.75,0.75,0.75] in this example\n",
      "the use case is that if i want to find the values corresponding to a particular derived_field_list quantity (say, stellar positions), accessing them from `region.ds` looks like it will still just access the original loaded dataset, and not the region\n",
      "athe use-case is that if i want to find the values in a particular field in\n",
      "maybe you want the ytdata frontend then?\n",
      "or just get the fields from the data object\n",
      "`reg[“star”, “particle_position_x”]`\n",
      "there’s no need to reload as a new dataset if all you want is field values in the data object\n",
      "ah i think this is exactly what i need - thanks <@u042fh0rb>!\n",
      "hello all! im trying to make an off axis projection plot for a ramses simulation (new horizon), and im getting confused with the output. i'm putting in vectors to do the projection along, but when i run it, the xlim, ylim, and zlim values are strange as the box goes from 0 -&gt; 1 but values are running from negative to positive?\n",
      "here is an example of what i made\n",
      "and here is what is coming up\n",
      "the limits issue and the off axis projection plots are still there even when i don't use a sphere, and the l vector which i chose as my axis was obtained from the angular momentum information from the newhorizon hop catalogs\n",
      "and even when i use sp.quantities.angular_momentum_vector() and use that value for l, the same issue comes up and the projection plot looks pretty much the same\n",
      "my key issue here seems to be that the limits are being fed in incorrectly as these are not around the centre value that i put in\n",
      "so the values being negative is a red herring, the origin of the plot coordinate system is the center of the plot\n",
      "what’s ds.domain_left_edge and ds.domain_right_edge?\n",
      "i’m curious if the center values you’re putting in by hand disagree with the coordinate system yt is inferring\n",
      "oh wait, it’s in your output\n",
      "i don’t see anything obviously wrong in the screenshot you shared\n",
      "if you can make a runnable example using one of the ramses datasets on <http://yt-project.org/data|yt-project.org/data> that would make it easier for one of us to see exactly what’s going wrong since we’ll be able to run your code locally and poke around\n",
      "ok will do! thanks!!\n",
      "hey is there a way to access adjacent cells in art/amr codes? i want to check if a cell has a certain property and one of the cells touching it has the opposite property (that is, i'm looking for a boundary). i could make little spheres and ask about np.any but that seems like it'll be prohibitively slow. my best guess is that it'll have something to do with particle indexing.\n",
      "<@uc0r9khha> has joined the channel\n",
      "<@u1dlem6kw> has joined the channel\n",
      "i’m trying to use `yt` as an i/o wrapper; is there any way to stop it meshing my data and just give me access to the particle fields?\n",
      "not without the demeshening no\n",
      "(the yt-4.0 branch)\n",
      "no worries thanks nathan \n",
      "<@ute2603sq> has joined the channel\n",
      "hey there! i find myself very frequently calling things like `plot.set_cmap('density', 'some_cmap')`. is there a way to make this permanent?\n",
      "<@u37dtbl6n> i believe there's a config option for default cmap globally. i don't think there is per field though.\n",
      "too bad that would have been neat\n",
      "<@u0hjx841z> has joined the channel\n",
      "i agree completely.  i think <@u042fh0rb> and i chatted about that a little bit ago. i do know that if you set your yt config parameter `default_colormap` it'l change the default\n",
      "it’s a reasonable feature request, please feel free to file an issue about it\n",
      "there you go <https://github.com/yt-project/yt/issues/1888>\n",
      "hi! i’m trying to take a sphere out of a halo with `ds.sphere(center, radius)`, i wonder if the radius is in proper or comoving units?\n",
      "hi! `radius` is in proper units, unless you set it like `(10, 'mpccm')`\n",
      "the suffix `cm` stands for comoving. this is probably only supported if your dataset is detected as a cosmological one\n",
      "i see. so i calculate the distance between two points, it will always be in proper, unless i change it to cm?\n",
      "you'd get proper cm\n",
      "what’s proper cm?\n",
      "proper cm would be the physical distance between two objects\n",
      "oh i see, thank a lot!!!\n",
      "don't trust me too much on that topic though, you should probably check all that in a textbook so that everything makes sense :slightly_smiling_face:\n",
      "in any case, `pc` in yt is physical parsec, while `pccm` would be comoving parsec\n",
      "yeah, append \"cm\" to the name of any length unit to get the comoving version\n",
      "you can also divide by h if you're a masochist like that :wink:\n",
      "so comoving kiloparsecs/h would be \"kppcm/h\"\n",
      "i'm pretty sure this is covered in the docs...\n",
      "<http://yt-project.org/doc/analyzing/units/comoving_units_and_code_units.html>\n",
      "i guess i need to introduce a particle filter but i don't know what the star/gas/dark matter particles are assigned as (i.e. 1, 2 etc) in ramses\n",
      "hi kriti, if you are using ramses, then there is no such thing as gas particles, as the code is grid-based\n",
      "if you want to do weighted projections of data on the grid, you can use `yt.projectionplot(ds, 'x', ('gas', 'density'), weight_field=('cell', 'mass'))`\n",
      "for particle-based data, you can either use their projection on the grid (e.g. `('deposit', 'all_cic')` which would be the cic-deposition of all the particles) or use `yt.particleprojectionplot`\n",
      "hi\n",
      "i tried the ('gas', 'density') thing\n",
      "and another problem has arisen where all of my plots seem to just be coloured squares\n",
      "which i also assume is a resolution issue but when i tried to use the set_buff_size() function it didn't recognise it\n",
      "and also in response to the particle projection plot answer, how would i isolate just the stars or just the dark matter?\n",
      "it depends on how old your ramses version is. if you use recent versions (less than 1-2 years old), then yt detects stars from dark matter automatically. otherwise, you dark matter particles have a null birth time while star do not\n",
      "so you can use a particle filter (look in the doc) to distinguish between the two\n",
      "by default, yt uses a 800x800 grid so you should be fine. if you see only coloured squares, it may indicate that your run has only very coarse cells, or that you tried to make a plot that is too small.\n",
      "in any case, you can look at the ramses log to know how much cells have been created at each level\n",
      "ahh ok ok nice\n",
      "which version are you using?\n",
      "im using new horizon\n",
      "not sure how to tell how old it is\n",
      "but i think it might be 2019?\n",
      "(i'm yohan dubois' old phd student)\n",
      "and no, it is not based on a recent version of ramses unfortunately. you'll have to use a particle filter\n",
      "are you using new horizon's code, or new horizon the simulation?\n",
      "new horizon the simulation\n",
      "<https://yt-project.org/doc/analyzing/filtering.html>\n",
      "cphyc would know better than me how to identify particle types in your output\n",
      "```def stars(pfilter, data):\n",
      "    '''\n",
      "    select stars particles\n",
      "    '''\n",
      "    if data.ds.cosmological_simulation==1:\n",
      "        filter = (data['io','particle_birth_time'] != 0) &amp; (data['io','particle_birth_time'] != none)\n",
      "    else:\n",
      "        filter = (data['io','particle_birth_time'] != 0)\n",
      "    return filter\n",
      "\n",
      "\n",
      "def dm(pfilter, data):\n",
      "    '''\n",
      "    select dm particles\n",
      "    '''\n",
      "    if data.ds.cosmological_simulation==1:    \n",
      "     if data['io','particle_birth_time'] != none:\n",
      "         filter = (data['io','particle_birth_time'] == 0) &amp; (data['io','particle_identity'] &gt; 0)\n",
      "     else:\n",
      "         filter= (data['io','particle_identity'] &gt;0)\n",
      "    else:\n",
      "         filter = ((data['io','particle_birth_time'] == 0 ) &amp; (data['io','particle_identity'] &gt; 0))\n",
      "    return filter\n",
      "\n",
      "\n",
      "def young_stars(pfilter, data):\n",
      "    '''\n",
      "    select particles created after the beginning of the simulation,\n",
      "    that are younger than 10myr.'''\n",
      "    filter = ((data['io','particle_birth_time'] &gt; 0) &amp;\n",
      "              (data['io','particle_birth_time'] &lt; data.ds.arr(10, 'myr')) &amp;\n",
      "              (data['io','particle_birth_time'] != none))\n",
      "    return filter```\n",
      "what do i do about the fact that any time a try and pan, zoom or set buff size, it keeps saying 'attributeerror: 'list' object has no attribute 'pan_rel' '\n",
      "you can use this code to filter the particles\n",
      "or no attribute zoom, or no attribute buff size\n",
      "could you paste the code that doesn't work? from the look of it, you're trying to apply functions that work on `projectionplot` objects but on `list` objects\n",
      "sure one sec\n",
      "plot1 = yt.projectionplot(ds, \"z\", (\"gas\", \"density\"), width = (140, \"kpc\")).save()\n",
      "yt : [info     ] 2019-11-28 14:11:05,307 projection completed\n",
      "yt : [info     ] 2019-11-28 14:11:05,308 xlim = 0.499014 0.500986\n",
      "yt : [info     ] 2019-11-28 14:11:05,308 ylim = 0.499014 0.500986\n",
      "yt : [info     ] 2019-11-28 14:11:05,309 xlim = 0.499014 0.500986\n",
      "yt : [info     ] 2019-11-28 14:11:05,309 ylim = 0.499014 0.500986\n",
      "yt : [info     ] 2019-11-28 14:11:05,310 making a fixed resolution buffer of (('gas', 'density')) 800 by 800\n",
      "yt : [info     ] 2019-11-28 14:11:05,733 saving plot info_00448_projection_z_density.png\n",
      "&gt;&gt;&gt; plot1.set_buff_size(1600)\n",
      "traceback (most recent call last):\n",
      "  file \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n",
      "attributeerror: 'list' object has no attribute 'set_buff_size'\n",
      "&gt;&gt;&gt; plot1.pan_rel((-0.1, -0.2))\n",
      "traceback (most recent call last):\n",
      "  file \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n",
      "attributeerror: 'list' object has no attribute 'pan_rel'\n",
      "is it because i did the .save() thing\n",
      "surely not\n",
      "yup, `.save()` returns the list of the file names that have been stored\n",
      "lame, ok\n",
      "if you print plot1, you'll see `['info_00448_projection_z_density.png']`\n",
      "yeah :confused:\n",
      "ok ok so to be able to move around on it i need to do the projection plot without saving it\n",
      "i think this is a unit issue - i'll take a look this afternoon. i think its because one of your plots is (gas, density) which defaults to cgs but this is filter for (parttype0, density) which defaults to code length \n",
      "well, code_mass/code_length**2 for projected density\n",
      "i think those results are correct, you're just comparing different units but this was just a 2 minute glance\n",
      "replace this line,\n",
      "```\n",
      "plot = plt.imshow(\n",
      "    np.array(image), cmap=\"magma\", norm=matplotlib.colors.lognorm()\n",
      ")\n",
      "```\n",
      "with,\n",
      "```\n",
      "plot = plt.imshow(\n",
      "    image.in_base('cgs').d, cmap=\"magma\", norm=matplotlib.colors.lognorm()\n",
      ")\n",
      "```\n",
      "i think that will fix your issue, sorry this has been a pain for you :slightly_smiling_face:\n",
      "ah-ha that fixes it!\n",
      "thought it would be nice to get rid of this potential tripping point by not needing to blow away the units :thinking_face:\n",
      "i suppose that's why `yt.projectionplot` exists and eventually it'll be supported in 4.0\n",
      "it is supported?\n",
      "as is `offaxisprojectionplot`\n",
      "<@u91855pa9> is something wrong with either of those? i’m puzzled by your comment that they’re not supported\n",
      "hm. i swear i tried that and i got a notimplementederror. i'll check back in a bit\n",
      "and you don’t need to blow away the units either, you do need to convert them to whatever units you want to plot them in though\n",
      "hunh, that's very strange. i have no idea what i was doing that threw a notimplementederror, but as you say yt.projectionplot works just fine\n",
      "oh found it. it's from doing a yt.projectionplot with a filtered field\n",
      "ah ok - if you can make an example triggering that error i can try to take a look\n",
      "i bet this is the internals going `field[0] != ds._sph_ptype: panic and error`\n",
      "\n",
      "this is really no big deal for me, i can easily just use the manual plotting interface. it was just a bit of a tripping hazard with units earlier\n",
      "because `imshow`'ing an array with units doesn't work\n",
      "yeah, it should work though\n",
      "i suspect we have a few areas where we check the basic, `field[0] != ds._sph_type`, maybe we should boilerplate this and make it more general for derived fields <@u042fh0rb>?\n",
      "yup, agreed :slightly_smiling_face:\n",
      "ah really?\n",
      "argh matplotlib keeps breaking that\n",
      "i keep on fixing it and they keep on breaking it lol\n",
      "are you fixing `matplotlib` itself ?\n",
      "i swear i got an exception last night about a ufunc not being applicable between a yt array and something unitless, but i can't triviall reproduce it now\n",
      "yes, fixing matplotlib itself\n",
      "<https://github.com/matplotlib/matplotlib/pulls?utf8=%e2%9c%93&amp;q=is%3apr+author%3angoldbaum>\n",
      "most of those are things found by yt users\n",
      "matplotlib is a fun codebase to dive into, i like big python codebases :slightly_smiling_face:\n",
      "mercurial is also pretty fun although also kind of insane in the things it tries to handle\n",
      "heisengbug spotted\n",
      "is there a way to prevent yt from using all the cpus when doing projections/slices?\n",
      "`export omp_num_threads=1` doesn't seem to do the trick\n",
      "oh wait... sorry i was being stupid\n",
      "didn't type it in the right terminal....\n",
      "heh\n",
      "and is there a way with matplotlib styles to mimic the look of yt's plot?\n",
      "i'm thinking about the fonts in particular\n",
      "there’s a `yt.matplotlib_style_context()` you can use\n",
      "not sure if it does everything you want it to do :slightly_smiling_face:\n",
      "the math fonts are annoying and need to be controlled with the style context manager\n",
      "but the other fonts are `stixgeneral`\n",
      "math fonts are used for anything inside of latex, the other fonts are used for all the other text\n",
      "where's the matplotlib_style_context object located? i don't have it under yt\n",
      "oh, sorry, `yt.funcs.matplotlib_style_context`\n",
      "you can use it as a context manager, it just sets a couple of matplotlib rc values\n",
      "great, let me try this\n",
      "hold on, yt installed succesfully but it definitely is broken. i made lightrays using trident and when i try to load them using `ds.all_data()`. it breaks and gives me this error :\n",
      "```in [7]: ad = r1.all_data()\n",
      "yt : [info     ] 2020-06-15 20:25:02,892 allocating for 7.715e+04 particles\n",
      "---------------------------------------------------------------------------\n",
      "oserror                                   traceback (most recent call last)\n",
      "/afs/mpa/temp/mrmgehlm/yt/yt/geometry/particle_geometry_handler.py in _initialize_index(self)\n",
      "    131         try:\n",
      "--&gt; 132             rflag = self.regions.load_bitmasks(fname)\n",
      "    133             rflag = self.regions.check_bitmasks()\n",
      "\n",
      "/afs/mpa/temp/mrmgehlm/yt/yt/geometry/particle_oct_container.pyx in yt.geometry.particle_oct_container.particlebitmap.load_bitmasks()\n",
      "\n",
      "oserror:\n",
      "\n",
      "during handling of the above exception, another exception occurred:\n",
      "\n",
      "attributeerror                            traceback (most recent call last)\n",
      "&lt;ipython-input-7-b84353e50918&gt; in &lt;module&gt;\n",
      "----&gt; 1 ad = r1.all_data()\n",
      "\n",
      "/afs/mpa/temp/mrmgehlm/yt/yt/data_objects/static_output.py in all_data(self, find_max, **kwargs)\n",
      "    928         which covers the entire simulation domain.\n",
      "    929         \"\"\"\n",
      "--&gt; 930         self.index\n",
      "    931         if find_max: c = self.find_max(\"density\")[1]\n",
      "    932         else: c = (self.domain_right_edge + self.domain_left_edge)/2.0\n",
      "\n",
      "/afs/mpa/temp/mrmgehlm/yt/yt/data_objects/static_output.py in index(self)\n",
      "    503                 raise runtimeerror(\"you should not instantiate dataset.\")\n",
      "    504             self._instantiated_index = self._index_class(\n",
      "--&gt; 505                 self, dataset_type=self.dataset_type)\n",
      "    506             # now we do things that we need an instantiated index for\n",
      "    507             # ...first off, we create our field_info now.\n",
      "\n",
      "/afs/mpa/temp/mrmgehlm/yt/yt/geometry/particle_geometry_handler.py in __init__(self, ds, dataset_type)\n",
      "     25         self.float_type = np.float64\n",
      "     26         super(particleindex, self).__init__(ds, dataset_type)\n",
      "---&gt; 27         self._initialize_index()\n",
      "     28\n",
      "     29     def _setup_geometry(self):\n",
      "\n",
      "/afs/mpa/temp/mrmgehlm/yt/yt/geometry/particle_geometry_handler.py in _initialize_index(self)\n",
      "    138             self.regions.reset_bitmasks()\n",
      "    139             self._initialize_coarse_index()\n",
      "--&gt; 140             self._initialize_refined_index()\n",
      "    141             wdir = os.path.dirname(fname)\n",
      "    142             if not dont_cache and os.access(wdir, os.w_ok):\n",
      "\n",
      "/afs/mpa/temp/mrmgehlm/yt/yt/geometry/particle_geometry_handler.py in _initialize_refined_index(self)\n",
      "    177         mylog.debug(\"using estimated thresholds of %s and %s for refinement\", mask_threshold, count_threshold)\n",
      "    178         total_refined = 0\n",
      "--&gt; 179         total_coarse_refined = ((mask &gt;= 2) &amp; (self.regions.particle_counts &gt; count_threshold)).sum()\n",
      "    180         mylog.debug(\"this should produce roughly %s zones, for %s of the domain\",\n",
      "    181                     total_coarse_refined, 100 * total_coarse_refined / mask.size)\n",
      "\n",
      "attributeerror: 'yt.geometry.particle_oct_container.particlebitmap' object has no attribute 'particle_counts'```\n",
      "so i guess copying the contents of `/yt/build` from an older version breaks it.\n",
      "for reference, it seems like the actual yt-4.0 release is still a substantial ways out based on the discussions online last week.  the yt-4.0 merge with the master branch is going to happen soon, though, but that will still require building from source at some level.\n",
      "yes, i didn’t mean to make it seem like it was only a matter of days, though i couldn’t resist the use of “soon”. it’s merely a product of my personal enthusiasm :sweat_smile:\n",
      "wow, really nice\n",
      "<https://user-images.githubusercontent.com/7817509/80519561-2dfd7700-8956-11ea-8ac2-500155a83abe.png>\n",
      "^ that's isolatedgalaxy for you galactic types\n",
      "i made a cookbook receipe with that\n",
      "this looks perfect for the “galaxy class” brought up at the cca workshop.\n",
      ":wink:\n",
      "i had a lot of trouble because the densities are 40 orders of magnitude smaller than i am used to\n",
      "excuse, is there a simple way to quick access the field at the location of the particle? for example, the gravitational potential at the location of the particle?\n",
      "i think you could use the `point` data object.  for example, you could create a `point` for your desired location, then sample any arbitrary field from that location.\n",
      "\n",
      "```import yt\n",
      "ds = yt.load('isolatedgalaxy/galaxy0030/galaxy0030')\n",
      "p = ds.point([x,y,z])\n",
      "print(p[('gas', 'density')])```\n",
      "\n",
      "you might have to make a new derived field for the gravitational potential in your sim.  what sort of dataset are you using?\n",
      "hey! is there a way in yt to do spherical projection (e.g. mollweide projections) from data on in cartesian coordinates?\n",
      "i know i can use cartopy but i was wondering if there was some stuff already included in yt. i remember seeing a long pr about this some time ago but i can't remember what it was about\n",
      "<@u37dtbl6n> <http://yt-project.org/docs/dev/visualizing/geographic_projections_and_transforms.html> but i don't know if that's what you're looking for\n",
      "doesn't it work only for data that has a \"geographic\" geometry? i.e. can i use with 'cartesian' data?\n",
      "i dunno, <@u44sw7j11> would be the person to ask\n",
      "there's code in the volume renderer to do stuff like this, but not elsewhere\n",
      "the code you need is probably in yt but would need to be wired up\n",
      "and yeah, there's no reason in principle why the cartopy wrappers couldn't work with your cartesian data, but you'd probably need to wire that up\n",
      "you could probably pass images to cartopy directly\n",
      "that's what i'm going to do then\n",
      "it's just a matter of doing a phaseplot and get the frb to be displayed on a \"regular\" projection\n",
      "so i set up the projections stuff so that it *could* be extensible to other types coordinate systems, but i didn’t test this out too much because i didn’t have a dataset immediately accessible to look at.\n",
      "there are properties in the coordinate handler called `data_projection` and `data_transform` that for all systems other than the geographic handler that are set to `none`. you could change these for your dataset and use them, i think, but it’ll still be backed by cartopy.\n",
      "the `data_transform` property should be what your data is in — i haven’t checked this but i imagine that cartopy has a way of handling a regular mesh.\n",
      "and then you could set `data_projection` to `mollweide`.\n",
      "again, i didn’t check this out for non-geo datasets but i’d love to help refine this feature if you have a good dataset i can use, <@u37dtbl6n>. it’d be great to at least get an example in the docs to show that it can be done for non geo datasets\n",
      "thanks!\n",
      "<@u5et0rnke> has joined the channel\n",
      "hi all, i'm a bit new to working in parallel with yt. my script appears to get stuck at the end of a yt.parallel_objects loop. this is happening on stampede2, python 2.7.15 (not sure which yt version since \"yt version\" on command line doesn't work). if anyone has experienced something similar to this, i appreciate any advice you may have!\n",
      "never mind! i forgot that (my version at least of) yt doesn't like it when i try to use more than 1 node. here is a ~minimal working example (might be possible to make more minimal but i had to wait a while to get the job started the first time around):\n",
      "\n",
      "idev -p skx-normal -n 2 -n 96 -a tg-youraccounthere -t 00:09:59\n",
      "ibrun python test.py\n",
      "\n",
      "# begin test.py\n",
      "import numpy as np\n",
      "import yt; yt.enable_parallelism()\n",
      "\n",
      "storage = {}\n",
      "num = 1250\n",
      "arr1 = np.zeros(num)\n",
      "arr2 = np.zeros(num)\n",
      "arr3 = np.zeros(num)\n",
      "arr4 = np.zeros(num)\n",
      "vals = list(range(int(num)))\n",
      "for sto,val in yt.parallel_objects(vals, storage=storage):\n",
      "  print(val) # this gives values from 0 to 1249, out of order since parallel but increasing every 96\n",
      "  sto.result_id = val\n",
      "  sto.result = {}\n",
      "  sto.result[\"arr1\"] = val+1\n",
      "  sto.result[\"arr2\"] = val+2\n",
      "  sto.result[\"arr3\"] = val+3\n",
      "  sto.result[\"arr4\"] = val+4\n",
      "# end for\n",
      "print(\"done\") # this never executes\n",
      "***i posted this in general first (oops!) but am deleting it from there now. sorry!\n",
      "<@u01346q15dy> has joined the channel\n",
      "hi mihir, yes, but at the moment it has to be done in a slightly different way, i.e., by calling the rockstarhalofinder directly with a time-series object. here is a sample script that should get you going.\n",
      "<http://paste.yt-project.org/show/135/>\n",
      "we need to add this to the docs.\n",
      "thanks britton. i knew this way of doing it. i was thinking of a situation where i didn't have the initial parameter file for some reason, but i realized that i can also use a snapshot file in place of the initial parameter file to load the time series. thanks again!\n",
      "you’re welcome. you can also specify a time-series with a list of snapshots, too.\n",
      "hi folks, quick question.  i’m trying to extract data from a yt.phaseplot, and can’t figure out how to do it.  so, for example, if i say something like this:\n",
      "\n",
      "```\n",
      "dens, cen = ds.find_max(\"density\")\n",
      "sphere = ds.sphere(cen,(300,'kpc'))\n",
      "phase = yt.phaseplot(sphere,'density','temperature','cell_mass')\n",
      "```\n",
      "\n",
      "and then want to extract the relevant information from the `phase` object (the actual 2d array values, as well as limits for x and y axes), how would i go about doing that?\n",
      "it’s relatively easy to do that for a slice or projection, but i can’t find an analogous process for a phase plot\n",
      "the big-picture reason i’m trying to do this is because one of my students is trying to make a multi-panel movie that includes slices, projections, and phase diagrams all together, and it seems easiest to try to extract them all to numpy arrays and then recombine them in a multi-panel plot\n",
      "you mean creating a file with just cosmologyoutputredshift values specified? i tried that but it fails saying that it couldn't find topgriddimensions. but it works with using the output of the earliest snapshot anyway, so not a problem.\n",
      "<@u1dlem6kw> i think <https://yt-project.org/doc/analyzing/generating_processed_data.html#profiles-and-histograms> covers how to get it out, including the mask as well as the other statistical quantities that come with it\n",
      "aha\n",
      "ok, this should do the trick for me.  thank you!\n",
      "hello! i am trying to get a fixed resolution buffer out of an off axis projection plot. how do i set the resolution of the frb?\n",
      "this is what i tried:\n",
      "pp=yt.offaxisprojectionplot(ds,[1,0,0], \"density\", weight_field=\"density\",center=center,north_vector=[0,0,-1])\n",
      "pp.set_buff_size(100)\n",
      "niba=pp.frb\n",
      "but the shape of this thing is still 800 by 800.\n",
      "also, there is off_axis_projection and offaxisprojectionplot. which one should i use?\n",
      "thank you!\n",
      "hi <@ud2365cuv>,\n",
      "\n",
      "this can be done with yt.off_axis_projection.\n",
      "for instance to make an 100x100 image you could do\n",
      "\n",
      "`image = yt.off_axis_projection(ds, center=[0.5,0.5,0.5], normal_vector=[1,1,1], resolution=100, item='density', weight='density')`\n",
      "\n",
      "the “resolution” parameter can be a single int or list of ints for a different number of pixels in each dimension. the “item” parameter is used to specify the field. note, this returns an imagearray, not an frb object but i think the effect probably the same. the imagearray is what you’d get if you did `frb.data['density']` with a true frb object.\n",
      "thanks, corey! it turned out that there was a bug with offaxisprojectionplot, so yes, i am using off_axis_projection now.\n",
      "hi again! would you mind surrounding your code snippet by triple backticks instead of single ones (i.e. ` ` `, without the spaces)? that would make the code much easier to read!\n",
      "\n",
      "also, would you mind sharing an image of what you get out of your code (and what you expect?)\n",
      "i am sorry, what do you mean with ` ` `, please? i used ctrl+shift+c for the code. or is it better to share it in <https://paste.ofcode.org/hyvqb2ae2shmuhbkgzstss|paste code>? i got from me code these outputs:\n",
      "the streamlines are not in correspondence to the vector field.\n",
      "<@ul6nbarlj> has joined the channel\n",
      "<@ubgr3s1ff> has joined the channel\n",
      "<@u042j5bn6> has joined the channel\n",
      "if it’s counts in bins you want, then i think you need to make the profiled field `ones` with `weight_field=none`\n",
      "oooh\n",
      "<@ueq7zhb0c> oh no, i also thought we had support for that!\n",
      "well the thing is, in my script i have some commands that also load the dataset (halo) that i want to analyse.  so since i don't know what to tell you exactly (in order for you to help me) i uploaded a tar with all the scripts i am using to do the analysis. <http://use.yt/upload/aed7f64d> that's the link. at make_the_catalog.py i load the data and create the catalog, whereas in example.py i calculate the radial velocities of the sub_halos in order to make radial profiles. the latter is not done with yt because i couldn't understand how to transform the positions to a meaningful center and insert the hubble flow (the simulation ds has only dm particles and is at z=0).\n",
      "i hope that those scripts will work\n",
      "can do - i'll submit a bug report. thanks!\n",
      "hi <@uelt6g0f6>, what’s the order that i would run these? `make_the_catalog.py` first? is there one script that demonstrates the problem you’re seeing?\n",
      "if you could send some explicit instructions for the order in which things are run, that would be great.\n",
      "<@uj7v15sfk> if you want to apply the patch i sent you, save it into a file then do:\n",
      "```\n",
      "git checkout master\n",
      "git patch /path/to/the/patch\n",
      "pip install -e .\n",
      "```\n",
      "commenting out `mass_sph` in `yt/frontends/ramses/data_structures.py` is not enough\n",
      "ds.field_list has: `('all', 'nlc_x'), ('all', 'nlc_y')`. just using `slc.annotate_quiver('nlc_x','nlc_y', factor=16,plot_args={\"color\": \"purple\"})` gives the error: `ytplotcallbackerror: annotate_quiver callback failed with the following error: buffer has wrong number of dimensions (expected 1, got 2)`\n",
      "it sounds like these fields are getting detected as particle fields for some reason\n",
      "what type should `velocity_x` and `velocity_y` be? for example here: <http://yt-project.org/doc/visualizing/callbacks.html#annotate_quiver>\n",
      "<@ufagql7u0> they should be mesh fields for that annotation\n",
      "so in that example we're working with a enzo amr simulation, which is very different from your exodus ii data\n",
      "i think the field type of your fields should be e.g. `'connect1'`, i'd need to have a copy of it to work with to understand what's going wrong here\n",
      "it's probably not specific to anything about your data, more our support for exodus ii data in general, it's possible you could trigger this with one of the test datasets on <http://yt-project.org/data|yt-project.org/data>\n",
      "in which case you wouldn't need to share your data if that's an issue\n",
      "because yt works with real-world research data it's sometimes very difficult to understand what's going wrong without a dataset to trigger issues with and test with\n",
      "thanks. my `ds.field_list` gives some variables that are `'connect1'` and some that are `'all'`. i have no problem sharing the exodus file. where can i share/upload?\n",
      "`yt upload my_file.tar.gz`\n",
      "that'll print out a url\n",
      "please file an issue on github: <https://github.com/yt-project/yt/issues/new>\n",
      "there are instructions in the issue template\n",
      "hi, i am trying to select a subset of halos in ytree and access their properties at once. after loading a merger tree, i select a subset of halos as\n",
      "```halos = a.select_halos('tree[\"tree\", \"pid\"] == -1')```\n",
      "where `halos` is an array of treenodes satisfying the given criterion. is there any way to access fields for these selected halos without looping over them? `halos['m200c']`, for example, doesn't seem to work. i know i can loop over selected halos and save fields of interest in a list but was wondering if there is a simpler/in-built way of doing this.\n",
      "<@u042s6y2g> ^\n",
      "(he's in europe and may be asleep)\n",
      "opened an issue here: <https://github.com/yt-project/yt/issues/2257>\n",
      "i'm having trouble with gizmo data. it loads just fine but when i try to call any functions with the data object i get an error message. the error message when using ds.field_list is \"oserror: unable to open file (unable to open file: name = '/users/michaeljennings/desktop/school/research/snapshot_160.0.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)\n",
      "\"\n",
      "<@uac394vdl> has joined the channel\n",
      "hi everyone, i am bhavesh, a student at georgia tech. i have recently started using yt to visualize some of my simulations results. first of all, thanks to all the developers of yt for creating this amazing toolkit.\n",
      "i had a small question about adding plots to yt. i wanted to extract the grid structure from h5 file and create a scatter plot of two numpy arrays over this grid structure. i see that there is a feature of annotate_grids in yt, but i always need to use some object like slice plot in order to visualize the grid. is there a way to visualize just the grid structure? and, is there a way to add a scatter plot of two numy arrays over it?\n",
      "<@u042fh0rb> yep, i can do that and submit a bug report if i still see it. in the meantime, i wrote a script to compute grid level based on simulation data which seems good enough at the moment.\n",
      "while installing yt on pycharm by conda environment i faced a situation while doing the statement bash install_script.sh the error was bash is not recognised as internal or external command, operable program or batch file. will someone please tell me how to proceed with this situation\n",
      "hi atharva, can i suggest instead of the install script, if you already have conda installed, you can do one of the following:\n",
      "```pip install yt```\n",
      "or clone the source with git and install:\n",
      "```git clone <https://github.com/yt-project/yt>\n",
      "cd yt\n",
      "pip install -e .```\n",
      "i got it fixed...i just had to update my git bash...thank you for your instant reply!\n",
      "excellent!\n",
      "regarding this: <https://yt-project.slack.com/archives/c042f73sr/p1572971208007400>, i was able to render multiple regions in unstructured mesh data using `add_source` function in the `meshsource` class. actually, it was documented ‘unstructured mesh rendering’ <https://yt-project.org/doc/visualizing/unstructured_mesh_rendering.html>. however, it must be necessary to explicitly specify the same color map because each mesh source is independent. now, i am wondering if i can create a color bar for rough idea of values. i think that for volume rendering, the color doesn’t precisely represent the value, but it would be helpful for me to know where high or low values in initial analysis.\n",
      "i think you can use any matplotlib colorbar you want in the volume rendering\n",
      "<https://matplotlib.org/3.1.1/tutorials/colors/colormap-manipulation.html>\n",
      "also, great :slightly_smiling_face:\n",
      "if there are places where our docs are lacking for this or if we can make the `notimplementederror` you initially got point to doing the correct thing, pull requests are very welcome :slightly_smiling_face:\n",
      "no pressure though\n",
      "i just want you to feel empowered to poke around and maybe alter or hack the yt codebase\n",
      "thank you for the suggestion. i will try to contribute the project when i have something i can do.\n",
      "<@u042s6y2g> has joined the channel\n",
      "<@u34g2m0ku> has joined the channel\n",
      "<@uardxrmdm> has joined the channel\n",
      "is there any way to generate a rendering/images (multi-band images) of galaxies in the ramses box (i have the halo coordinates) similar to the way pybody does with <https://pynbody.github.io/pynbody/tutorials/pictures.html#multi-band-images-of-stars> ??\n",
      "this uses the star particles…\n",
      "no, we don’t have anything like that in yt\n",
      "ok, thx.\n",
      "didn’t see it in the docs, but wanted to be sure :slightly_smiling_face:\n",
      "we used to have a way to export data to sunrise\n",
      "but unfortunately it bitrot :confused:\n",
      "i’m also not sure if sunrise is still a going concern\n",
      "sunrise??\n",
      "(i guess that’s another package)\n",
      "<https://bitbucket.org/lutorm/sunrise/src/master/>\n",
      "it is\n",
      "the original author left the field 7 or 8 years ago\n",
      "ah yeah, chris hayward took it over\n",
      "but it looks like it hasn’t been touched since 2015\n",
      "yea, just checked it out … 2015 :slightly_smiling_face:\n",
      "btw, when i say “it” bitrot, i mean the sunrise exporter in yt\n",
      "not sunrise itself\n",
      "one more question…\n",
      "in principle someone could update it, i have no idea how hard that would be\n",
      "to my knowledge, sunrise is still operational, but in stasis.\n",
      "<@u042fh0rb> set the channel topic: general help with yt. please mute this channel if you feel it’s too noisy.\n",
      "if i want to do a vol render on just a small fraction of my volume, say 20 kpc around a halo location, how do i specify that with \n",
      "```sc = yt.create_scene(ds, lens_type='perspective')                                                                                                                                                                                                                                                                           \n",
      "sc.camera.set_width(ds.quan(halo_size, 'kpc'))                                                                                                                                                                                                                                                                              ```\n",
      "you’d want to set the camera’s focus at the halo’s position\n",
      "but wait - you’re using ramses, right?\n",
      "yep.\n",
      "i thought the volume renderer didn’t work with ramses data at all\n",
      "oh… (good to know, was just starting to play with it this afternoon)\n",
      "yeah, the current blocker on that is generating vertex centered data\n",
      "for octree data\n",
      "ok… that explains this error then:\n",
      "```    sc = yt.create_scene(ds, lens_type='perspective')\n",
      "  file \"/home1/02744/earnric/.local/lib/python2.7/site-packages/yt/visualization/volume_rendering/volume_rendering.py\", line 70, in create_scene\n",
      "```\n",
      "thx! saved me a bunch of time.\n",
      "what error?\n",
      "oops… didn’t cut enough:\n",
      "```    sc = yt.create_scene(ds, lens_type='perspective')\n",
      "  file \"/home1/02744/earnric/.local/lib/python2.7/site-packages/yt/visualization/volume_rendering/volume_rendering.py\", line 70, in create_scene\n",
      "    if field not in data_source.ds.derived_field_list:\n",
      "```\n",
      "heh, still not enough :slightly_smiling_face:\n",
      "hehe… yea, i was running on 12 cpus, so the errors are a bit ‘jumbled’ i think… want me to post a loooong cutout?\n",
      "you can use <http://gist.github.com|gist.github.com>\n",
      "```  file \"./rendergalaxy.py\", line 56, in &lt;module&gt;\n",
      "    sc = yt.create_scene(ds, lens_type='perspective')\n",
      "  file \"/home1/02744/earnric/.local/lib/python2.7/site-packages/yt/visualization/volume_rendering/volume_rendering.py\", line 70, in create_scene\n",
      "    if field not in data_source.ds.derived_field_list:\n",
      "  file \"/home1/02744/earnric/.local/lib/python2.7/site-packages/yt/data_objects/static_output.py\", line 213, in ireq\n",
      "  file \"./rendergalaxy.py\", line 56, in &lt;module&gt;\n",
      "    sc = yt.create_scene(ds, lens_type='perspective')\n",
      "  file \"/home1/02744/earnric/.local/lib/python2.7/site-packages/yt/visualization/volume_rendering/volume_rendering.py\", line 70, in create_scene\n",
      "    if field not in data_source.ds.derived_field_list:\n",
      "  file \"/home1/02744/earnric/.local/lib/python2.7/site-packages/yt/data_objects/static_output.py\", line 213, in ireq\n",
      "    self.index\n",
      "  file \"/home1/02744/earnric/.local/lib/python2.7/site-packages/yt/data_objects/static_output.py\", line 504, in index\n",
      "    self, dataset_type=self.dataset_type)\n",
      "  file \"/home1/02744/earnric/.local/lib/python2.7/site-packages/yt/frontends/ramses/data_structures.py\", line 357, in __init__\n",
      "    self.index\n",
      "  file \"/home1/02744/earnric/.local/lib/python2.7/site-packages/yt/data_objects/static_output.py\", line 504, in index\n",
      "    self, dataset_type=self.dataset_type)\n",
      "```\n",
      "how’s that?\n",
      "i still don’t see the actual error anywhere, it looks like it’s dying when it tries to create the index?\n",
      "i don’t think that has anything to do with volume rendering\n",
      "all i can tell is its at the line:  `sc = yt.create_scene(ds, lens_type='perspective')  `\n",
      "that’s line 56 …\n",
      "right, but inside of that something is doing `ds.index`, which is crashing\n",
      "which is a bit surprising to me\n",
      "does the same script work on one core?\n",
      "this could be a bug in yt’s parallelism\n",
      "i can try…\n",
      "but it’s hard to say without the actual error :slightly_smiling_face:\n",
      "(one core)\n",
      "here’s everything from the parallel run: <https://gist.github.com/earnric/2f8e74c04e2e3947a81cdd3833e92a76>\n",
      "\n",
      "ah, the actual error is a `memoryerror`\n",
      "you ran out of memory\n",
      "probably because yt was trying to construct the octree repeatedly on each core\n",
      "i don’t think octree construction is parallelized\n",
      "hum… ok. but it won’t work for ramses (create_scene) anyway right?\n",
      "yeah, you’d hit a different error later\n",
      "but this is a general problem with trying to run yt in parallel like you’re trying to do\n",
      "not everything in yt is parallelized, so just running yt in parallel won’t necessarily get you any speedup\n",
      "in fact here, i bet it’s slower\n",
      "especially if there’s i/o contention reading the data from disk\n",
      "ok … i’m running single threaded now, its still going. you want me to post the error i eventually get?\n",
      "<@u2xtxd8s0> has joined the channel\n",
      "no, that’s ok\n",
      "and you don’t need to run single threaded either\n",
      "i now understand why it died :slightly_smiling_face:\n",
      "and i’m not worried\n",
      "btw\n",
      "corentin cadiou has been working recently on making constructing the octree *a lot* faster\n",
      "like 20-30x faster\n",
      "if you switch the development version of yt you might get some nice speedups\n",
      "this will be in yt 3.5\n",
      "ok great news!\n",
      "i’ll pull the current dev version now.\n",
      "thx!\n",
      "i have `finished processing dependencies for yt==3.5.dev0`\n",
      "<@u7ev2lfat> has joined the channel\n",
      "i’ve been looking for an easy way to force the center color of a diverging colormap to map a conceptually meaningful value. for instance here, when plotting signed velocities, not mapping white to zero make the figure somewhat misleading.\n",
      "in matplotlib, this can be achieved with e.g\n",
      "```import matplotlib.pyplot as plt\n",
      "from matplotlib.colors import divergingnorm\n",
      "plt.contourf(x, y, z, norm=divergingnorm(vcenter=0.0), cmap=\"rwb\")```\n",
      "but i’m falling to grasp how that would be passed down to yt. is the api there ? any information on this would help :slightly_smiling_face:\n",
      "i usually set zlim to +/- of max(|minval|, |maxval|)\n",
      "i think at one point we talked about having a way of indicating whether a field can have a zero point and then automagically choosing a diverging colormap with the diverging point set to the zero value\n",
      "i think right now though you’d need to poke at the matplotlib figure that your sliceplot wraps\n",
      "if you wanted to have full control\n",
      "alright, thanks guys. if that discussion is ever revived i’m interested in contributing. meanwhile i’ll go by <@u042f73r7>’s solution !\n",
      "<@uppavuhsm> has joined the channel\n",
      "guys i don't understand how to download yt on my macbook - the yt --help thing keeps returning -bash: yt command not found\n",
      "i'm using terminal\n",
      "i know this is a pretty basic question\n",
      "hi! how did you install yt in the first place?\n",
      "i just typed in pip install yt --user onto terminal\n",
      "(maybe you already found that page, but if not <https://yt-project.org/doc/installing.html> should help you !)\n",
      "i am a h u g e beginner this is for my university project\n",
      "no worries, we all started as beginners at some point :wink:\n",
      ":slightly_smiling_face:\n",
      "what does `python -c \"import yt; print(yt.__version__)\"` returns?\n",
      "3.5.1\n",
      "great! so you got it installed, it is just missing from your path\n",
      "ok what does that mean because i read it everywhere\n",
      "let me answer in a thread to prevent clogging the channel\n",
      "ok sounds good\n",
      "when you're typing commands in a terminal, they are looked for in a set of different directories\n",
      "ok\n",
      "the list of directories that will be used to search commands is stored in the `path` variable. try `echo $path` to see its content\n",
      "whatever is in one of these directories you can execute. in your case, you installed `yt` but the script is not in any of the directories of your `path`\n",
      "is this what i was supposed to do..\n",
      "no, echo $path\n",
      "it's case-sensitive :slightly_smiling_face:\n",
      "\"which pip\" might also be helpful\n",
      "\n",
      "what's the output of \"which pip\"?\n",
      "also you can copy/paste this stuff, you don't need to uplaod screenshots :slightly_smiling_face:\n",
      "oh right lol\n",
      "\n",
      "last one\n",
      "and how did you install python in the first place?\n",
      "or is it the version of python that shipped with the os?\n",
      "i think all i did was install pip\n",
      "and then i think python came with it?\n",
      "(rly not sure)\n",
      "ok, so the place that pip puts executable binaries isn't in your $path, and it needs to be\n",
      "i'm just not sure where that is on your system\n",
      "can you do \"pip uninstall yt\"\n",
      "then \"pip install yt\"\n",
      "done\n",
      "and copy/paste all of the output from that command into <http://gist.github.com|gist.github.com>\n",
      "and then share the gist here\n",
      "error: could not install packages due to an environmenterror: [errno 13] permission denied: '/library/python/2.7/site-packages/yt'\n",
      "consider using the `--user` option or check the permissions.\n",
      "oh ok\n",
      "i bet you did --user\n",
      "so i got this the first time i tried to install anything and then i did the user option\n",
      "yep\n",
      "and now i'm confused\n",
      "so you need to add ~/.local/bin to your $path\n",
      "how do i do this?\n",
      "(sorry)\n",
      "what's the output of `echo $shell`?\n",
      "\" /bin/bash \"\n",
      "if i may, i would suggest using anaconda since the python version kriti is using seems to be 2.7 (which will not be supported soon)\n",
      "yeah, i would as well\n",
      "yeah it keeps saying that on my pip\n",
      "is it possible to use yt through the online jupyter notebook\n",
      "?\n",
      "yes, but you'll still need to update your $path if you want it to work with your current python\n",
      "do you know how to use a text editor?\n",
      "you'll need to open your `~/.bashrc` file, and then add a line that looks like this to the bottom:\n",
      "```\n",
      "export path = $home/.local/bin:$path\n",
      "```\n",
      "i sort of know how to use a text editor\n",
      "i'd also strongly encourage you to go through the software carpentry lessons on the unix shell\n",
      "<http://swcarpentry.github.io/shell-novice/>\n",
      "another way to do this would be to install the anaconda python distribution\n",
      "the installation process for that will update your path for you\n",
      "ok\n",
      "is that just another pip install\n",
      "no, it's a separate python distribution, you can download the installer here:\n",
      "<https://www.anaconda.com/distribution/#download-section>\n",
      "i'd suggest installing the python 3.7 version\n",
      "ok awesome\n",
      "am reading this unix shell thing now\n",
      "once you have that installed you can get the latest version of yt by doing `conda install -c conda-forge yt`\n",
      "yes, going through that lesson and the python lesson would be a very good idea for you given your level of experience\n",
      "maybe the git lesson too\n",
      "good luck!\n",
      "thank you so much!\n",
      "there is more than one python lesson published on the carpentries pages. i have heard that the gapminder lesson is generally a bit more approachable than the inflammation lesson (if you plan on checking it out).\n",
      "hello yt world.\n",
      "i’m reading in a dm-only enzo output that gives the following error:\n",
      "\n",
      "note that the task i’m trying to do is very ordinary, boiled down to eliminate any extraneous function calls.\n",
      "the issue occurs at the step within grid_geometry_handler.py where the list of grid names “enzogrid_????” is sorted.\n",
      "from examining that list i’m not able to see anything wrong with it, and my installation of yt successfully opens a lot of other outputs that i’ve created over the years. here is the output in question: <https://www.dropbox.com/s/xa42dg9xolo3h5r/dd0064.tar.gz?dl=0>\n",
      "i think <@u042s6y2g> ran into and fixed this a little bit ago. which version of yt are you on <@uas3yas4v>?\n",
      "3.4.1 - perhaps fixed in 3.5?\n",
      "yeah i think this is <https://github.com/yt-project/yt/pull/1912>, that said it looks like the fix didn’t make it into 3.5.0\n",
      "we’re pretty overdue for a 3.5.1, i should probably get that set up…\n",
      "but for now you can run yt from the master branch (<http://yt-project.org/doc/installing.html#installing-yt-from-source>) to load these data\n",
      "oh wait, that might be wrong, the fix that was actually accepted was <https://github.com/yt-project/yt/pull/1919>\n",
      "ah yeah, and that *did* make it into 3.5.0, so yeah updating to the latest yt release should fix your issue\n",
      "in general with any yt issue, updating and seeing if the latest version has a fix is always worth a try :slightly_smiling_face:\n",
      "“corner case” is a polite way of saying “wtf are you doing” but i am glad to report that the “corner case” i’m using was ultimately derived from <@u042s6y2g>’s own ic scripts.\n",
      "and i just did a fresh update to 3.5 on my other machine where i install from conda - and it’s fixed.\n",
      "so all good.\n",
      "<@uas3yas4v>, the situation where this pops up is a rather strange one. it’s a situation of a dark matter only simulation in which there is an amr grid with no particles. in this case, the grid object doesn’t get written to disk at all because there is no gas information to save. it’s only when i was doing the runs like you’re doing now that i ever encountered this, hence why it only recently got fixed.\n",
      "hey ! is there a way to do a deposition onto particles, e.g. create a new particle field that would contain the density of the cell containing the particle (in the case of an amr dataset)?\n",
      "i know i can always find the value in the cell using `ds.r[position][field]`, but it is extremely slow for a large number of particles\n",
      "hm, yeah that's probably what i would have done as well to create a new particle dataset. there's likely  a better way i'm unaware of\n",
      "<https://github.com/yt-project/yt/blob/master/yt/geometry/particle_deposit.pyx#l431> this almost does what you want, i think you’d need to write a new deposit operation\n",
      "thanks! i'm just a bit lost about the signature of the `process` function\n",
      "is there a way to access the data stored in the amr structure from there?\n",
      "no, for ramses data i think it would be processed oct by oct\n",
      "so you only get information about the oct you’re in\n",
      "ok. but then is there a way to quickly get, say the density of each cell in the oct?\n",
      "nope, it looks like `domain_ind` is specifically hacked in there for the `meshidentifier` deposit operation. i think you’d need to modify the signature of `process` to accept an np.float64_t[:, :] that can optionally hold field values.\n",
      "is there somewhere i could read to know to get the field values from e.g. <https://github.com/yt-project/yt/blob/master/yt/geometry/particle_deposit.pyx#l104>?\n",
      "you mean you want to know where `process_octree` is getting called from?\n",
      "i'm just a bit lost in all the yt internal machinery.\n",
      "if i understand correctly, the correct way to deposit field values onto the particles is to modify `process_octree` so that is passes the relevant field value to the `process` function (that is overloaded for each subclass of `particledepositoperation`).\n",
      "however i don't know how to actually get the field values\n",
      "yeah, there’s definitely a couple levels of indirection here\n",
      "i'll take a look\n",
      "to get an idea of how the flow control works, you can take a look at the derived fields defined in `particle_deposition_functions` in `yt.fields.particle_fiels`\n",
      "then look at the `deposit` method of both `amrgridpatch` and `octreesubset`\n",
      "`process_octree` gets called in `octreesubset.deposit`\n",
      "`octreesubset` acts like a yt data object, you can get field values from it\n",
      "the only thing is you’re also going to need to get this working for block amr data, which shouldn’t be too much harder\n",
      "if you want to upstream it anyway, which would be very much appreciated, it sounds like useful functionality\n",
      "i need to drive to work so i’ll be a little while before i can respond, hope that provides some context\n",
      "hi folks! i'm trying to do a projection through a large, unigrid dataset (4096^3), and running into out-of-memory issues.\n",
      "this dataset has one, single precision variable in it, so the data takes up ~275 gb.\n",
      "i'm running it on 16 mpi tasks, 1 task per node, and each node has 128 gb of ram\n",
      "so 2 tb total memory to work with\n",
      "it runs out of memory during this part of the projection:\n",
      "\n",
      "in the `get_data` method of `ytquadtreeproj`\n",
      "turning the log level to `10`, the memory usage reported is 68 gigs per task, which should fit\n",
      "\n",
      "i verified that all the task report less than 70 gb\n",
      "i think (?) that this is memory that the python garbage collector thinks is in use, and wouldn't include any mallocing that might happen at the cython level\n",
      "if i use the `sstat` tool on cori that shows the max memory used per task, it does show that the usage goes up to over 128 gb on task 1\n",
      "any advice on how to debug this?\n",
      "even if the data is getting promoted to double internally, that seems like a lot of memory usage\n",
      "could be a memory leak\n",
      "how much memory does the python garbage collector say it’s tracking?\n",
      "it looks to me like all the tasks have ~70 gb\n",
      "how are you measuring that?\n",
      "`get_memory_usage()/1024.`\n",
      "which does this:\n",
      "sorry, where does that come from?\n",
      "\n",
      "yt/funcs.py\n",
      "ah ok, so that is total memory used according to the os\n",
      "but you said one task exceeds that?\n",
      "the outcome of `get_memory_usage` gets reported to the debug log every time a chunk is added to the tree. in those logs, i never see any usage go over 70 gb\n",
      "i see, so something transient might be happening\n",
      "so definitely the data are getting promoted to double precision\n",
      "yt doesn’t keep data in single precision like you seem to be expecting (it would be nice if it could but it doesn’t)\n",
      "i would offhand think that this operation shouldn’t need a huge amount of ram since it’s a loop over chunks\n",
      "and each chunk is independent\n",
      "do you have any idea where the memory usage is going?\n",
      "running under tracemalloc might be instructive\n",
      "you can also use a tool like objgraph to find what python objects use the most memory, although that won’t see memory that’s allocated in c\n",
      "<@u042hlt7u> or <@u042s6y2g> might have other ideas\n",
      "how many grids in the dataset?\n",
      "or is it one huge grid?\n",
      "the grids are 128^3\n",
      "in principle i'd think the operation could be done with ~ 128*4096*4096*8 bytes = 17 gb total\n",
      "clearly it’s not :slightly_smiling_face:\n",
      "unfortunately these sorts of issues are often hard to debug in python\n",
      "a quick and dirty hack would be to add a call to `gc.collect()` in the problematic loop\n",
      "i doubt that will fix anything but it’s a quick thing to check\n",
      "it would be interesting to see what objgraph says about the state of the heap inside that loop\n",
      "it could be that we’re caching something for speed and that’s causing issues here\n",
      "i don’t have any concrete suggestions unfortunately, i don’t know enough about the details of how the projection works at the c level to be helpful, that’s more a matt question\n",
      "i guess try again with more ram if you don’t have the appetite to dive into debugging this\n",
      "that's an option\n",
      "<@u043bna00> it should be able to be, but right now an issue is that it's caching the masks\n",
      "i would guess that is currently theb iggest and most problematic issue\n",
      "masks, ires, icoords\n",
      "one thing it that the the memory usage reported by `get_memory_usage` and that reported by the `sstat` tool, which is a slurm thing, don't correspond\n",
      "<@u042hlt7u> thanks for chiming in - those are per cell quantities on each grid?\n",
      "yup\n",
      "you could try commenting out the decorators that cache them and see if that helps\n",
      "grep for `@cached_property`\n",
      "dunno offhand if everything will explode if you turn off those caches\n",
      "<@u042fh0rb> thanks, i'll try that\n",
      "more specifically, instead of commenting it out, replace it with `@property`\n",
      "it looks to me like a large spike in memory usage happens here:\n",
      "\n",
      "i.e., in between the start and the end of that function, which gets called on each proc, the memory usage jumps from 4 gb to 82 gb (max over all procs)\n",
      "so this is grid metadata?\n",
      "ah yeah, i bet it’s ires, icoords, and masks\n",
      "`select_grids` in particular needs those i believe\n",
      "this is with the `cached_property` decorator changed fwiw\n",
      "does this actually need to be done in the case that the dobj is all_data?\n",
      "i think not for this dataset? unless there are psuedo-grids that are fully masked\n",
      "but if all the grids are unmasked then no, we don’t need to do this\n",
      "ah ok, so it’s not the caches i was worried about then, it’s something else\n",
      "it would be nice to find out where the bulk of the allocations are happening\n",
      "tracemalloc might tell you that\n",
      "it is doing it, but it should not have to, especially for a unigrid datasdet\n",
      "<@u44sw7j11> and i are in my office talking about this use case right now, and where it fails\n",
      "hey britton, i think you have the right idea of what i’m trying to do, but i don’t think that you should commit development time to it at this point.  i’ll experiment around with a few things and get back to you if i need more help.  thanks!\n",
      "the problem turned out to be that i added units _before_ using `apply_along_axis` , which meant that the field produced a `ytquantity` instead of a `ytarray` .  the error went away when i simply added the units by multiplying the symbolic units to the output of `apply_along_axis`\n",
      "well done ! thanks for the feedback\n",
      "hi folks, quick question - if i generate a sphere data object in a cosmological simulation (e.g., around a galaxy) and then make a phase plot of radial velocities, are those radial velocities calculated using the raw velocity values in the simulation, or is the mean velocity of the sphere (weighted by baryon or dark matter mass in some way, perhaps) used instead?\n",
      "i very much hope it’s the latter, but one of my students is seeing some odd behavior that makes me suspect the former  :confused:\n",
      "hi <@u1dlem6kw>, you have to set the `bulk_velocity` field parameter by hand, otherwise it’s 0 by default\n",
      "e.g.\n",
      "```\n",
      "sp = ds.sphere(c, radius)\n",
      "vx = sp.mean(\"velocity_x\", weight=\"density\")\n",
      "vy = sp.mean(\"velocity_y\", weight=\"density\")\n",
      "vz = sp.mean(\"velocity_z\", weight=\"density\")\n",
      "sp.set_field_parameter(\"bulk_velocity\", [vx, vy, vz])\n",
      "```\n",
      "oops\n",
      "let me edit\n",
      "aha\n",
      "that is extremely useful, thank you!\n",
      "shouldn’t the results of `ad['h_p1_fraction']` be the same as `ad['h_p1_density'] / ad['h_density']`?\n",
      "this is what i expected, but i am finding this to not be true.\n",
      "i *think* based on my reading of ytep 3, this should also be true. <https://ytep.readthedocs.io/en/latest/yteps/ytep-0003.html>\n",
      "i’m looking through the code to see what is going on, but not sure what is up.\n",
      "isn't h p 1 plus 1?\n",
      "yeah.\n",
      "sorry, misread\n",
      "you weren't asking that\n",
      "i’m looking for the mass fraction of h ii\n",
      "i thought it was over the total density\n",
      "rather than neutral\n",
      "oh you’re right.\n",
      "in this instance, i’m misspeaking\n",
      "but i get the same results when i do this with nuclei density\n",
      "one sec.\n",
      "`ad['h_p1_fraction']` != `ad['h_p1_number_density'] / ad['h_nuclei_density']`\n",
      "`ad['h_p1_fraction'] = ytarray([ 0.75994256,  0.75994258,  0.75994255, ...,  0.35083768, 0.27859338,  0.03780791]) (dimensionless)`\n",
      "`ad['h_p1_number_density'] / ad['h_nuclei_density'] = ytarray([ 0.99999891,  0.99999891,  0.99999891, ...,  0.46166219, 0.36664719,  0.04998957]) (dimensionless)`\n",
      "again, it seems like these should be the same.\n",
      "but maybe i’m just missing something.\n",
      "it’s the same if i replace `ad['h_nuclei_density']` with the sum of the `h_p0_` and `h_p1_` number densities.\n",
      "rather, the same mismatch occurs if i swap the nuclei density with the sum of the two species number densities\n",
      "there's helium in your sim, right?\n",
      "yup.\n",
      "i”m just using `isolatedgalaxy` to test this out.\n",
      "i guess i need you to tell me what the numbers should be?\n",
      "because they kind of match up with my expectations\n",
      "my issue isn’t so much that 0.75, … is better than 0.99, …\n",
      "it’s that those two calculations *should* be giving the *same* values.\n",
      "i think.\n",
      "one is over total, the other over just h\n",
      "oh!\n",
      "so you’re saying the `h_p1_fraction` is the total h ii out of the entire density field??\n",
      "oh, i guess i naively would have thought that h_p1_fraction was just the ionization fraction of the gas…i.e. the h ii out of the hydrogen density\n",
      "that’s how we do it in trident.\n",
      "i hope we aren’t making bad assumptions using that info.\n",
      "hmmmm.\n",
      "but i’ll double check what you’re saying.  that may make sense at explaining these numbers\n",
      "yup, `h_p1_fraction` is mass fraction out of total density.\n",
      "yeah, confirmed.\n",
      "interesting…\n",
      "ok, that resolves my confusion about this.  now i just have to make sure trident isn’t going haywire on that.\n",
      "oh my bad.  we create an `h_p1_ion_fraction` field\n",
      "ok ok.  sorry for the noise.  thanks for the help in understanding this, <@u042hlt7u>!  much appreciated.\n",
      "hooray! everyone is okay!\n",
      ":slightly_smiling_face:\n",
      "ah ok! that's certainly good to know  :smile: i still think you stumbled across some gather/scatter disagreements so still good to do some digging\n",
      "<@u013q575drv> has joined the channel\n",
      "hi all, is add_volume_weighted_smoothed_field only for particles? is there a way to smooth a field for grid-data to make the grid less obvious?\n",
      "there are a couple of ways to remesh grid data, such as `ds.arbitrary_grid`, `ds.covering_grid` or `ds.smoothed_grid`. that latter is probably what you are looking for.\n",
      "<@ulqt4pxbp> oh, weird!  let me check into that.  it may be that we don't special case for jpg, just png and pdf.\n",
      "<@ulqt4pxbp> so we use the `figurecanvasagg` for anything that isn't postscript or pdf.  looks like the routine is just to see if it's asked for png, pdf, eps, ps, and if not, it uses agg, which may or may not do the right thing with jpg.\n",
      "<@u042hlt7u> so should i create a new issue for this?\n",
      "<@ulqt4pxbp> yup\n",
      "<@u042hlt7u> i will do that later today.\n",
      "thanks!\n",
      "<@u042hlt7u> i created two issues:\n",
      "<https://github.com/yt-project/yt/issues/2309>\n",
      "and\n",
      "<https://github.com/yt-project/yt/issues/2310>\n",
      "thanks!\n",
      "sure, seems fine, prs welcome :slightly_smiling_face:\n",
      "is the \"cic\" or \"nearest\" deposition method for particle fields working correctly? it appears to be amplifying all my velocities by a factor of 10^4?\n",
      "<@uppavuhsm> interesting.  is it exactly 10^4?\n",
      "sounds suspiciously like a units bug\n",
      "if you can make a short script demonstrating the issue, preferably using one of the public datasets on <http://yt-project.org/data|yt-project.org/data>, that would clarify things\n",
      "also what yt version is this?\n",
      "its hard to tell which values in the \"star\" field correspond to which values in the \"deposit\" field, but when i make a profile plot of (\"deposit\", \"star_cic_velocity_cylindrical_theta\") (for example), the velocities come out on the order of 10^4 higher than what i'm expecting them to be\n",
      "sorry this is so dumb but how do i check the yt version.....? i tried just typing in 'yt version' but it said \"invalid syntax\" on \"version\"\n",
      "`import yt; print(yt.__version__)`\n",
      "oh, this is a profile!\n",
      "i bet you are plotting the sum of the field and not the average\n",
      "what does the `create_profile` call look like?\n",
      "or however you're creating the profile object\n",
      "i guess with `profileplot` maybe?\n",
      "3.5.1\n",
      "e.g. maybe `weight_field` is set to `none`?\n",
      "another way to to check if the actual field values are correct is to make a plot of that field or to print out the raw field values with a data object\n",
      "sorry, make a `sliceplot`, specifically\n",
      "im using profile plot:\n",
      "```ds.add_deposited_particle_field((\"star\", \"particle_velocity_cylindrical_theta\"), method=\"cic\") \n",
      "out: ('deposit', 'star_cic_velocity_cylindrical_theta')\n",
      "check1 = yt.profileplot(sphere, \"radius\", \"star_cic_velocity_cylindrical_theta\", n_bins= 256, weight_field=none, accumulation=false) ```\n",
      "yeah, `weight_field=none` means that the profile is the *sum* of the field in each bin\n",
      "<https://yt-project.org/docs/dev/reference/api/yt.visualization.profile_plotter.html?highlight=profileplot#yt.visualization.profile_plotter.profileplot>\n",
      "if you want an average, than `weight_field='ones'` is what you want\n",
      "and i get this, which really shouldn't be negative but whatever\n",
      "so i tried doing weight_field=\"star_mass\"\n",
      "often for amr data that's not the most sensible thing though, which is why the default weight field is `cell_mass`\n",
      "you could do `weight_field=(\"deposit\", \"star_mass\")`\n",
      "now, i've checked the star_mass profile and it is definitely correct (when i use accumulation then it sums up to exactly the value i have in the catalogs)\n",
      "and this is what happens:\n",
      "\n",
      "which was even more depressing\n",
      "here is the accumulated and non accumulated star mass profiles for the galaxy (using (\"deposit\", \"star_mass\"))\n",
      "what's wrong with that plot?\n",
      "well, here are the star mass profile plots\n",
      "\n",
      "this is non accumulated\n",
      "ok\n",
      "this is accumulated\n",
      "there's really nothing that should be causing the cylindrical velocity to fly up and down like that at the beginning, but even if we just ignore that and look at the rest of the plot more zoomed in, it looks like this\n",
      "\n",
      "this is just chopping off anything before r=1kpc\n",
      "so you're worried about that dip at 2 kpc?\n",
      "the order of magnitude is still too high\n",
      "well, the whole profile looks wrong\n",
      "can you make a `sliceplot` of this field?\n",
      "<@uppavuhsm> what're you weighting it with?  and, if you're doing a profile, you may not need to use the cic deposited fields.  (maybe i missed above why you're doing it, but you can profile particles directly.)\n",
      "yeah, you can directly profile the particle fields too\n",
      "no need to deposit\n",
      "i was weighting it with (\"deposit\", \"star_mass\")\n",
      "e.g. you can profile with `particle_radius` as the x field\n",
      "you guys!!!!!!!!\n",
      "really!!!!!!!!!!!!!!\n",
      "yup\n",
      "oh but will particle radius give me the averaged quantity over radial bins\n",
      "that's what i was wondering about using that\n",
      "i don't quite understand that question\n",
      "sorry, badly worded\n",
      "so im trying to create the profile of the cylindrical velocity of the stars as a function of radius. i would like to compare this to my profiles of the cylindrical velocity of the gas as a function of radius, and the way that yt.create_profile() has done the gas profile is that it has taken a series of radial bins centred on my galaxy, and averaged the cylindrical velocity of the gas in each radial bin.\n",
      "\n",
      "my concern is that creating a profile of the cylindrical velocity of the stars as a function of particle_radius will not create the radial bins as with the gas, and/or will not average the star cylindrical velocity over each radial bin as i have with the gas.\n",
      "it would\n",
      "rather, it will create a plot of each star cylindrical velocity, plotted in line with that stars particle radius\n",
      "it would make bins in `particle_radius`\n",
      "every particle that falls in between 1.1 and 1.2 kpc in its `particle_radius` field would go into one bin\n",
      "say, for example\n",
      "you can think of yt's profiles as a fancy histogram\n",
      "in this case you're making this histogram using `particle_radius` as the bin field\n",
      "from yt's perspective this is no different than using `radius` as the bin field, the actual histogramming process proceeds identically\n",
      "i see.\n",
      "i think i was just confusing myself because i wasn't sure how \"particle_radius\" was organised\n",
      "you can take a look at all these fields just by accessing your `sphere` data object\n",
      "like is each element in that field paired to it's corresponding star properties?\n",
      "e.g. `print(sphere['star', 'particle_radius'])`\n",
      "or `print(sphere['star', 'velocity_cylindrical_theta'])`\n",
      "yes of course, but how would i be able to tell if each radii corresponded to which \"star_mass\" element\n",
      "or whatever property\n",
      "i'm still not quite following what you're asking, sorry\n",
      "sorry it doesn't matter\n",
      "i'll just make the profile with the particle fields, much simpler solution\n",
      "also, for the `cylindrical_velocity_theta` field, keep in mind that field uses two diferent field paramters to define the coordinate system\n",
      "would love to know what was up with the deposition though, as that is what i've spent my last 5 hours on\n",
      "but no worries if not\n",
      "and if you're not using the correct field parameters, you may get junk answers for the field values\n",
      "it may be the field parameter issue i just pointed out\n",
      "e.g. you might have the wrong center or north vector\n",
      "ah so this actually leads me to another question about north vectors\n",
      "but first.. if i do\n",
      "```sphere.set_field_parameter(\"normal\", unit_l) ```\n",
      "where lets say that unit_l is the normalised angular momentum vector that i've extracted from the sphere\n",
      "is that different to \"north vector\"\n",
      "sorry, it's `normal`\n",
      "as i thought this might explain why all my cylindrical velocities are coming out negative\n",
      "north_vector is the wrong term i was misremembering\n",
      "`normal` is the correct name, that should work assiming `unit_l` is correct\n",
      "but definitely double-check that the answers make sense\n",
      "got it\n",
      "thanks for the help\n",
      "also keep in mind that weighting by the deposited star mass might not be the correct thing for your data, e.g. that big dip at 2 kpc might be caused by a single cell that happens to have a lot of massive particles\n",
      "it's easier to reason about this (imo) by looking at slices or raw field values before binning and averaging\n",
      "i see\n",
      "what if i was just doing a profile of the particle fields\n",
      "then it would be ok?\n",
      "that should be less noisy\n",
      "or perhaps \"particle_ones\" is my best bet\n",
      "yeah, definitely compare with \"particle_ones\"\n",
      "cool\n",
      "also happy to help out in the future, but notice how in this discussion we initially thought it was a units bug but it ended up being a usage issue\n",
      "it's much easier to have concrete discussions about code if we all have a code sample we can run locally and think about\n",
      "my guess about it being a units issue was just a guess, and it turned out to be wrong\n",
      "whereas if we had a code example to look at i would have immediately spotted that you were making a profile with weight_field=none\n",
      "it can sometimes be annoying to make a runnable example when asking a question like that\n",
      "but i think it's less annoying in the end for everyone if you try to do that\n",
      "anyway, good luck with your research :slightly_smiling_face:\n",
      "of course! i understand. next time will do :slightly_smiling_face:\n",
      "<@ub934apl1> has joined the channel\n",
      "two (hopefully quick?) questions about getting my datasets to work better with yt\n",
      "- how do i get yt to recognize my vector fields? i'm loading using load_uniform_grid right now, and it doesn't catch any of them besides magnetic_field_x/y/z. i've tried messing around with create_vector_fields and create_vector_fluid_fields, but i think i'm missing a step\n",
      "- is there a way to override the default settings for field labels? ideally i'd like to get ions labelled like \"o+  ..\" instead of \"oii ..\" without relabelling all of my plots by hand\n",
      "for the first one, no not really, i think that would require adding a new keyword argument to load_uniform_grid that tells yt that certain fields are vector fields, i can point you at the low-level code that handles vector fields\n",
      "the reason it works for magnetic fields is because there’s already special handling for velocity and magnetic fields\n",
      "for the second one i think the only way to do it right now is to add a bunch of derived fields that define aliases to your species fields\n",
      "i also think that if you load the species fields with the names yt expects for species it’ll work\n",
      "i think getting proper display names working with the stream frontend would also be something that’s worth fixing\n",
      "again, perhaps with another keyword argument to load_uniform_grid and friends\n",
      "oh so the field names work for the most part are displayed nicely, but they're in the oii, oiii form rather than the o+, o++ form\n",
      "so it was more a domain context switching question i guess?\n",
      "yeah, i agree, that’s something that would eventually be covered by the domain context system\n",
      "having a way to switch the display names between those two types of notation would also be useful\n",
      "you can see how that is set up in `yt/fields/species_fields.py`\n",
      "maybe we could add `ds.set_species_naming()` or something like that that toggles between those two settings\n",
      "cool, i'll take a look at that and maybe see about adding a setting\n",
      "just didn't want to re-invent the wheel\n",
      "re: vector fields, i'm cool with using the low level settings until i eventually build a proper frontend, but i think i'm just doing it wrong? or in the wrong order? it never seems to update the ds.derived_field_list with the things i want\n",
      "i mean that if the names aren’t exactly magnetic_field_[x,y,z] or velocity_[x,y,z] yt just fundamentally won’t see them as vector fields\n",
      "we need some what of indicating to the low-level machinery that some of the fields are actually vector fields\n",
      "actually, sorry\n",
      "are you passing in 2d arrays for the vector fields?\n",
      "or three 1d arrays?\n",
      "3 1d arrays that are all things like \"o2_p1_velocity_x\"\n",
      "i also tried setting them as different fluid types like (\"o2_p1\", \"velocity_x\"), but was still having trouble\n",
      "and you want yt to generate the derived fields for those vector fields? like o2_p1_velocity_magnitude?\n",
      "er i guess they aren't 1d arrays, they're 3d**\n",
      "yeah, or o2_p1_radial_velocity if i make a sphere\n",
      "right now im doing it all by setting up derived fields that duplicate the built in ones\n",
      "so the reason it works for velocity and magnetic fields is these two lines:\n",
      "<https://github.com/yt-project/yt/blob/master/yt/fields/fluid_fields.py#l59>\n",
      "a super hacky way to do it would be to add lines after that for your species velocities\n",
      "i guess we could also pass in a list of `extra_vector_fields` to `setup_fluid_fields` to also make the derived fields for\n",
      "does yt generate fields for the components of the velocities already?\n",
      "i don’t remember offhand if we added support to the stream frontend for that\n",
      "i know we do it in the particle frontends already\n",
      "im not sure exactly what you mean?\n",
      "so you pass into `load_uniform_grid` a 3d array with a field name like `'o2_p1_velocity'`?\n",
      "yup\n",
      "if so, i’m asking whether you can access `o2_p1_velocity_x` already\n",
      "oh no no sorry\n",
      "i pass in \"o2_p1_velocity_x\", y and z\n",
      "same with \"magnetic_field_x\" as an nxnxn array\n",
      "ahhhhh i see\n",
      "so one way to do this would be in the stream frontend to look for group of fields that might be the components of a vector field\n",
      "and automatically say “ah, they passed in `'o2_p1_velocity_[x,y,z]'`”\n",
      "“so therefore `'o2_p1_velocity'` is a vector field and we should treat it as such and generate the derived fields for that\n",
      "that might require modifying the `setup_fluid_fields` function i linked to earlier so it can optionally set up additional vector fields\n",
      "got it\n",
      "i guess another more general way to do this would be to define `ds.setup_vector_field(fx, fy, fz)` that lets you setup the vector fields manually for a set up of three field components\n",
      "like `ds.add_gradient_field` for example\n",
      "i think my plan for now will be to go the super hacky way and get that to work, but eventually do sometime more general to make a valid scientific workflow (tm)\n",
      "thanks so much for your help!\n",
      "np, any time :slightly_smiling_face:\n",
      "<@ub95pd8ba> has joined the channel\n",
      "<@u4esetp43> has joined the channel\n",
      "question about fluxes and surfaces and derived fields. i’m trying to get the flux in / out of a series of surfaces of increasing radii, so i have like, \n",
      "```for radius in radii:\n",
      "        surface = ds.surface(big_sphere, 'radius', (radius, 'code_length'))\n",
      "        gas_flux = surface.calculate_flux(\"velocity_x\", \"velocity_y\", \"velocity_z\", \"density\")\n",
      "```\n",
      "where `big_sphere` is a sphere with radius larger than the ones in `radii`.  but this returns the _net_ flux through that surface. sooo…\n",
      "\n",
      "and then do like ```        gas_flux_in = surface.calculate_flux(\"velocity_x\", \"velocity_y\", \"velocity_z\", \"gas_density_in\")\n",
      "        gas_flux_out = surface.calculate_flux(\"velocity_x\", \"velocity_y\", \"velocity_z\", \"gas_density_out\")\n",
      "```\n",
      "for all the radii. fine. but….\n",
      "this is what i get for `gas_flux_in` and `gas_flux_out` versus radius --- but shouldn’t one be all positive and one all negative?\n",
      "any guesses what i’m doing wrong? i’m sure this is just me not understanding the derived fields and/or surfaces and/or fluxes correctly.\n",
      "i don’t see you doing anything obviously wrong\n",
      "damn :upside_down_face:\n",
      "it could be that the flux calculation happens using vertex centered data\n",
      "and when we interpolate to generate vertex centered versions of your derived field something goes wrong there\n",
      "tbh i’d approach this by modifying the yt source code\n",
      "one could imagine adding a keyword argument to calculate_flux to tell it to ignore values above or below a threshold\n",
      "related question-- what exactly does ` surface._vertices = none` do? (it’s in my loop but after everything is calculated because :shrug:)\n",
      "why did you need to do that?\n",
      "that’s destroying the cached isocontour data yt created when it calculated the isocontour\n",
      "i have no idea :laughing:  the comment in the code is `  # this apparently makes fluxes work in a loop?`\n",
      "ok, i’m surprised you need to do that, maybe there’s a bug somewhere\n",
      "it’s hard to say without an example demonstrating the behavior\n",
      "for your original question, i think you’d need to modify `march_cubes_grid_flux`\n",
      "ok so unrelated to the actual issue i’m having.\n",
      "which does the actual isocontour flux calculation\n",
      "what does vertex centered data mean / where would i find some documentation on that? i’ve tried reading about th emarching cubes it but it’s all kind of hairy still.\n",
      "normally data in yt are defined at the centers of cells\n",
      "but many operations (isocontours, volume rendering) need data defined at the vertices of the cells (i.e. the 6 faces)\n",
      "generating vertex cenerted data means that yt does a trilinear interpolation to generate data at the cell faces based on the surrounding cell centered data\n",
      "your derived fields are defined at the cell center\n",
      "aaaaaah and the radial velocity is the velocity at the center of the cell\n",
      "yes, ok. still wild that it’d cause that much of a difference, given the large number of cells that go into the calculation\n",
      "that’s just me guessing\n",
      "it may not be the real issue\n",
      "tbh all of this isocontour flux machinery is overboard for your use case\n",
      "but yt doesn’t have a way to calculate fluxes across geometric objects\n",
      "it would be nice to have that though, particularly spheres and disks\n",
      "yeah, hence how we wound up with this. it would be nice to have, but i have no clue if i’d be up for writing it ¯\\_(ツ)_/¯\n",
      "doing marching cubes to generate a spherical surface is the part that’s going overboard, it’s also giving you an approximate answer when in principle we could get the exact answer\n",
      "ooh, fair point\n",
      "i did a thing a while ago to do this for uniform resolution data for cylindrical surfaces\n",
      "to calculate the radial mass flux in a galaxy sim\n",
      "it would be a fun project to port that to yt\n",
      "anyway\n",
      "did you check that the radial_velocity is with respect to the center of your sphere?\n",
      "i thought it was automatically?\n",
      "i think so, but easy enough to double check\n",
      "we’ve had bugs related to ensuring field parameters get passed around yt correctly in the past\n",
      "the few spot checks i have done look pretty reasonable but i haven’t don’t the full subtract the bulk velocity from what the quivers plot, but i think i know how to do that\n",
      "yeah, i’d try dropping into a debugger in your field definition\n",
      "here’s a trick:\n",
      "is there a way to tell `radial_velocity` which ~bv~ centerpoint to use?\n",
      "add the following to a field definition and you’ll pop into a debugger when the field is first called with real data (i.e. not during field detection):\n",
      "\n",
      "```\n",
      "def my_field(field, data):\n",
      "    if 'fielddetector' not in str(type(data)):\n",
      "        import pdb; pdb.set_trace()\n",
      "```\n",
      "you need to set the `center` field parameter\n",
      "it respects `bulk_velocity` as well iirc\n",
      "i’m doing my testing in a notebook so i should(?) have access to all the things. it was unclear to me how to set the `center` for the radial_velocity since i’m adding the field to the full `ds` but it doesn’t make sense until there’s actually a sphere or surface.\n",
      "how are you generating the isocontour in the first place?\n",
      "from `ds.all_data()`?\n",
      "ah, i see, `big_sphere`\n",
      "`big_sphere = ds.sphere(halo_center, 0.45*width_code)` and then later `surface = ds.surface(big_sphere, 'radius', (radius, 'code_length'))`\n",
      "so you’d do `big_sphere.set_field_parameter('center', [0.3, 0.4, 0.5])`\n",
      "and i do the eg `ds.add_field(('gas_density_in'), function=_gas_density_in, units=\"msun/kpc**3\", force_override=true)` before `big_sphere` gets defined\n",
      "that should work\n",
      "in your field definition you should be able to say `data.get_field_parameter('center')` and get the value you supplied back\n",
      "if not then there’s a bug\n",
      "i’ll try it, but it looks like it’s in there right?\n",
      "if the field parameter isn’t being used in the field definition that could explain the behavior you’re seeing\n",
      "easy enough to check :slightly_smiling_face:\n",
      "i’m just guessing though :slightly_smiling_face:\n",
      "yep, `get_field_parameter('center')` gives the same thing as `.center` :thumbsup:\n",
      "if you want, you could make a runnable version of your script and then file an issue, i can try to look closer at this in the next few days sometime\n",
      "if you decide to do that, it helps if you either upload the dataset you’re using or make use of one of the public datasets on <http://yt-project.org/data|yt-project.org/data>\n",
      "ok, thanks. may or may not happen — i’m traveling between now and the enzo workshop next week :airplane_departure: :airplane_arriving:\n",
      "ok\n",
      "i’ll be at the enzo workshop\n",
      "i may also just try to hack together a spherical flux calcuation\n",
      "sure, if you want you can take a look at <https://bitbucket.org/ngoldbaum/galaxy_analysis/src/default/galanyl/radial_flux_analyzer/radial_flux_analyzer.py> which does this for cylinders\n",
      "i also generated a covering grid so i didn’t need to worry about amr\n",
      "hi i am wondering if i can rotate the figure by 90 degree in a slice plot\n",
      "this is a different issue than fixed by <https://github.com/trident-project/trident/pull/105>?\n",
      "yes, i’ve opened an issue here, if you want to follow, <@u8fuk8kcl> :\n",
      "<https://github.com/yt-project/yt/issues/2383>\n",
      "it seems to have more to do with data retrieval in yt, as opposed to spectrum deposition once the ray is retrieved.\n",
      "i got the issue, but have to finish up teaching this week before i can really dig in, unfortunately!\n",
      "no worries, <@u042hlt7u>!\n",
      "<@uc85n7j94> has joined the channel\n",
      "maybe a silly question, but is it possible to change the background of a volume rendering to white for example instead of the usual black? i’ve looked into the `volume_rendering/scene.save()` method which calls `data_objects/image_array.write_png()`, but for the life of me i can’t figure out how to change the background colour. the `write_png()` method accepts a kwarg called `background` which defaults to black, but changing this to something else does nothing... any thoughts on this or suggestions where i should look?\n",
      "this has come up before and unfortunately because of the way the volume renderer works this is very hard, or at least it wasn’t obvious to me how to do it the last time i looked \n",
      "the reason the keyword is there is that it used to work but a major change to the volume renderer to do alpha compositing broke it and no one noticed\n",
      "<https://github.com/yt-project/yt/issues/1579|https://github.com/yt-project/yt/issues/1579>\n",
      "there are some possible workarounds in there\n",
      "aah i see\n",
      "thanks for clarifying this, i’ll see if i can get one of the workarounds to do what i want\n",
      "yeah, you would need to multiply `dts` by the physical length of the ray\n",
      "does anyone know offhand why a `ds.r[-0.2,0.0,0.5]` would through an “valueerror: setting an array element with a sequence.” compared to `ds.r[0.2,0.0,0.5]` which runs perfectly correct? my x code units go from -1 to 1.\n",
      "<@uc6l85lbb> i bet a bug! report an issue and i'll take a look?\n",
      "okay, will do. thanks!\n",
      "<@ulqt4pxbp> ugh, that is annoying!  i think it is possible, but it would be via matplotlib, and honestly i don't know how.\n",
      "one solution, altough very hacky and not satisfactory is to export in pdf and edit it using for example inkscape. but if you have multiple plots, that's really cumbersome…\n",
      "<@u37dtbl6n> i generate a lot of plots from the data of many simulation studies, so there are so many plots. it wold be very useful if i can do that with just one line of code.\n",
      "<@u042hlt7u> if i want to use a particular matplotlib option, do i have to implement an interface in yt, like `set_zlim`, which changes the range of colorbar?\n",
      "<@ulqt4pxbp> you can get access to the `axes` object -- if you save it once, or call `setup_plots` or something (i cannot recall the fastest way) it will be stored in something like `.plots` and `.axes`\n",
      "<@u042hlt7u> thank you for your advice. i think i have found the way to do that though it is not straight forward and not robust because colorbar tickers contain latex characters\n",
      "here is a plot in which +s are added in front of the positive numbers\n",
      "hi everybody. i'm trying to get yt-4.0 working on nersc so i can use `trident/sph-viz`  to look at the agora galaxies (<https://sites.google.com/site/santacruzcomparisonproject/>)\n",
      "\n",
      "it seems like yt-4.0 can't be installed on login nodes on nersc as it tries to use mpi in a way that isn't allowed. can i install it without testing mpi4py, but in such a way that mpi will still be installed when i am allowed to use it on compute nodes later? terminal output below.\n",
      "\n",
      "```(myenv) cstrawn@cori11:~/yt&gt; pip install -e .\n",
      "deprecation: python 2.7 will reach the end of its life on january 1st, 2020. please upgrade your python as python 2.7 won't be maintained after that date. a future version of pip will drop support for python 2.7.\n",
      "obtaining file:///global/u2/c/cstrawn/yt\n",
      "    complete output from command python setup.py egg_info:\n",
      "    using openmp to compile parallel extensions\n",
      "    running egg_info\n",
      "    writing requirements to yt.egg-info/requires.txt\n",
      "    writing yt.egg-info/pkg-info\n",
      "    writing top-level names to yt.egg-info/top_level.txt\n",
      "    writing dependency_links to yt.egg-info/dependency_links.txt\n",
      "    writing entry points to yt.egg-info/entry_points.txt\n",
      "    [mon feb 25 10:47:58 2019] [unknown] fatal error in pmpi_init_thread: other mpi error, error stack:\n",
      "    mpir_init_thread(537):\n",
      "    mpid_init(246).......: channel initialization failed\n",
      "    mpid_init(638).......:  pmi2 init failed: 1```\n",
      "so yt doesn’t actually need the mpi functionality in cykdtree\n",
      "the easiest thing to do would probably be to manually disable the mpi-parallel kdtree in cykdtree’s setup.py\n",
      "i realize this is a pain point and i’ll hopefully have a fix for this soon\n",
      "<@u042fh0rb> i was able to do it by filtering the list of arguments: <https://github.com/cykdtree/cykdtree/issues/9>\n",
      "ok\n",
      "i think ultimately it might make the most sense to vendor the bits of cykdtree we need into yt itself\n",
      "for a while there we were talking about making cyktree its own going concern\n",
      "and it’s not really\n",
      "<@u042fh0rb> by \"manually disable the mpi-parallel kdtree in cykdtree’s setup.py\" do you mean changing the \"true\" here to \"false\"?\n",
      "\n",
      "```# cython_trace required for coverage and line_profiler.  remove for release.\n",
      "if not release:\n",
      "    ext_options['define_macros'] = [('cython_trace', '1')]\n",
      "\n",
      "ext_options_mpi = copy.deepcopy(ext_options)\n",
      "compile_parallel = true\n",
      "if rtdflag:```\n",
      "i think that’ll do it\n",
      "nope, still hitting the error. it was actually able to install cykdtree either way, that's not where the problem was. it just breaks on yt.\n",
      "cameron's notebook (<https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb>) for installing sph-viz says to install cykdtree from source before trying to install yt, so i have that installed from a separate repo. that went fine both before and after i changed the cykdtree setup.py file.\n",
      "oh, interesting\n",
      "i have no idea why that’s happening\n",
      "yt’s setup.py shouldn’t be invoking mpi\n",
      "can you do “pip install -v -e .” to get more verbose output?\n",
      "```(myenv) cstrawn@cori11:~/yt&gt; pip install -v -e .\n",
      "deprecation: python 2.7 will reach the end of its life on january 1st, 2020. please upgrade your python as python 2.7 won't be maintained after that date. a future version of pip will drop support for python 2.7.\n",
      "created temporary directory: /tmp/pip-ephem-wheel-cache-jnmja5\n",
      "created temporary directory: /tmp/pip-req-tracker-6phufo\n",
      "created requirements tracker '/tmp/pip-req-tracker-6phufo'\n",
      "created temporary directory: /tmp/pip-install-xkhv1z\n",
      "obtaining file:///global/u2/c/cstrawn/yt\n",
      "  added file:///global/u2/c/cstrawn/yt to build tracker '/tmp/pip-req-tracker-6phufo'\n",
      "    running setup.py (path:/global/u2/c/cstrawn/yt/setup.py) egg_info for package from file:///global/u2/c/cstrawn/yt\n",
      "    running command python setup.py egg_info\n",
      "    using openmp to compile parallel extensions\n",
      "    running egg_info\n",
      "    writing requirements to yt.egg-info/requires.txt\n",
      "    writing yt.egg-info/pkg-info\n",
      "    writing top-level names to yt.egg-info/top_level.txt\n",
      "    writing dependency_links to yt.egg-info/dependency_links.txt\n",
      "    writing entry points to yt.egg-info/entry_points.txt\n",
      "    compiling yt/utilities/lib/particle_kdtree_tools.pyx because it depends on /global/u2/c/cstrawn/cykdtree/cykdtree/kdtree.pxd.\n",
      "    compiling yt/utilities/lib/pixelization_routines.pyx because it depends on /global/u2/c/cstrawn/cykdtree/cykdtree/kdtree.pxd.\n",
      "    [1/2] cythonizing yt/utilities/lib/particle_kdtree_tools.pyx\n",
      "    [2/2] cythonizing yt/utilities/lib/pixelization_routines.pyx\n",
      "    [mon feb 25 11:40:48 2019] [unknown] fatal error in pmpi_init_thread: other mpi error, error stack:\n",
      "    mpir_init_thread(537):\n",
      "    mpid_init(246).......: channel initialization failed\n",
      "    mpid_init(638).......:  pmi2 init failed: 1\n",
      "cleaning up...\n",
      "removed file:///global/u2/c/cstrawn/yt from build tracker '/tmp/pip-req-tracker-6phufo'\n",
      "removed build tracker '/tmp/pip-req-tracker-6phufo'\n",
      "command \"python setup.py egg_info\" failed with error code -6 in /global/u2/c/cstrawn/yt/\n",
      "exception information:\n",
      "traceback (most recent call last):\n",
      "  file \"/global/homes/c/cstrawn/.conda/envs/myenv/lib/python2.7/site-packages/pip/_internal/cli/base_command.py\", line 179, in main\n",
      "    status = self.run(options, args)\n",
      "  file \"/global/homes/c/cstrawn/.conda/envs/myenv/lib/python2.7/site-packages/pip/_internal/commands/install.py\", line 315, in run\n",
      "    resolver.resolve(requirement_set)\n",
      "  file \"/global/homes/c/cstrawn/.conda/envs/myenv/lib/python2.7/site-packages/pip/_internal/resolve.py\", line 131, in resolve\n",
      "    self._resolve_one(requirement_set, req)\n",
      "  file \"/global/homes/c/cstrawn/.conda/envs/myenv/lib/python2.7/site-packages/pip/_internal/resolve.py\", line 294, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  file \"/global/homes/c/cstrawn/.conda/envs/myenv/lib/python2.7/site-packages/pip/_internal/resolve.py\", line 226, in _get_abstract_dist_for\n",
      "    req, self.require_hashes, self.use_user_site, self.finder,\n",
      "  file \"/global/homes/c/cstrawn/.conda/envs/myenv/lib/python2.7/site-packages/pip/_internal/operations/prepare.py\", line 382, in prepare_editable_requirement\n",
      "    abstract_dist.prep_for_dist(finder, self.build_isolation)\n",
      "  file \"/global/homes/c/cstrawn/.conda/envs/myenv/lib/python2.7/site-packages/pip/_internal/operations/prepare.py\", line 158, in prep_for_dist\n",
      "    self.req.prepare_metadata()\n",
      "  file \"/global/homes/c/cstrawn/.conda/envs/myenv/lib/python2.7/site-packages/pip/_internal/req/req_install.py\", line 530, in prepare_metadata\n",
      "    self.run_egg_info()\n",
      "  file \"/global/homes/c/cstrawn/.conda/envs/myenv/lib/python2.7/site-packages/pip/_internal/req/req_install.py\", line 609, in run_egg_info\n",
      "    command_desc='python setup.py egg_info')\n",
      "  file \"/global/homes/c/cstrawn/.conda/envs/myenv/lib/python2.7/site-packages/pip/_internal/utils/misc.py\", line 761, in call_subprocess\n",
      "    % (command_desc, proc.returncode, cwd))\n",
      "\n",
      "installationerror: command \"python setup.py egg_info\" failed with error code -6 in /global/u2/c/cstrawn/yt/\n",
      "```\n",
      "compiling pixelization_routines.pyx triggered an mpi failure?\n",
      "super weird\n",
      "maybe its the use of openmp?\n",
      "i guess you could disable openmp in yt’s setup.py?\n",
      "it also seems worth noting that while cykdtree would install ok, actually calling `import cykdtree` in a python terminal gave a similar error\n",
      "ah i see\n",
      "so yes, it is the cykdtree parallel stuff\n",
      "when you turned it off earlier did you clean the installation?\n",
      "e.g. “git clean -fxd”?\n",
      "that will delete the old installation, you could then recompile\n",
      "yt doesn’t use cykdtree’s parallel kdtree stuff so you can just completely disable it\n",
      "oh okay that worked! i hadn't done the \"git clean -fxd\"  so i guess it may have continued trying to use the parallel installation of cykdtree. after i did that, and reinstalled with parallel off, it was able to install yt\n",
      "thanks!\n",
      "ok great :slightly_smiling_face:\n",
      "<@ua70p65gs> has joined the channel\n",
      "i've since tried again in a new environment and everything seemed to work. i'm afraid i'm not sure what i did differently the second time around\n",
      "<@u010zdngdpa> the important thing is that it's now working!\n",
      "thanks for reporting back !\n",
      "<@u010u6b8wf5> has joined the channel\n",
      "hi, now i can load the ramses data, but there is a new issue when i try to add a field or make a plot. the error is: raise ytfilenotparseable(fname, i+1)\n",
      "yt.utilities.exceptions.ytfilenotparseable: error while parsing file\n",
      "there is no problem if i go back to yt 3.4.1\n",
      "hmm, let me se if i can reproduce with that dataset you shared yesterday\n",
      "there was some reworking of how ramses outputs get parsed for yt 3.5 (ping <@u37dtbl6n>)\n",
      "ah yes i can reproduce it too\n",
      "<@u37dtbl6n> the line in the descriptor file it’s dying on looks like `' 8, b_\\x00_right, d\\n'`\n",
      "it’s dying trying to deal with this regex <https://github.com/yt-project/yt/blob/master/yt/frontends/ramses/io.py#l302-l304>\n",
      "let me see\n",
      "the dataset is this one <http://use.yt/upload/2d98cead>\n",
      "~5 gb zipped\n",
      "oh right, the regexp is missing the backslash `\\`\n",
      "where should that go?\n",
      "the regexp should become something like\n",
      "```\n",
      " var_desc_re = re.compile(r'\\s*(\\s+),\\s*(\\s+),\\s*(\\s+)')\n",
      "```\n",
      "<https://github.com/yt-project/yt/blob/a3cfbf5982464199a8391db0a4d301066cc26e04/yt/frontends/ramses/io.py#l217>\n",
      "instead of using `\\w` (any alphanumeric character + underscore), `\\s` would match any character except white spaces\n",
      "seems to work :slightly_smiling_face:\n",
      "let me submit a pr\n",
      "<@ubju11gju> <https://i.imgur.com/wqzozdw.png> does that look right?\n",
      "<@ubju11gju> thank you for the reports, please keep them coming :slightly_smiling_face:\n",
      "and sorry for the trouble and confusion, we try to avoid breaking things but when we do it’s really great to hear about it\n",
      "<@ubju11gju> this should fix your bug <https://github.com/yt-project/yt/pull/2202>\n",
      "thanks for the quick fix corentin :slightly_smiling_face:\n",
      "always a pleasure!\n",
      "great! thank you all!\n",
      "hi, there should be another line var_desc_re = re.compile(r’\\s*(\\d+),\\s*(\\w+),\\s*(\\w+)’) in io.py reads the fluid data. that line should also be replaced.\n",
      "<@ubju11gju> <https://github.com/yt-project/yt/pull/2202/files#r263888458>, feel free to comment on github :slightly_smiling_face:\n",
      "hi, i'm trying to load some `tipsy` data. it gets past the yt.load ok, but then when i try to do things it breaks.\n",
      "```\n",
      "c = ds.find_max(('gas','density'))[1].in_units(\"kpc\")\n",
      "traceback (most recent call last):\n",
      "  file \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n",
      "  file \"/vol/sci/astro/home/cjstrawn/yt/yt/data_objects/static_output.py\", line 855, in find_max\n",
      "    source.quantities.max_location(field)\n",
      "  file \"/vol/sci/astro/home/cjstrawn/yt/yt/data_objects/derived_quantities.py\", line 638, in __call__\n",
      "    rv = super(maxlocation, self).__call__(field, sample_fields)\n",
      "  file \"/vol/sci/astro/home/cjstrawn/yt/yt/data_objects/derived_quantities.py\", line 594, in __call__\n",
      "    rv = super(sampleatmaxfieldvalues, self).__call__(field, sample_fields)\n",
      "  file \"/vol/sci/astro/home/cjstrawn/yt/yt/data_objects/derived_quantities.py\", line 75, in __call__\n",
      "    sto.result = self.process_chunk(ds, *args, **kwargs)\n",
      "  file \"/vol/sci/astro/home/cjstrawn/yt/yt/data_objects/derived_quantities.py\", line 603, in process_chunk\n",
      "    if data[field].size &gt; 0:\n",
      "  file \"/vol/sci/astro/home/cjstrawn/yt/yt/data_objects/data_containers.py\", line 257, in __getitem__\n",
      "    self.get_data(f)\n",
      "  file \"/vol/sci/astro/home/cjstrawn/yt/yt/data_objects/data_containers.py\", line 1411, in get_data\n",
      "    particles, self, self._current_chunk)\n",
      "  file \"/vol/sci/astro/home/cjstrawn/yt/yt/geometry/geometry_handler.py\", line 227, in _read_particle_fields\n",
      "    fields_to_read)\n",
      "  file \"/vol/sci/astro/home/cjstrawn/yt/yt/utilities/io_handler.py\", line 218, in _read_particle_selection\n",
      "    for field_r, vals in self._read_particle_fields(chunks, ptf, selector):\n",
      "  file \"/vol/sci/astro/home/cjstrawn/yt/yt/frontends/tipsy/io.py\", line 237, in _read_particle_fields\n",
      "    tf = self._fill_fields(field_list, p, hsml, mask, data_file)\n",
      "unboundlocalerror: local variable 'hsml' referenced before assignment \n",
      "```\n",
      "i went and checked out the source code, and it indeed looks like that variable is never assigned. i'd try to fix it myself, but i have not idea what 'hsml' is supposed to do. can anyone explain?\n",
      "it might be fixed in this recent pr: <https://github.com/yt-project/yt/pull/2193>\n",
      "<@u7ku54sg5> could you try updating to yt-4.0 tip and see if it's solved?\n",
      "oh ok thanks. i did a `pull` but i forgot to merge it into my fork first so i pulled before it came in. thanks <@u0860sxlk>!\n",
      "<@u6gmgd668> has joined the channel\n",
      "<@u0610ftp0> has joined the channel\n",
      "<@uecq4dea1> has joined the channel\n",
      "<@umkgp9eje> has joined the channel\n",
      "<@u91855pa9> has joined the channel\n",
      "i've been doing stuff like this\n",
      "`gas_position = data[('parttype0', 'coordinates')][glist]`\n",
      "where `data = ds.all_data()`\n",
      "is there a way for me to just grab all the `parttype0` stuff, then index into that with an integer ndarray and pull off the coordinates or such later?\n",
      "i would like to\n",
      "```\n",
      "data = ds.all_data()\n",
      "gas = data['parttype0']\n",
      "# some time later\n",
      "gas = gas[glist]\n",
      "# some time later\n",
      "func(gas['coordinates'])\n",
      "```\n",
      "nope\n",
      "not off-hand anyway\n",
      ":disappointed:\n",
      "<@udxdkjcdt> so the issue is that we don’t have great support for vector field parameters at the moment, so during field detection we assume that field parameters are scalars\n",
      "there are a special set of whitelisted field parameters that do get set up as vectors during field detection, but it’s all hard-coded in the field detection machinery\n",
      "as a workaround, if you modify your script like this it does work:\n",
      "```\n",
      "     if isinstance(data, fielddetector):\n",
      "        vec_a = np.array([0, 0, 1])\n",
      "    elif data.has_field_parameter(\"vector_a\"):\n",
      "        vec_a = data.get_field_parameter(\"vector_a\")\n",
      "    else:\n",
      "        vec_a = np.array([0, 0, 1])\n",
      "```\n",
      "where `fielddetector` is imported like this: `from yt.fields.field_detector import fielddetector`\n",
      "sorry for the trouble, ideally your script would work, but i think in order for it to do so we’ll need to change the field detector api to first make you register a default value for new field parameters that are unknown to yt\n",
      "thanks <@u042fh0rb>! this is very helpful, i’ll give it a try soon!\n",
      "thanks for responding, i actually figured it out. now i'm having trouble loading/visualizing particle data.\n",
      "yt has documentation here <https://yt-project.org/doc/index.html> sounds to me like you want the quickstart notebooks or the cookbook in the top bar\n",
      "thank you! i will check this out! \n",
      "hey y'all,\n",
      "i'm having problems with the `x_nuclei_density` and the `x_number_density`  fields in my gadget/owls snapshots. the two fields have the exact same values.  from what i understand the `x_number_density`  fields represent the neutral ion number densities right? so `x_number_density`  and `x_p0_number_density`  values should be equal.\n",
      "not sure why `x_nuclei_density` and the `x_number_density`  are equal, but this doesn't seem to be right. can someone confirm this? thanks\n",
      "hello everyone, i was hoping i could get some help here.\n",
      "i am trying to put my cosmological simulation data onto a regular grid with sph smoothing (using yt-4.0.dev0), but i consistently seem to be running into a problem with any simulation i try. i have tested this on illustris and llustristng (both high resolution and low resolution 75 mpc/h boxes), eagle and some of my own arepo-based simulations and for all of them the mean baryon density seems to come out too low compared to the critical density. the factor off is of the order of (1+z)³, although not exactly, which would imply that yt is converting the densities to physical ones, but i am not feeding the redshift into load_particles, so i do not understand how this is possible.\n",
      "here is my script, in this case running on the low-resolution tng100-3 box, snapshot 33 (which is z = 2):\n",
      "\n",
      "```import numpy as np\n",
      "import h5py\n",
      "from astropy.cosmology import flatlambdacdm\n",
      "from astropy import units as u\n",
      "import yt\n",
      "\n",
      "sim = 'tng100-3' #path to the simulation files\n",
      "\n",
      "snap = 33 #snapshot number\n",
      "\n",
      "#extracting cosmology, etc.\n",
      "header = h5py.file(sim+'/snap_0%d.0.hdf5'%snap,'r')['header'].attrs\n",
      "rs = header['redshift']\n",
      "maxchunks = header['numfilespersnapshot']\n",
      "l = header['boxsize']\n",
      "\n",
      "cosmo = flatlambdacdm(h0=header['hubbleparam']*100.,om0=header['omega0'],ob0=header['omegabaryon'],tcmb0=2.725)\n",
      "\n",
      "rhocrit_bar = cosmo.critical_density(rs)*cosmo.ob(rs)  #critical_density in cgs units\n",
      "rhocrit_bar_simunits = rhocrit_<http://bar.to|bar.to>(1.e10*u.m_sun/cosmo.h/((u.kpc/cosmo.h)**3)) #critical density in simulation units\n",
      "\n",
      "#loading necessary parameters from hdf5 files into memory\n",
      "posx = []\n",
      "posy = []\n",
      "posz = []\n",
      "dens = []\n",
      "m = []\n",
      "smolen = []\n",
      "for chunk in range(maxchunks):\n",
      "  data = h5py.file(sim+'/snap_0%d.%d.hdf5'%(snap,chunk),'r')  \n",
      "  coords = np.array(data['parttype0/coordinates'])\n",
      "  density = np.array(data['parttype0/density'])\n",
      "  mass = np.array(data['parttype0/masses'])\n",
      "    \n",
      "  posx.append(coords[:,0])\n",
      "  posy.append(coords[:,1])\n",
      "  posz.append(coords[:,2])\n",
      "  dens.append(density)\n",
      "  m.append(mass)\n",
      "  \n",
      "  if (sim == 'illustris-1') or (sim == 'illustris-3'):\n",
      "    smolen.append(np.array(data['parttype0/smoothinglength']))\n",
      "  elif (sim == 'tng100-1') or (sim == 'tng100-3'):\n",
      "    smolen.append((3./4./np.pi*mass/density)**(1./3.))\n",
      "    \n",
      "posx = np.hstack(posx).astype('float64')\n",
      "posy = np.hstack(posy).astype('float64')\n",
      "posz = np.hstack(posz).astype('float64')\n",
      "dens = np.hstack(dens).astype('float64')\n",
      "m = np.hstack(m).astype('float64')\n",
      "smolen = np.hstack(smolen).astype('float64')\n",
      "\n",
      "bbox = np.array([[0.,l],[0.,l],[0.,l]])\n",
      "  \n",
      "data = dict(density = dens,\n",
      "            particle_position_x = posx,\n",
      "            particle_position_y = posy,\n",
      "            particle_position_z = posz,\n",
      "            particle_mass = m,\n",
      "            smoothing_length = smolen\n",
      "            )\n",
      "\n",
      "ds = yt.load_particles(data,bbox=bbox)\n",
      "ds.num_neighbors = 16\n",
      "ds.sph_smoothing_style = 'gather'\n",
      "\n",
      "n = 600 #number of grid-cells per side\n",
      "ag = ds.arbitrary_grid(ds.domain_left_edge,ds.domain_right_edge,dims=[n,n,n])\n",
      "mesh_dens = ag['io','density']```\n",
      "note that i am using yt.load_particles instead of yt.load because for the higher resolution simulations i need to split up the box to conserve memory.\n",
      "overall the sph smoothing seems to be working fine, just the units seem to be off after coming out of yt (see attached image of the central slice in rho/rho_critb).\n",
      "has anyone else noticed similar problems or does anyone see if i'm doing something wrong?\n",
      "if i follow, it looks like you are computing the physical baryon density at z=2? (using astropy units)\n",
      "\n",
      "then the image, is doing the sph deposition in simulation units, which is typically is co-moving units / h?\n",
      "\n",
      "also just to note: you calculate the baryon density in 1e+10 msun/h / (kpc/h)^3, is the simulation in kpc or mpc?\n",
      "isn't the astropy critical density comoving? it's just 3h²/8pig *ob\n",
      "\n",
      "indeed the simulations are in kpc/h\n",
      "hi <@u012raulp7t>, this is far from my expertise but i think i can answer a specific point in your question :\n",
      "&gt;  which would imply that yt is converting the densities to physical ones,\n",
      "yes: yt will _always_ convert known quantities (such as density) to physical units. how it does so in this case, with incomplete data, i don’t know, but i’d guess that some default assumption about the redshift is done internally.\n",
      "thank for your replies. it's still surprising then that it manages to get the redshift roughly correct every time. i tried this with snapshots at z = 2, z= 3 and z = 20 and the outcoming mean density is always off by a factor of the order of (1+z)³, but not exactly\n",
      "\n",
      "for reference, i get a factor of 23 for z = 2, a factor 55.6 for z = 3 and a factor ~9290 for z = 20\n",
      "i'm not familiar with astropy, but *if* astropy was returning the physical baryon density, would that explain the difference?\n",
      "(assuming the simulations output co-moving kpc)\n",
      "if i manually calculate 3h²/8pig * ob i get the same answer as atropy, and that should be comoving. so i don't think it would explain the difference. also note that it is not exactly (1+z)³, so it could just be a coincidence\n",
      "ok, just checking!\n",
      "i think this is the part of the source code you’d want to inspect : <https://github.com/yt-project/yt/blob/e890856a697b9aa163d6c4b36299ac9e738537a6/yt/frontends/stream/data_structures.py#l321>\n",
      "no problem, i have been thinking and checking the same for the past few days :sweat_smile:. really appreciate any help i can get\n",
      "is yt reporting the units of `ag['io','density']` as code_mass/code_length^3?\n",
      "yes it is\n",
      "what happens if you set `ds.use_sph_normalization = false` before you deposit?\n",
      "does not seem to make a difference\n",
      "hi <@utk5ejyfj>, thanks for moving the discussion over here. i’m pretty sure this is because your data is not tracking the ionization states of the elements, so in essence what is listed as `number_density` is in reality the `nuclei_density`, i.e., the total density of all ions of that element. are there specific elements where you track the individual ion species? i think you said h, but are there others as well?\n",
      "hi <@u042s6y2g> thanks a lot for your response. the simulation tracks, h i, he i and he ii ion fractions individually.\n",
      "for metals, only the total metal mass fractions are tracked and not individual ionization states for different species.\n",
      "that is strange!\n",
      "ok, i think i know why the normalization didn't work. i think thats a bug in yt - not sure if that relates to the whole problem you find though\n",
      "indeed, curious\n",
      "i guess normally people define the overdensity by simply taking the mean of the entire simulation, so that's why nobody notices. one other unlikely thing i can think of is that the densities given by simulations are actually in physical kpc/h instead of comoving as their specifications say...\n",
      "it could be that... i do think this could be a yt issue though. i can make a pull request with a potential fix. i think it could be the normalization as i don't `ds.use_sph_normalization = false` is doing anything right now\n",
      "in fact, i don't think the backend is even switching to gather\n",
      "do you see a progress bar when you deposit? with the description `interpolating sph field`\n",
      "yes, it shows\n",
      "```interpolating (gather) sph field: ```\n",
      "ah! that means it's doing both a scatter and a normalization. i think i know why, i'll make a pr now\n",
      "alright, thanks for looking into it!\n",
      "using this pr (<https://github.com/yt-project/yt/pull/2558>) and the `ds.use_sph_normalization = false`  should fix the issue (i hope)\n",
      "stupid question, but how do i install the pr with pip? :sweat_smile: never done this before\n",
      "my git and pip foo is subpar i'm afraid... i assume <@u042hlt7u> <@u44sw7j11> or <@u042fh0rb> will know though\n",
      "in looking through `yt/frontends/owls/fields.py`, there is a section that sets up these ionic fields (look for the `for ion in self._ions` loop). there should be fields available called `h_p[0, 1]_[_mass, density, number_density_]` and `he_p[0, 1, 2]__[mass, density, number_density_]` . do these exist when you load your dataset?\n",
      "will be in an out today, but will check back.\n",
      "here you go\n",
      "great, thank you!\n",
      "it definitely makes a difference, for one it runs much faster now\n",
      "there's no more status bar, instead it displays\n",
      "```interpolating sph field ('io', 'density'): 0it [00:00, ?it/s]```\n",
      "the resulting image looks better value-wise, but now the mean is about a factor 120 too high. correct me if i'm wrong, but it looks more like scatter, doesn't it? there are a lot of completely empty cells and overall the image looks much less smooth\n",
      "that is strange. locally i get: `interpolating (gather) sph field:`\n",
      "it tells me it is doing a `gather`\n",
      "also thanks a lot <@ud9l1d44t> i learnt something there :stuck_out_tongue:\n",
      "you’re welcome :slightly_smiling_face:\n",
      "i think you've discovered some fishy behaviour <@u012raulp7t> - i think this will require a bit of digging\n",
      "alright, i was affraid of that\n",
      "let me know if there is anything you need me to contribute\n",
      "ah, so i ended up manually changing the code in my installed version.  maybe i looked at it the wrong way, but in the pr it looks like you changed `sph_smoothing_style` to `_sph_smoothing_style`, `use_sph_normalization` to `_use_sph_normalization` and `num_neighbors` to `_num_neighbors`\n",
      "if i do keep the false in `normalize = getattr(self.ds, 'use_sph_normalization', false)`  then now things seem to be gather smoothing again as the status bar came back:\n",
      " and it took much longer to allocate the kdtree\n",
      "```interpolating (gather) sph field: 100%|████▉| 215990000/216000000 [34:58<00:00, 130008.65it/s]```\n",
      "and things took much longer again\n",
      "the result is a density field who's mean is 0.6 of the critical density and whose values by eye look much better\n",
      "i'll test it on the other sims as well, just to make sure\n",
      "ah yeah! i think there is an issue with the frontends. so if the frontend inherits from sphfrontend e.g. if you do a yt.load on a gadget dataset then it should `_` but if you do what you do and make a yt dataset that doesn't know it holds sph data, then it should not have an `_` . i need to fix this and make it consistent across all datasets\n",
      "\n",
      "i also suspected the result without normalisation would be better. i think that it is something we need to look at in yt, i think the normalization is redundant given how we deposit, so we end up over normalizing\n",
      "thanks a lot for digging into this! your feedback has been really useful in diagnosing what is going wrong internally\n",
      "thank you for helping figure it out :slightly_smiling_face: i have been scratching my head over this problem for two weeks, so glad to see it working\n",
      "feel free to open an issue on the yt github about getting strange results with the normalization=true, or i can do it if you prefer :slightly_smiling_face:\n",
      "while running <http://sc.cr|sc.create_scene(ds) >\n",
      "for the isolatedgalaxy dataset\n",
      "galaxy0030\n",
      "i am getting a runtime error in sc.show()\n",
      "could somebody help\n",
      "volume renderring tutorial\n",
      "\n",
      "```import yt \n",
      "import numpy as np\n",
      "from yt.visualization.volume_rendering.transfer_function_helper import transferfunctionhelper\n",
      "from yt.visualization.volume_rendering.api import scene, volumesource```\n",
      "```\n",
      "ds\n",
      "galaxy0030```\n",
      "1\n",
      "```sc = yt.create_scene(ds)\n",
      "yt : [info     ] 2020-04-23 23:45:38,473 setting default field to ('gas', 'density')```\n",
      "```print(sc)\n",
      "&lt;scene object&gt;:\n",
      "sources: \n",
      "    source_00: &lt;volume source&gt;:ytregion (galaxy0030): , center=[1.543e+24 1.543e+24 1.543e+24] cm, left_edge=[0. 0. 0.] cm, right_edge=[3.086e+24 3.086e+24 3.086e+24] cm transfer_function:none\n",
      "camera: \n",
      "    &lt;camera object&gt;:\n",
      "\tposition:[1. 1. 1.] code_length\n",
      "\tfocus:[0.5 0.5 0.5] code_length\n",
      "\tnorth_vector:[ 0.81649658 -0.40824829 -0.40824829]\n",
      "\twidth:[1.5 1.5 1.5] code_length\n",
      "\tlight:none\n",
      "\tresolution:(512, 512)\n",
      "lens: &lt;lens object&gt;:\n",
      "\tlens_type:plane-parallel\n",
      "\tviewpoint:[-866025.33679714 -866025.33679714 -866025.33679714] code_length```\n",
      "```print(sc.get_source())\n",
      "&lt;volume source&gt;:ytregion (galaxy0030): , center=[1.543e+24 1.543e+24 1.543e+24] cm, left_edge=[0. 0. 0.] cm, right_edge=[3.086e+24 3.086e+24 3.086e+24] cm transfer_function:none```\n",
      "\n",
      "```sc.show()\n",
      "yt : [info     ] 2020-04-23 23:40:05,922 rendering scene (can take a while).\n",
      "yt : [info     ] 2020-04-23 23:40:05,924 creating volume\n",
      "yt : [info     ] 2020-04-23 23:40:31,386 creating transfer function\n",
      "yt : [info     ] 2020-04-23 23:40:31,387 calculating data bounds. this may take a while.  set the transferfunctionhelper.bounds to avoid this.\n",
      "---------------------------------------------------------------------------\n",
      "runtimeerror                              traceback (most recent call last)\n",
      "e:\\softwares\\anaconda\\lib\\site-packages\\ipython\\core\\formatters.py in __call__(self, obj)\n",
      "    343             method = get_real_method(obj, self.print_method)\n",
      "    344             if method is not none:\n",
      "--&gt; 345                 return method()\n",
      "    346             return none\n",
      "    347         else:\n",
      "\n",
      "e:\\softwares\\anaconda\\lib\\site-packages\\yt\\visualization\\volume_rendering\\scene.py in _repr_png_(self)\n",
      "    926         png = self._last_render.write_png(filename=none,\n",
      "    927                                           sigma_clip=self._sigma_clip,\n",
      "--&gt; 928                                           background='black')\n",
      "    929         self._sigma_clip = none\n",
      "    930         return png\n",
      "\n",
      "e:\\softwares\\anaconda\\lib\\site-packages\\yt\\data_objects\\image_array.py in write_png(self, filename, sigma_clip, background, rescale, clip_ratio)\n",
      "    287         \"\"\"\n",
      "    288         if rescale:\n",
      "--&gt; 289             scaled = self.rescale(inline=false)\n",
      "    290         else:\n",
      "    291             scaled = self\n",
      "\n",
      "e:\\softwares\\anaconda\\lib\\site-packages\\yt\\data_objects\\image_array.py in rescale(self, cmax, amax, inline)\n",
      "    240                 np.multiply(self[:, :, 3], 1.0/amax, out[:, :, 3])\n",
      "    241 \n",
      "--&gt; 242         np.clip(out, 0.0, 1.0, out)\n",
      "    243         return out\n",
      "    244 \n",
      "\n",
      "&lt;__array_function__ internals&gt; in clip(*args, **kwargs)\n",
      "\n",
      "e:\\softwares\\anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py in clip(a, a_min, a_max, out, **kwargs)\n",
      "   2082 \n",
      "   2083     \"\"\"\n",
      "-&gt; 2084     return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n",
      "   2085 \n",
      "   2086 \n",
      "\n",
      "e:\\softwares\\anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds)\n",
      "     59 \n",
      "     60     try:\n",
      "---&gt; 61         return bound(*args, **kwds)\n",
      "     62     except typeerror:\n",
      "     63         # a typeerror occurs if the object does have such a method in its\n",
      "\n",
      "e:\\softwares\\anaconda\\lib\\site-packages\\numpy\\core\\_methods.py in _clip(a, min, max, out, casting, **kwargs)\n",
      "    130     else:\n",
      "    131         return _clip_dep_invoke_with_casting(\n",
      "--&gt; 132             um.clip, a, min, max, out=out, casting=casting, **kwargs)\n",
      "    133 \n",
      "    134 def _mean(a, axis=none, dtype=none, out=none, keepdims=false):\n",
      "\n",
      "e:\\softwares\\anaconda\\lib\\site-packages\\numpy\\core\\_methods.py in _clip_dep_invoke_with_casting(ufunc, out, casting, *args, **kwargs)\n",
      "     83     # try to deal with broken casting rules\n",
      "     84     try:\n",
      "---&gt; 85         return ufunc(*args, out=out, **kwargs)\n",
      "     86     except _exceptions._ufuncoutputcastingerror as e:\n",
      "     87         # numpy 1.17.0, 2019-02-24\n",
      "\n",
      "e:\\softwares\\anaconda\\lib\\site-packages\\yt\\units\\yt_array.py in __array_ufunc__(self, ufunc, method, *inputs, **kwargs)\n",
      "   1399                 raise runtimeerror(\n",
      "   1400                     \"support for the %s ufunc with %i inputs has not been\"\n",
      "-&gt; 1401                     \"added to ytarray.\" % (str(ufunc), len(inputs)))\n",
      "   1402             if unit is none:\n",
      "   1403                 out_arr = np.array(out_arr, copy=false)\n",
      "\n",
      "runtimeerror: support for the &lt;ufunc 'clip'&gt; ufunc with 3 inputs has not beenadded to ytarray.\n",
      "\n",
      "&lt;scene object&gt;:\n",
      "sources: \n",
      "    source_00: &lt;volume source&gt;:ytregion (data0029): , center=[1.96740872e+26 1.96740872e+26 1.96740872e+26] cm, left_edge=[0. 0. 0.] cm, right_edge=[3.93481744e+26 3.93481744e+26 3.93481744e+26] cm transfer_function:none\n",
      "camera: \n",
      "    &lt;camera object&gt;:\n",
      "\tposition:[1. 1. 1.] code_length\n",
      "\tfocus:[0.5 0.5 0.5] code_length\n",
      "\tnorth_vector:[ 0.81649658 -0.40824829 -0.40824829]\n",
      "\twidth:[1.5 1.5 1.5] code_length\n",
      "\tlight:none\n",
      "\tresolution:(512, 512)\n",
      "lens: &lt;lens object&gt;:\n",
      "\tlens_type:plane-parallel\n",
      "\tviewpoint:[-866025.33679714 -866025.33679714 -866025.33679714] code_length```\n",
      "this is the error i am getting...but in the volume rendering tutorial in documentation no such error was spotted with same code and same input data\n",
      "excuse me, anybody knows how i can change the marker size in particleplot? e.g. i would like to make a particle plot with a single particle. the default maker size is too small to be seen.\n",
      "hello! i think you can try `particle_plot.annotate_particles(p_size=size, width=width)`, but this might not be the best way\n",
      "i'll see what i can do, but i found another issue when testing it on eagle and illustris.\n",
      "in their case the smoothing length is given, so i didn't estimate it by hand like i did for tng. however, it turns out that the density comes out much too low again for those two simulations. note that for eagle i increased the number of nearest neighbors to 58, which is the number they mention in the simulation paper.\n",
      "however, when i determine the smoothing length from the density and mass, like i did for tng, the gridded density comes out fine again.\n",
      "\n",
      "so i had a look at the values of the smoothing lengths, when using the density and mass i get maximum smoothing lengths of ~270 kpc/h, but the values from eagle and illustris themselves can be as large as ~3 mpc/h\n",
      "\n",
      "any idea why things could go so wrong with the smoothing lengths provided by the simulations themselves?\n",
      "hi <@udju40mfy>, i’m not sure if this answers your question, but if the node you’re looking for is `a[0]['tree'][1]`, i.e., the first ancestor (2nd node) of the first tree, then its mass would be `a[0]['tree', 'mass'][1]`, or as well you could do:\n",
      "```\n",
      "node = a[0]['tree'][1]\n",
      "node['mass']\n",
      "```\n",
      "i’m around all day if you want to talk this out\n",
      "hey <@u042s6y2g>, thanks for chiming in.  this isn’t really what i’m looking for…but now that i’m thinking more about it, i think i might be able to make it work.  will think about this a bit more…\n",
      "\n",
      "in the meantime though, what i’m really looking for is a way to go from the `node` interface to the root field interface.  in other words, how do i get the location of the properties of `node` (in your example above) in a[‘position_x’], for example?\n",
      "i’m still not quite sure what you mean by going from the node interface to the root field interface\n",
      "it may be possible for me to make the tree/node-based interface work though…what i’m trying to avoid is having to work on the halo-by-halo level, and instead copy over properties/set indices tree-by-tree wherever possible.  our existing codebase works with a structure where each halo is an entry in a set of array-likes, and there are index properties that allow you to move — i’m looking to set those indices basically.\n",
      "umm, let me try to give a clearer example\n",
      "you want the field values for all nodes in a single tree expressed as an array? is that right?\n",
      "no, more the other way around.  i want the indices of the field values for all the nodes in a single tree\n",
      "oh ok, i think i got it, one sec\n",
      "```\n",
      "node = a[1]\n",
      "ancestor = node.ancestors[0]\n",
      "print(ancestor)\n",
      "treenode[343]\n",
      "\n",
      "ancestor\n",
      "print(a['position_x'][343])\n",
      "print(ancestor['position_x'])\n",
      "0.675810317993164 unitary\n",
      "0.6937116980552673 unitary\n",
      "```\n",
      "so here, i have an ancestor, which is treenode[343] — that 343 is the same as the `uid` of the node i’ve found, but it’s not an index, and unfortunately it doesn’t match with the `uid` i get if i do `a['uid']`\n",
      "right, `a['uid']` will just be the uids of the roots\n",
      "each node has a `treeid` attribute\n",
      "as illustrated here\n",
      "```\n",
      "print(ancestor['uid'])\n",
      "print(ancestor['uid'] in a['uid'])\n",
      "print(ancestor['uid'] in a['uid'].astype('int'))\n",
      "343\n",
      "false\n",
      "false\n",
      "```\n",
      "so, for example:\n",
      "```\n",
      "tid = np.array([t.treeid for t in a[0]['tree']])\n",
      "```\n",
      "but as far as i can tell, that is only valid _within_ the tree\n",
      "```\n",
      "node = a[1]\n",
      "ancestor = node.ancestors[0]\n",
      "print(ancestor.treeid)\n",
      "\n",
      "node = a[2]\n",
      "ancestor = node.ancestors[0]\n",
      "print(ancestor.treeid)\n",
      "\n",
      "1\n",
      "1\n",
      "```\n",
      "right, that’s local to the tree\n",
      "so i still can’t use that to directly reference `a[property_name]`\n",
      "yeah, is there a non-local version of that?\n",
      "right, no, unfortunately, `a[&lt;field&gt;] ` will only ever give you the field values for the roots\n",
      "wait, i’m not sure i understand that…if i do `a['redshift']` i get a lot more than z = 0\n",
      "yeah, those are trees that had no further descendents past that redshift\n",
      "oooooh\n",
      "ok\n",
      "you see that frequently with rockstar\n",
      "hmm\n",
      "presumably trees that would get tied in with others that do go to z = 0 via phantoms if consistent trees were run\n",
      "ok, so i was misunderstanding what `a[&lt;field&gt;]` was giving me\n",
      "if you run your rockstar catalogs through consistent-trees, all of your trees go down to the final redshift. i don’t totally understand what happens to the rest of them\n",
      "ah, i see\n",
      "yes, i’m running into some issues with consistent trees unfortunately, so i’m trying to work with the raw rockstar trees.\n",
      "ok, sounds like i may need to rethink how i was tackling this problem.  i may hit you up for more help later if that’s ok, but this did give me a couple ideas\n",
      "sure, not a problem\n",
      "how do i get more pixels in a particleprojectionplot? i set `window_size=16.0` but then yt still prints `splatting (('halo0_dm', 'masses')) onto a 800 by 800 mesh` so i think i set the wrong thing\n",
      "<@u91855pa9> i think there's a `buff_size` argument?\n",
      "window_size is the figure size in inches\n",
      "if you want to increase the resolution you can do `plot.set_buff_size((1600, 1600))` (or whatever resolution you want)\n",
      "ohhhhh thanks!\n",
      "that sets the resolution of the image generated by the frb\n",
      "for the resolution of the image saved by `plot.save()`, that depends on the figure size in inches and the dpi\n",
      "hm, i'm dumping them into a pdf so i hope the matplotlib backend is just embedding the frb\n",
      "we'll see\n",
      "so, if you do `image = plot.frb[my_field]`, `image.shape` will be the resolution you specify in `plot.set_buff_size`\n",
      "but then when you save the plot you might also need to do `plot.save(mpl_kwargs={'dpi':400})` (or some other high dpi) to actually get a high resolution image saved to disk\n",
      "hope that was clear, it’s a confusing subtlety that working with matplotlib introduces\n",
      "i'm trying what you're saying and things are happening\n",
      "i get two prints about splatting now though, which is odd\n",
      "i can imagine why that is, just didn't really expect it\n",
      "i’d need to see your script, but the way the plot system works is that we try to delay pixelizing until you actually need it\n",
      "it’s possible there’s a logic error there and we’re doing it twice unnecessarily in this case?\n",
      "if you look inside yt’s plotting code that’s what the `validate_plot` and `validate_figure` decorators are doing\n",
      "<https://gist.github.com/saethlin/98346d747e3695a41f2fd7aa47c79988>\n",
      "gotta run to a meeting now\n",
      "thanks again for all your help\n",
      "would anyone be able to explain to me why if i am making a volume render doing ```sc = yt.create_scene(ds, lens_type='perspective', field='density')\n",
      "    sc_list.append(sc)\n",
      "    source = sc[0]```\n",
      "then doing ```source.tfh.set_field('density')\n",
      "    source.tfh.set_bounds((0.16e-3,1.2e-3))\n",
      "    source.tfh.set_log(false)\n",
      "    source.tfh.build_transfer_function()\n",
      "    source.tfh.setup_default()```\n",
      "gives me a visual but setting `source.tfh.set_log(true)` leaves the resulting image of `sc.save()` blank?\n",
      "<@uc6l85lbb> sadly the limits are confusing with this - change the limits to be the logged vals.\n",
      "so change `source.tfh.set_bounds((0.16e-3,1.2e-3))` to their log values?\n",
      "yup\n",
      "hi yt-ites, i have a quick question\n",
      "is this documentation still correct regarding generic loading generic arrays?  <http://yt-project.org/docs/dev/examining/loading_data.html#generic-array-data>\n",
      "nowhere in there do you set units\n",
      "that example should probably be updated\n",
      "<http://yt-project.org/docs/dev/examining/generic_array_data.html#loading-numpy-array> is more up-to-date\n",
      "oh awesome, thank you\n",
      "if you file an issue about that someone will fix it before the next release, although we’d appreciate a pr that updates it of course :slightly_smiling_face:\n",
      "of course.\n",
      "the reason i’m asking is that i have a couple of colleagues here who are dealing with plant biology data and want to do analysis of it\n",
      "it’s cat scans of plants, basically, represented as stacks of 2d images\n",
      "and so they have been rolling their own tools\n",
      "but feel there might be something better out there\n",
      ":smile:\n",
      "cool, i hope yt is useful for them\n",
      "i think sam skillman did something similar with an animated gif of a succession of ct scan cross-sections\n",
      "we spent the day in my data viz class talking about volume rendering, isosurfaces, etc. and the astro grad students were pushing yt as a solution to many problems, so the grad student doing the cat scan stuff came up afterward and was asking questions\n",
      "i seem to recall that as well\n",
      "anyway, this is super-useful, i will try to get them up and running.  thanks!\n",
      "cool :slightly_smiling_face:\n",
      "yeah, that's how i'd do that\n",
      "<@u042fh0rb> i did it with some dicom images i got as well and it worked alrighty\n",
      "iirc sam even did it by stacking the animated gif of disapproving sam\n",
      "if i have a particle-based dataset (e.g., fire), and i want to get the angular momentum vector of a data object but *only* include the gas particles, is there a way to do this easily?  i know that the derived quantity for calculating `angular_momentum_vector` has flags for `use_gas` (i.e., grid data), and `use_particles` (presumably for dm particles), but this does not allow for subspecifying the gas particles in an all-particle frontend like gizmo.\n",
      "now that we can do `off_axis_projections` with the demeshening, i’m trying to figure out which direction to project along. :slightly_smiling_face:\n",
      "no, i don’t think so right now\n",
      "it wouldn’t be crazy to add a `particle_type` keyword to the `angularmomentumvector` derived quantity\n",
      "or to the other derived quantities that touch particle fields\n",
      "i guess in the demeshening if you only use gas, you’ll use the gas particle fields\n",
      "so maybe that will work\n",
      "hmm…oh, you mean, where `particle_type` would be something like `parttype0` to specify the ftype to only use gas?\n",
      "because when i’m determining the angular momentum of the galaxy, the net disk rotation of the gas particles gets overwhelmed by the dm angular momentum which is not aligned.\n",
      "right\n",
      "setting the `use_gas=true` and `use_particles=false` didn’t seem to align either. :confused:\n",
      "so i haven’t tried to do what you’re trying to do before\n",
      "i’m doing other things right now but i’m happy to look closer at this\n",
      "i don’t think it would be very hard to add a `particle_type` keyword\n",
      "no worries, i figured i’d just bounce ideas around.\n",
      "ok, i’ll poke around at that.\n",
      "and that’s something that would be useful for other reasons, e.g. to get the angular momentum of star particles\n",
      "wait\n",
      "yes, i think now that more particle-based frontend users will be using yt, this will be more useful\n",
      "are you sure you can't do this manually?\n",
      "i can do it manually in that i can read in the file in hdf5 and calculate it without yt.\n",
      "no\n",
      "i mean, by accessing fields by particles\n",
      "angular momentum vector is a pretty simple process, right?\n",
      "yeah, it’s simple.\n",
      "sure, that would work too\n",
      "i could do that.  i just wanted to see if there was something in the code.\n",
      "sure sure, i'm not trying to well-actually you\n",
      "i’ll see if i can add this particle_ftype kwarg\n",
      "would you be opposed to adding a `particle_type` keyword to a few of the derived quantities?\n",
      "<@u042fh0rb> no, i wouldn't\n",
      "i was just trying to figure out if there was a *right-now* way of doing it for <@u042j5bn6>\n",
      ":slightly_smiling_face:\n",
      "i’ll see if i can add this now.\n",
      "thanks, team!\n",
      "for the additional check in the derived quantity, do you prefer the kwarg to be `particle_type` and then give it the `ftype`?\n",
      "of the particles\n",
      "yeah, i think that’s the easiest way to phrase it, and let it default to ‘all’ for backward compatibility\n",
      "ok.\n",
      "sounds like a plan.\n",
      "also a basic git question: if i’m pulling updates from the main repo and i want to get both updates to `master` as well as `yt-4.0`, how do i do that?  normally i just do:\n",
      "\n",
      "```\n",
      "git pull upstream master\n",
      "```\n",
      "\n",
      "but i realize this won’t pull updates to `yt-4.0`.  but it seems like maybe a bad idea to do \n",
      "\n",
      "```\n",
      "git pull upstream yt-4.0\n",
      "```\n",
      "just make sure you have that branch checked out before pulling\n",
      "so `get checkout master` then `git pull upstream master`\n",
      "and `get checkout yt-4.0` then `git pull upstream yt-4.0`\n",
      "(also this is why it’s a bad idea to commit directly to master or yt-4.0)\n",
      "(because if you did that then this would generate a merge commit)\n",
      "ok, perfect.\n",
      "thanks, <@u042fh0rb>!\n",
      "<@u042j5bn6> maybe you try giving a look at <https://github.com/ppinard/matplotlib-scalebar/> and <https://github.com/ppinard/matplotlib-colorbar/>\n",
      "those are two smale python packages to help you build plots with a scalebar/colorbar\n",
      "thanks, <@u37dtbl6n>.  i’ll check those out.  i *think* i got something working using <@u042fh0rb>’s suggestion, but anything that makes colorbar customization easier will be beneficial.\n",
      "does anyone have an idea of how you could link to quadpack from cython?\n",
      "i know it's been done for scipy, but then everything is wrapped in a python function and the integrations become super slow\n",
      "is this something that’s exposed via scipy’s lowlevelcallable?\n",
      "i’ve never used that before\n",
      "otherwise you’d need to make your own bindings for it, i think\n",
      "unless scipy exposes their bindings and distributes them\n",
      "<@u37dtbl6n> i *think* i remember there being a way to supply callable c function handles somehow to scipy for calling its odes\n",
      "ah yes\n",
      "<https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.integrate.quad.html>\n",
      "the function can be a lowlevelcallable\n",
      "<https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.lowlevelcallable.html#scipy.lowlevelcallable>\n",
      "<https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.lowlevelcallable.from_cython.html#scipy.lowlevelcallable.from_cython>\n",
      "oh, that's great. now i have to figure out how to create a lowlevelcallable\n",
      "<@u042fh0rb> regarding the issue with ghost zones, the issue is actually more about computing face-centered data right?\n",
      "i've got an almost-working version to get neighboring cells for octree-based data (ex. ramses). then if you have the neighboring cell, you can get the face-centered values with a simple linear interpolation.\n",
      "<@ucnrc0276> has joined the channel\n",
      "<@u37dtbl6n> if i remember right, the clumps don't use face center values, but the do need the kdtree iteration to get to neighbors. that may mean ghost zones depending on how it's implemented\n",
      "<@u042hlt7u> in any case we'd need an efficient way of accessing the neighboring cells?\n",
      "absolutely\n",
      "so far i'm querying the index of the up-to-4 neighboring cells in a given direction, which takes `o(log n)` time.\n",
      "that sounds pretty darn awesome :slightly_smiling_face:\n",
      "for the volume renderer it’s face-centered data, it also uses the kdtree to iterate over grids, but we can do the same thing with the octree\n",
      "and for isocontours it’s just generating face-centered data i think\n",
      "and of course if we have ghost zones then fields like voriticity can be computed :slightly_smiling_face:\n",
      "would you know how to implement ghost zones if i write a function that returns the index of neighboring cells?\n",
      "sure, if the index corresponds to data at the same amr level, just return that data by reading it\n",
      "if it’s at a lower level then sample the lower level data (there are various methods we can use for the interpolation)\n",
      "is lower coarser or finer for you?\n",
      "coarser\n",
      "does ramses store the field data for non-leaf octs?\n",
      "good question, i don't know\n",
      "for finer data it’s harder, you would need to do cascading interpolation on the other fine data that also is contained in the ghost oct you’re generating data for\n",
      "presumably you can get that by walking the octree\n",
      "it's made easier because there can't be more than one level difference between neighbors\n",
      "i’m not expert here on the interpolation algorithms you can use, i’m also not sure what precise interpolation for coarse-to-fine and fine-to-coarse yt uses for amr data\n",
      "sure, but if you go out to 4 neighbors like you’re saying :slightly_smiling_face:\n",
      "but i guess you can just go out zone by zone\n",
      "so it’s always &lt;=1 level difference\n",
      "i think the terms used for this in the amr literature is prolongation and restriction?\n",
      "ah yeah, sorry, prolongation (coarse-to-fine) is interpolation\n",
      "you can use trilinear interpolation for this, we can see if something fancier is necessary\n",
      "for restriction (fine-to-coarse) it’s a weighted averaging operation\n",
      "presumably for an octree it’s natural to do it recursively by averaging data in leaf octs onto non-leaf octs\n",
      "how can you access the underlying octree container from a ytregion object?\n",
      "let’s move this discussion to <#cbe6579cz|development>\n",
      "(lots of people in this channel that don’t care about this)\n",
      "<@u042fh0rb> isocontours does use vertex centered, but i believe that clump finding only uses ghost zones\n",
      "is that not what i said?\n",
      "sorry\n",
      "can never keep stuff straight without double-checking\n",
      "is there a way to format a ytquantity and still get the units? right now i'm doing something like `f\"{gal.sfr:.2f} {gal.sfr.units}\"`\n",
      "formatting as a string should get it all in one go\n",
      "<@u91855pa9> i think there was a pull request recently about this\n",
      "<https://github.com/yt-project/yt/pull/1985>\n",
      "not sure this helps.\n",
      "if i format it as a string i get a silly number of sig figs. checking the pr\n",
      "hm that looks like exactly what i want and don't seem to have\n",
      "do i not have it because i'm on the 4.0 branch?\n",
      "ah yeah\n",
      "i need to merge with master don’t i\n",
      "you can just separately format the units though\n",
      "yeah that's what i'm doing at the moment\n",
      "just a little papercut, not serious\n",
      "papercuts hurt\n",
      "<https://github.com/yt-project/yt/pull/2006>\n",
      "i have some surface brightness images that i'd like to plot alongside a projectionplot, with the same styling. any ideas on where to start?\n",
      "hmm, that’s not really something we support\n",
      "how complex are these plots?\n",
      "are there lots of annotations?\n",
      "i think i'd do just fine with a pointer to where the code for the colorbar positioning and font/label config is\n",
      "you can get something resonably like a plotwindow plot with e.g. this:\n",
      "\n",
      "```\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.imshow(image_data, norm=matplotlib.colors.lognorm(),\n",
      "           origin='lower', cmap='viridis',\n",
      "           extent=[-7.5, 7.5, -7.5, 7.5])\n",
      "\n",
      "cb = plt.colorbar()\n",
      "\n",
      "cb.set_label(r'projected density $(\\rm{g}/\\rm{cm}^2)$')\n",
      "\n",
      "plt.xlabel('x (kpc)')\n",
      "\n",
      "plt.ylabel('y (kpc)')\n",
      "\n",
      "plt.savefig('plot.png', bbox_inches='tight')\n",
      "```\n",
      "where `image_data` can come from yt via a `fixedresolutionbuffer` object\n",
      "as far as the code goes, let me take a look\n",
      "interesting, didn't know about the `norm` argument to imshow\n",
      "so most of the action of setting up a plot is in this function:\n",
      "<https://github.com/yt-project/yt/blob/827111f2547cc79327958784cdd39aba5cd7cb4d/yt/visualization/plot_window.py#l740>\n",
      "it calls into other code though so it’s not quite a 1-1 thing of just that one file\n",
      "you can think of plot window plot objects as containers for multiple plots, one plot for each field requested by the user\n",
      "the plots for each field are contained in the `.plots` dictionary\n",
      "e.g. `plot = p.plots['gas', 'density']`\n",
      "where `p` is the thing you get back from `sliceplot`\n",
      "the plot object itself will be an instance of `windowplotmpl`, which is a wrapper around a matplotlib figure\n",
      "`windowplotmpl` is a subclass of `imageplotmpl`, which is defined here:\n",
      "<https://github.com/yt-project/yt/blob/827111f2547cc79327958784cdd39aba5cd7cb4d/yt/visualization/base_plot_types.py#l190>\n",
      "there’s a lot of interaction with matplotlib there as well as in `_setup_plots`\n",
      "there’s also `plotmpl` which doesn’t have a colorbar and represents line plots, that’s used by `profileplot`\n",
      "(apologies if that’s information overload :slightly_smiling_face: )\n",
      "i think i will need much of what you just said, and it'll stick around for a while :slightly_smiling_face:\n",
      "yeah, the plotting code kinda grew organically\n",
      "there’s aspects of this problem that it’s hard to make pretty and neat\n",
      "but also i was still learning python when i wrote a lot of this code\n",
      "i was also just fiddling with the plot methods. i have been trying to hide the axes for plots made with `yt.offaxisprojectionplot`. here's my attempt,\n",
      "```\n",
      "    prj = yt.offaxisprojectionplot(ds = ds, center = [0,0,0], normal = los_vec , fields= selected_field\n",
      "      ,width       =(4, 'kpc')\n",
      "      ,north_vector=up_vec\n",
      "      ,weight_field='density',\n",
      "      )\n",
      "\n",
      "    prj.set_xlabel('kpc')\n",
      "    if iplot == 1:\n",
      "      print \"*\"*40\n",
      "      prj.set_ylabel('')\n",
      "      #help(prj)\n",
      "      #prj.set_yticks([])  # doesn't work..\n",
      "      prj.hide_axes()\n",
      "    else:\n",
      "      prj.set_ylabel(' kpc')\n",
      "```\n",
      "but it still shows the axes labels and the axes ticklabels, anyway to hide this? i looked into base_plot_types.py, plot_window.py, off_axis_projection.py but couldn't find anything.\n",
      "hmm, doesn’t `offaxisprojectionplot` have a `hide_axes` method?\n",
      "yes, i tried that, but it doesn't seem to work..\n",
      "works for me over here?\n",
      "<https://i.imgur.com/uunhzur.png>\n",
      "```\n",
      "in [1]: plot = yt.offaxisprojectionplot(ds, [1,1,1], ('gas', 'density'))\n",
      "\n",
      "in [2]: plot.hide_axes()\n",
      "out[2]: &lt;yt.visualization.plot_window.offaxisprojectionplot at 0x115242080&gt;\n",
      "\n",
      "in [3]: plot.save()\n",
      "out[3]: ['galaxy0030_offaxisprojection_density.png']\n",
      "```\n",
      "hmm, doesn't work for me, i wonder if it has to do with the other annotate functions that i call later in the code. i will try moving part of my script.\n",
      "ok cool, if you can make an example i can run to reproduce the issue i might be able to help\n",
      "on stackoverflow they call this an mcve\n",
      "<https://stackoverflow.com/help/mcve>\n",
      "oh ok\n",
      "```\n",
      "    prj = yt.offaxisprojectionplot(ds = ds, center = [0,0,0], normal = los_vec , fields= selected_field\n",
      "      ,width       =(4, 'kpc')\n",
      "      ,north_vector=up_vec\n",
      "      ,weight_field='density',\n",
      "      )\n",
      "\n",
      "    prj.hide_axes()\n",
      "```\n",
      "itself works, didnt work probably because i accidentally also call `prj.set_xlabel('kpc')` after.\n",
      "ah i see\n",
      "to change the fontsize of the label, what is the keyword arg? i tried fontsize and size, but it says typeerror: set_ylabel() got an unexpected keyword argument 'size'\n",
      "`      prj.set_ylabel('kpc', size=15)`\n",
      "```\n",
      "prj.set_font_size(15)\n",
      "```\n",
      "you might want to also look at the documentation for the yt plotting objects\n",
      "e.g.:\n",
      "<http://yt-project.org/doc/reference/api/yt.visualization.plot_window.html#yt.visualization.plot_window.axisalignedsliceplot.set_ylabel>\n",
      "you can also look at the docstrings inside ipython with the `?` operator\n",
      "e.g.:\n",
      "```\n",
      "in [12]: plot.set_ylabel?\n",
      "signature: plot.set_ylabel(label)\n",
      "docstring:\n",
      "allow the user to modify the y-axis title\n",
      "defaults to the global value.\n",
      "\n",
      "parameters\n",
      "----------\n",
      "label: str\n",
      "  the new string for the y-axis.\n",
      "\n",
      "&gt;&gt;&gt;  plot.set_ylabel(\"temperature (k)\")\n",
      "file:      ~/documents/yt-git-fixes/yt/visualization/plot_container.py\n",
      "type:      method\n",
      "```\n",
      "there’s also e.g. `set_font` for more customization\n",
      "<http://yt-project.org/doc/visualizing/plots.html#fonts>\n",
      "oh ok. thanks!\n",
      "<@u042fh0rb> i found a way to make what i want happen, but it's dirty\n",
      "```\n",
      "    for axis in ['x', 'y', 'z']:\n",
      "        plot = yt.projectionplot(...\n",
      "        density_plot = plot.plots[('gas', 'density')]\n",
      "        pdf.savefig(figure=density_plot.figure)\n",
      "\n",
      "    for i, image in enumerate(all_images):\n",
      "        density_plot._init_image(data=yt.ytarray(image, 'm'), cbnorm='log10', cblinthresh=none, cmap='inferno', extent=[0, 500, 0, 500], aspect=none)\n",
      "        pdf.savefig(figure=density_plot.figure)\n",
      "```\n",
      "i hijacked one of the plots from a projectionplot and re-init the image to plot each of mine\n",
      "it should be possible to expose that in a nicer way\n",
      "if you want to take the time to put in a pr\n",
      "i understand if you don’t\n",
      "could also just open an issue with that snippet\n",
      "i'll open an issue for now and think on this tonight\n",
      "cool!\n",
      "happy to help out\n",
      "teach someone to fish instead of giving them a fish, etc etc :slightly_smiling_face:\n",
      ":slightly_smiling_face:\n",
      "so do i just ignore all the issue default text because i'm not reporting a bug?\n",
      "oh yes, that’s just there as a guideline\n",
      "you can feel free to delete it\n",
      "i'm unsure how much credit you're due or want for that hack :slightly_smiling_face:\n",
      "where is the code that sets the colorbar label?\n",
      "i've found some snippets but nothing that's... helpful?\n",
      "there's this, which as far as i can tell is just validating the label\n",
      "<https://github.com/yt-project/yt/blob/827111f2547cc79327958784cdd39aba5cd7cb4d/yt/visualization/plot_window.py#l896>\n",
      "and i'm sure this is important, but i can't figure out where `cbax.yaxis.label` is set\n",
      "<https://github.com/yt-project/yt/blob/827111f2547cc79327958784cdd39aba5cd7cb4d/yt/visualization/base_plot_types.py#l363>\n",
      "hello everyone, i am new to yt and fairly new to python, so i apologize if this is a fairly simple question, but i've been unable to answer it by browsing the documentation. i am using yt to read data produced by athena, and i'm able to make some nice looking projection plots fairly easily. my issue comes when trying to  make more simple 2d line plots like you would typically make with say, matplotlib. i think my issue comes from not knowing how to access the data once yt has read it and stored it in fields. is there a way to call the data from the fields and use pyplot to plot it? i've tried using the lineplot function in yt, but this isn't quite the plot i want to make.\n",
      "\n",
      "to be more specific, i'm trying to make a plot of the average particle density as a function of x. so i need to find the average value of particle density for all z cells, all y cells and the 1st x cell and then repeat the process at each of the x cell locations. hopefully this makes some sense, i'd be happy to clarify further if it doesn't. thanks for your help in advance!\n",
      "<@u01046wns02>, it sounds like you desire a `profileplot` (<https://yt-project.org/doc/visualizing/plots.html#d-profile-plots>).  if you want the average density as a function of x, then your binning field would be `x` where you are profiling `density`.   a `profileplot`  operates on a data object, therefore, you can define a yt object relevant to your problem (e.g., a box, sphere, or disk).  alternatively, the `profileplot` can use the entire data set by defining the object  `dd = ds.all_data()` .\n",
      "<@uhkuhfybf> i did look into these, but i was a bit confused of how to define the \"box\" i want profiled. the code ran in a grid of 64 cells in the x, y, and z directions, and in code units the sides go from -0.1 to 0.1\n",
      "try something along the lines of\n",
      "\n",
      "```import yt \n",
      "\n",
      "# import athdf file\n",
      "ds = yt.load(\"my_athena_file.athdf\")\n",
      "\n",
      "# create data object for profile\n",
      "dd = ds.all_data()\n",
      "\n",
      "# create profile\n",
      "my_profile = yt.create_profile(dd,\n",
      "                               'x', 'density',\n",
      "                               n_bins=64, weight_field='cell_volume', \n",
      "                               logs={'x':false})\n",
      "\n",
      "# create the profile plot\n",
      "plot = yt.profileplot.from_profiles(my_profile)\n",
      "\n",
      "# show the plot\n",
      "plot.show()```\n",
      "this will profile over the entire simulation domain\n",
      "ok, i will try this. thank you so much for your help.\n",
      "if you want to restrict the analysis to some subsection of the domain, then instead of `dd=ds.all_data()` , consult object creation wiki page (<https://yt-project.org/doc/analyzing/objects.html>).\n",
      "hope this helps :smile:\n",
      "<@u0110218h8e> has joined the channel\n",
      "`import numpy as np`\n",
      "`import matplotlib.pyplot as plt`\n",
      "`from numpy import array`\n",
      "`import matplotlib as mpl`\n",
      "`from mpl_toolkits.mplot3d import axes3d                 # 3d graph`\n",
      "`from mpl_toolkits.mplot3d import proj3d                 # 3d graph`\n",
      "`import math`\n",
      "`from matplotlib import patches`\n",
      "`import code`\n",
      "`import yt`\n",
      "`from yt import ytarray  # arrays in yt module`\n",
      "`from yt.visualization.api import streamlines  # force lines`\n",
      "`import matplotlib.pylab as pl`\n",
      "\n",
      "`# choose point in field`\n",
      "`x_point = 0.007089085922957821`\n",
      "`y_point = 0.038439192046320805`\n",
      "`z_point = 0`\n",
      "\n",
      "`# load data (dictionary)`\n",
      "`try:`\n",
      "    `import cpickle as pickle`\n",
      "`except importerror:  # python 3.x`\n",
      "    `import pickle`\n",
      "\n",
      "\n",
      "`with open('data.p', 'rb') as fp:`\n",
      "    `data = pickle.load(fp)`\n",
      "\n",
      "`bx_d = data[\"bx\"]`\n",
      "`by_d = data[\"by\"]`\n",
      "`bz_d = data[\"bz\"]`\n",
      "\n",
      "\n",
      "`# 3d array of dipole magnetic field` \n",
      "`print(type(data))`\n",
      "`bbox = np.array([[-0.15, 0.15], [0, 0.2], [-0.1, 0.1]]) # box, border`\n",
      "`ds = yt.load_uniform_grid(data, bx_d.shape, length_unit=\"mpc\", bbox=bbox, nprocs=100) # data, dimension`\n",
      "\n",
      "`c = ytarray([x_point, y_point, z_point], 'm') # define c: the center of the box, chosen point`\n",
      "`c1 = ds.domain_center`\n",
      "`print('c1', c1)`\n",
      "`print(type(c1))`\n",
      "`print('center',c)`\n",
      "`n = 1 # n: the number of streamlines`\n",
      "`scale = ds.domain_width[0] # scale: the spatial scale of the streamlines relative to the boxsize,`\n",
      "`pos = c`\n",
      "\n",
      "`# create streamlines of the 3d vector velocity and integrate them through`\n",
      "`# the box defined above`\n",
      "`streamlines = streamlines(ds, pos, 'bx', 'by', 'bz', length=none) # length of integration`\n",
      "`streamlines.integrate_through_volume()`\n",
      "\n",
      "`# create a 3d plot, trace the streamlines through the 3d volume of the plot`\n",
      "`fig=pl.figure()`\n",
      "`ax = axes3d(fig)`\n",
      "`ax.scatter(x_point, y_point, z_point, marker = 'o', s=40, c='green')`\n",
      "`print('tisk', streamlines.streamlines)`\n",
      "\n",
      "`for stream in streamlines.streamlines:`\n",
      "    `stream = stream[np.all(stream != 0.0, axis=1)]`\n",
      "    `ax.plot3d(stream[:,0], stream[:,1], stream[:,2], alpha=0.1)`\n",
      "\n",
      "`# save the plot to disk.`\n",
      "`pl.savefig('streamlines.png')`\n",
      "`plt.show()`\n",
      "hi, could you help me with this code, please? i am trying to integrate force line in the given point. i don't know where is mistake - there is no streamline in the plot. i tried this example <https://yt-project.org/doc/visualizing/streamlines.html> with the change of data and the change of number of streamlines. thank you very much\n",
      "data - dipole magnetic field\n",
      "hey! is there a yt field call `eletron_number_density` or something like that? i’m only able to find `electron_density`, but would love to see if i can get the number density values, thanks!\n",
      "yes, there should be.  `electron_density` is an enzo field, which will be in enzo units.\n",
      "one sec.\n",
      "you can see `electron_density` here: <https://yt-project.org/docs/dev/reference/field_list.html#enzo-specific-fields>\n",
      "`('gas', 'el_number_density')`\n",
      "is what you want.\n",
      "hope this helps.\n",
      "sweet, found it! thanks!\n",
      "<@u7ku54sg5> any chance you could insert a debug and see what `data_file` is?\n",
      "sure. i added the line `mylog.debug(str(data_file))` right above `for axi, ax in enumerate('xyz'):`\n",
      "\n",
      "```\n",
      "generate smoothing length: 1430244it [00:14, 100282.69it/s]                             \n",
      "yt : [info     ] 2019-05-17 14:29:23,674 allocating for 5.185e+06 particles\n",
      "initializing coarse index :   0%|          | 0/13 [00:00&lt;?, ?it/s]yt : [debug    ] 2019-05-17 14:29:23,684 &lt;yt.frontends.tipsy.data_structures.tipsyfile object at 0x113742bd0&gt;\n",
      "yt : [debug    ] 2019-05-17 14:29:23,688 spanning: -7.163e-02 .. -9.002e-03 in x\n",
      "yt : [debug    ] 2019-05-17 14:29:23,692 spanning: -5.661e-02 .. 6.474e-03 in y\n",
      "yt : [debug    ] 2019-05-17 14:29:23,696 spanning: -1.528e-02 .. 2.801e-02 in z\n",
      "yt : [debug    ] 2019-05-17 14:29:23,761 &lt;yt.frontends.tipsy.data_structures.tipsyfile object at 0x113742bd0&gt;\n",
      "yt : [debug    ] 2019-05-17 14:29:23,764 spanning: -9.839e-01 .. 3.689e-01 in x\n",
      "yt : [debug    ] 2019-05-17 14:29:23,767 spanning: -1.003e+00 .. 4.364e-01 in y\n",
      "yt : [debug    ] 2019-05-17 14:29:23,770 spanning: -4.846e-01 .. 9.265e-01 in z\n",
      "yt : [debug    ] 2019-05-17 14:29:23,784 &lt;yt.frontends.tipsy.data_structures.tipsyfile object at 0x113742bd0&gt;\n",
      "```\n",
      "(stacktrace)\n",
      "```\n",
      "---------------------------------------------------------------------------\n",
      "valueerror                                traceback (most recent call last)\n",
      "&lt;ipython-input-1-9427a547d7d9&gt; in &lt;module&gt;()\n",
      "      9 dsname = \"/users/claytonstrawn/testnihaoinput/g8.26e11/g8.26e11.00320\"\n",
      "     10 ds = yt.load(dsname)\n",
      "---&gt; 11 print ds.field_list\n",
      "\n",
      "/users/claytonstrawn/yt/yt/data_objects/static_output.pyc in field_list(self)\n",
      "    556     @property\n",
      "    557     def field_list(self):\n",
      "--&gt; 558         return self.index.field_list\n",
      "    559 \n",
      "    560     def create_field_info(self):\n",
      "\n",
      "/users/claytonstrawn/yt/yt/data_objects/static_output.pyc in index(self)\n",
      "    514                 raise runtimeerror(\"you should not instantiate dataset.\")\n",
      "    515             self._instantiated_index = self._index_class(\n",
      "--&gt; 516                 self, dataset_type=self.dataset_type)\n",
      "    517             # now we do things that we need an instantiated index for\n",
      "    518             # ...first off, we create our field_info now.\n",
      "\n",
      "/users/claytonstrawn/yt/yt/geometry/particle_geometry_handler.pyc in __init__(self, ds, dataset_type)\n",
      "     41         self.float_type = np.float64\n",
      "     42         super(particleindex, self).__init__(ds, dataset_type)\n",
      "---&gt; 43         self._initialize_index()\n",
      "     44 \n",
      "     45     def _setup_geometry(self):\n",
      "\n",
      "/users/claytonstrawn/yt/yt/frontends/sph/data_structures.pyc in _initialize_index(self)\n",
      "     90             self.io._generate_smoothing_length(self.data_files, self.kdtree)\n",
      "     91 \n",
      "---&gt; 92         super(sphparticleindex, self)._initialize_index()\n",
      "     93 \n",
      "     94     def _generate_kdtree(self, fname):\n",
      "\n",
      "/users/claytonstrawn/yt/yt/geometry/particle_geometry_handler.pyc in _initialize_index(self)\n",
      "    148         except oserror:\n",
      "    149             self.regions.reset_bitmasks()\n",
      "--&gt; 150             self._initialize_coarse_index()\n",
      "    151             self._initialize_refined_index()\n",
      "    152             wdir = os.path.dirname(fname)\n",
      "\n",
      "/users/claytonstrawn/yt/yt/geometry/particle_geometry_handler.pyc in _initialize_coarse_index(self)\n",
      "    164         for i, data_file in enumerate(self.data_files):\n",
      "    165             pb.update(i)\n",
      "--&gt; 166             for ptype, pos in self.io._yield_coordinates(data_file):\n",
      "    167                 ds = self.ds\n",
      "    168                 if hasattr(ds, '_sph_ptype') and ptype == ds._sph_ptype:\n",
      "\n",
      "/users/claytonstrawn/yt/yt/frontends/tipsy/io.pyc in _yield_coordinates(self, data_file, needed_ptype)\n",
      "    311                 mylog.debug(str(data_file))\n",
      "    312                 for axi, ax in enumerate('xyz'):\n",
      "--&gt; 313                     mi = pp[\"coordinates\"][ax].min()\n",
      "    314                     ma = pp[\"coordinates\"][ax].max()\n",
      "    315                     mylog.debug(\n",
      "\n",
      "/users/claytonstrawn/anaconda2/envs/myenv/lib/python2.7/site-packages/numpy/core/_methods.pyc in _amin(a, axis, out, keepdims, initial)\n",
      "     30 def _amin(a, axis=none, out=none, keepdims=false,\n",
      "     31           initial=_novalue):\n",
      "---&gt; 32     return umr_minimum(a, axis, none, out, keepdims, initial)\n",
      "     33 \n",
      "     34 def _sum(a, axis=none, dtype=none, out=none, keepdims=false,\n",
      "\n",
      "valueerror: zero-size array to reduction operation minimum which has no identity\n",
      "```\n",
      "it does do it in io order.  same for off-axis projections.  with volume rendering i believe it does front-to-back unless specifically asked not to.\n",
      "can we do monday 10am pst?\n",
      "monday actually doesn't work for me, i'll be grading a final all day. after that i should be free all week. same time tuesday?\n",
      "<@u1dlem6kw> this seems like something we should fix on the yt side!  i know nathan had been unhappy with field parameters, and this might be a low-hanging fix.  something for us to think about.\n",
      "cool, thanks!\n",
      "<@u7hs3717v> has joined the channel\n",
      "does anyone know if there's a way to rotate a sliceplot by 90 degrees?\n",
      "<@u043bna00>, it looks like a very similar question was asked yesterday here. will that solution work for you?\n",
      "the one answered by nathan just above, in case you didn’t see it\n",
      "<@u042s6y2g> oh duh - i guess i should try reading first\n",
      "thanks for pointing me to that\n",
      "no prob, it would not have occurred to me either, i just remembered seeing it\n",
      "we should add a cookbook recipe for that\n",
      "how do i figure out what i can plot in a projection plot?\n",
      "sorry, i don’t understand the question\n",
      "can you explain a little bit more what you’re trying to do?\n",
      "i want to figure out what my snapshot looks like, as a sort of sanity check(s). i'd like to know where/how the gas is distributed, stars, dark matter, etc. maybe some properties thereof later.\n",
      "i cobbled together this almost straight out of the yt docs. my instinct is that i can replace the `item` keyword with something else, and that there's a way to get my `ds` to spit out a list of valid `item`s, but no luck so far.\n",
      "```\n",
      "image_array = yt.off_axis_projection(\n",
      "    ds,\n",
      "    center,\n",
      "    normal_vector=[0., 1., 0.],\n",
      "    width=[500., 500., 500.],\n",
      "    resolution=[1024, 1024],\n",
      "    item=(\"gas\", \"density\"),\n",
      "    north_vector=[0., 0., 1.],\n",
      ")\n",
      "```\n",
      "ah, i see\n",
      "you want to look at ds.field_list\n",
      "and ds.derived_field_list\n",
      "actually, really, just `ds.derived_field_list`\n",
      "all of the `\"gas\"` and `\"deposit\"` fields can be plotted like you’re doing\n",
      "for particle fields, you can use the `particleplot` machinery\n",
      "sorry, can you remind me, are you using the demeshening?\n",
      "you work with sph data, right?\n",
      "that is correct :slightly_smiling_face:\n",
      "love me that yt-4.0\n",
      "ok, part of the issue is that you’re using an in-development version of yt that’s not fully working yet\n",
      "like, i’m not sure that the “deposit” fields will work at the moment\n",
      "for off-axis projections anyway\n",
      "all of the “gas” fields will work though\n",
      "for dark matter and stars you probably should use the `particleplot` machinery\n",
      "this is a good point though, we’ve made it less clear what kind of fields there are in the demeshening, we need to make this clear in the docs before we do a release\n",
      "wonders if we have an off-axis particle plot\n",
      "if we don’t we should\n",
      "it would just mean rotating the particles before passing it to the regular particle plot\n",
      ":thinking_face: so you're saying i could just rotate the particle coordinates myself?\n",
      "yes, that’s what off-axis projection plot does for sph fields\n",
      "<https://github.com/yt-project/yt/blob/yt-4.0/yt/utilities/lib/pixelization_routines.pyx#l1519>\n",
      "^ that’s the cython routine that does that, we could do the same thing for particle plots\n",
      "thanks so much for your help. i'll get around to trying this soon!\n",
      "is there a splash or yt projection plotting thing for gpu?\n",
      "<@u91855pa9> enh, yt does not have a reliable one, but it's being worked on. more slowly than i'd like! :) (it's being worked on by me...)\n",
      "nope, not in yt yet\n",
      "ah okay\n",
      "i kinda just taught myself cuda and i'm wondering why my (crude, simplified) implementation of a density projection plot is not all that much faster than the yt one\n",
      "sph particles are _huge_. i never knew.\n",
      "<@u042hlt7u> i'm probably going to poke at this just occasionally, but here's what i've come up with so far\n",
      "<https://gist.github.com/saethlin/d4fd4b8baaed8042b96c5970feb12ac7>\n",
      "<@u91855pa9> awesome!\n",
      "<@ub71egs8g> has joined the channel\n",
      "<@uuuuc4uf4> has joined the channel\n",
      "<@uc6l85lbb> this is interesting, and it's possible that there are other issues that could be causing this.  would you be able to look at it with different color scaling, perhaps using symmetric log?  that might help us see any possible artifacts.\n",
      "yep! here is symmetric log:\n",
      "\n",
      "note i didn’t restrict to the same limits as before\n",
      "\n",
      "also, this is what the grid looks like. it doesn’t appear to be a boundary issue.\n",
      "that's just an idea off-hand, but could it be some kind of normalisation w.r.t the volume of cells or the cell size?\n",
      "i.e. if you divide the divergence by the cell size/volume, do you get the right answer?\n",
      "i am not sure if that will do it because that would change the divergence by a factor of 8, which wouldn’t be correct, since the first set of images shows them being pretty close.\n",
      "<@u042fh0rb> `p.set_font(24)` does not work. your second suggestion worked. however,  `p.set_unit` is not working when called to work on both the x and y axis. is this a known bug?\n",
      "i don’t think so, if you can open an issue on github that would be great, if you can please include an example script to trigger the issue, you can use one of the testing datasets on <http://yt-project.org/data|yt-project.org/data> or e.g. `ds = yt.testing.fake_random_ds()`\n",
      "<@u042j5bn6> this is for agora. it's basically just testing at this point because all the simulations are at pretty high redshifts still, and the specific results i get won't matter that much until the simulations get closer to observations\n",
      "\n",
      "indeed, i'm just putting random sightlines through at different impact parameters and recording various data (inflow vs outflow, temperatures, densities etc)\n",
      "i guess you could choose random start and endpoints inside the domain?\n",
      "e.g. between ds.domain_left_edge and ds.domain_right_edge\n",
      "that's what i ended up doing. once i get virial radius information from the larger analysis suite i can use that to scale it too\n",
      "hi all!  i'm going a little bananas - is there something special one needs to do to use yt in a notebook?  the following works fine in a regular ol' python script:\n",
      "import yt\n",
      "\n",
      "ds = yt.load(\"/users/jillnaiman1/data/isolatedgalaxy/galaxy0030/galaxy0030\")\n",
      "\n",
      "p = yt.projectionplot(ds, \"y\", \"density\")\n",
      "p.save()\n",
      "but in a jupyter notebook i get:\n",
      "runtimeerror: ytquantity values must be numeric\n",
      "<@ugpuvhm7w> how recent a version is it?  are you sure both the notebook and the console are using the same one?\n",
      "also if you can give a bit more of the traceback that'd help\n",
      "(how long until class? :slightly_smiling_face: )\n",
      "so i tried upgrading yt and checking its the same version stored in each\n",
      "(like 4 hours, so maybe no sci viz today :stuck_out_tongue: )\n",
      "can you show the entire tb?\n",
      "so like:\n",
      "\n",
      "oh ho wait!  i think i've got more than 1 version of python 3.7 ...\n",
      "well, with some magic of setting special environments, it now works.\n",
      "hurray.\n",
      ":confused:\n",
      "^ this is my excited face\n",
      "hooray!\n",
      "<@ug37nt2rm> how are you loading your dataset? can you share the script that generates the image you just posted?\n",
      "just to make it clear exactly what's happening, it's hard to provide help without all the details :slightly_smiling_face:\n",
      "of course, cheers. here it is\n",
      "```def slices(folder,nbmax,nbmin=1,time_unit='myr'): # should find nbmax automatically\n",
      "    locpath = galdatapath + folder + '/'\n",
      "    figpath = galfigpath + folder + '/'\n",
      "\n",
      "    for i in range(nbmin,nbmax+1):\n",
      "        # load a dataset\n",
      "        ds = yt.load(locpath+'output_'+\"%05i\"%i+'/info_'+\"%05i\"%i+'.txt')\n",
      "\n",
      "        # this is to make a slice plot and a projection plot\n",
      "        p = yt.sliceplot(ds, 'z', 'density')\n",
      "        p.set_axes_unit('kpc')\n",
      "        p.set_width(width, unit)\n",
      "        p.annotate_title(\"t = %.1f %s\"%(ds.current_time,time_unit))\n",
      "        p.save(figpath)  # save it as a png```\n",
      "so the `set_axes_unit` call is getting overrided by the call to `set_width`\n",
      "what's `ds.length_unit`?\n",
      "```1.0 cm```\n",
      "\n",
      "hmm, so your data aren't being loaded with the correct units it seems\n",
      "i see\n",
      "it comes from a ramses simulation\n",
      "<@u37dtbl6n> would know more details about how ramses units are parsed, but you should be able to pass `units_override` to `yt.load` to override the assumptions yt is making about the units of your data\n",
      "i look at the manual about that, thanks!\n",
      "how did you produce your simulation matt?\n",
      "is it the one you pasted on the ramses slack?\n",
      "e.g. something like this:\n",
      "\n",
      "```units_override = {\"length_unit\": (1.0, \"mpc\"),\n",
      "                  \"time_unit\": (1.0, \"myr\"),\n",
      "                  \"mass_unit\": (1.0e14, \"msun\")}\n",
      "\n",
      "ds = yt.load(dataset_filename, units_override=units_override)```\n",
      "<@u042fh0rb> it works, cheers! (if i don't set_axes_unit not set_width) :slightly_smiling_face:\n",
      "we do have support for inferring the units from the ramses output, so something in that is broken for your data\n",
      "<@u37dtbl6n> so i simply ran the sedov example from the tuto joki gave me there, without any change\n",
      "e.g. this code: <https://github.com/yt-project/yt/blob/master/yt/frontends/ramses/data_structures.py#l455>\n",
      "if i recall correctly, the default sedov explosion namelist has no units specified. in this case, units are assumed to be cgs by default (which is why you get `ds.unit_length == 1cm` )\n",
      "you can either pass units on ramses' side (<https://bitbucket.org/rteyssie/ramses/wiki/physics>) or use the code <@u042fh0rb> wrote above. i would however argue strongly in favour of the first choice, so that you are sure that everything is computed in the right units in the first place.\n",
      "so, i am very beginner with ramses/yt, but i see conversion in namelist from cgs to user units (but no name for them) here:\n",
      "```&units_params\n",
      "units_density=1.66000d-24\n",
      "units_time=3.1556926d13\t\t! 1 myr in seconds\n",
      "units_length=3.08568025d21 \t! 1 kpc in cm```\n",
      "(i guess mass shoud be set to dalton too, instead of having a density displayed in g/cm3)\n",
      "density in yt is a mass density, you may want to plot the `number_density` field if you want something with units of inverse volume\n",
      "if you have these parameters in your namelist, yt should be able to read the output properly with the right units. if it fails, this a bug!\n",
      "(either on ramses side or yt's)\n",
      "hi matt. check the units in output_xxxxx/info_xxxxx.txt, to make absolutely sure they are read from the namelist\n",
      "<@u042fh0rb> thanks.\n",
      "<@u37dtbl6n> can it give them proper names?\n",
      "<@uqddmqeea> i have this block in info.txt:\n",
      "```unit_l      =  0.100000000000000e+01\n",
      "unit_d      =  0.100000000000000e+01\n",
      "unit_t      =  0.100000000000000e+01```\n",
      "\n",
      "so ramses is not reading the namelist correctly\n",
      "indeed\n",
      "i think you probably have an older ramses version, where the units are in another namelist\n",
      "these lines should match your `&amp;unit_params` block. are you sure you correctly closed it with a trailing `/`?\n",
      "oh right, that may be!\n",
      "this is a run from slacks tuto, unchanged. i suppose i should ask on ramses slack now...\n",
      "oh, ok, i should try to update then\n",
      "the namelist parameters were moved around a bit a couple of years ago i think\n",
      "it's always fun figuring out if something is a bug in yt, a bug in the simulation code, a bug in your custom modifications to the simulation code ....\n",
      "glad we could help figure this out though :slightly_smiling_face:\n",
      "good luck with your journeys into computational astrophysics\n",
      ":+1:\n",
      "thank you very much. i suppose now i must refork repo and recompile. cheers guys!\n",
      "they were moved on september 2017 if i remember correctly ; so if your version of ramses is based on a public version older than this, you probably don't have the namelist parameters at the same location.\n",
      "i'm not sure what version i'm using. i'm going to reinstall now.\n",
      "ok, it works now, cheers! :slightly_smiling_face:  one thing, though: i try to annotate slices with simulation time through `p.annotate_title(\"t = %.1f %s\"%(ds.current_time,ds.time_unit))`. but now, `ds.time_unit` is 31556926000000.0 s... instead of manually displaying the unit i want, i guess there is a clean way to do that automatically with yt?\n",
      "`<http://ds.current_time.to|ds.current_time.to>('kyr')` ?\n",
      "yeah, that ^ :slightly_smiling_face:\n",
      "you're brilliant! :+1:\n",
      "i ran the code again, but this time on 128 cores instead of 64, which, with having triggered adaptative mesh refinement, is the only change from last time. nevertheless, on yt.load, i now get this error message:\n",
      "```ytoutputnotidentified: supplied ('../data/sedov3/output_00002/info_00002.txt',) {}, but could not load!```\n",
      "any idea?\n",
      "<@ug37nt2rm> does that file still exist?\n",
      "ok, it seems i have some folder tree issues, sorry to have bothered you!\n",
      "<@ug37nt2rm> these things happen :wink:\n",
      "<@ufypfd5cl> has joined the channel\n",
      "i have a piece of code that spend most of its time navigating an octree. i know yt employs this particle index thing and ewah files to navigate data structures faster. do you have any tips for octree work?\n",
      "<@u91855pa9> possibly!  can you describe more about the piece of code?\n",
      "it's doing ray-tracing on an octree, so when a ray exits one cell it spends a lot of time navigating to the cell the ray is entering\n",
      "<@u91855pa9> is it written in python, cython, etc?\n",
      "c\n",
      "i know that there's a way to do space-filling curve navigation of octrees, but it will take me a minute to find the paper\n",
      "that sounds helpful! i've done all the basic stuff to profile and optimize the code, at this point i'm just curious as to whether there are better algorithms or like an alternative data structure i can keep next to the octree to find the cell i want faster\n",
      "i'm not sure it was either of these papers but i am on crummy wifi: <http://wscg.zcu.cz/wscg2000/papers_2000/x31.pdf> <https://www.cse.iitb.ac.in/~sharat/icvgip.org/icvgip00/g-51.pdf>\n",
      "<http://www.merl.com/publications/docs/tr2002-41.pdf>\n",
      "the keywords that you might look for are \"viewpoint traversal\"\n",
      "first one has octree traveral in the title\n",
      "oo so does the last one\n",
      "this looks like good stuff, thanks!\n",
      "<@u91855pa9> let me know how it goes -- i'm very interested\n",
      "is there any way to pull my particle data out of a yt dataset without going through all the region selection machinery? the rest of yt is pretty great and the units are invaluable but the amount of time i have to wait while it does all the loading machinery just to slurp everything off disk is... annoying\n",
      "you could open the file handle with h5py?\n",
      "yeah i've been doing that\n",
      "what sort of api are you looking for in yt?\n",
      "the whole `ds.all_data()[(\"parttype0\", \"coordinates\")]` is a totally workable api for me. it's just slow, that's all\n",
      "i see, it’s probably possible to short-circuit index creation for that specific case\n",
      "from some cursory profiling, it looks like the region that `.all_data()` returns goes through every point in my dataset and makes sure it's in the region which is... silly\n",
      "oh!  yes, *that* can definitely be short-circuited\n",
      "it isn't implemented, but it could be, as a quick-path check\n",
      "yeah, we could make the all_data() selector a special case :slightly_smiling_face:\n",
      "i believe we currently have a way to signify that an entire object is included in a selector\n",
      "we do this for short-circuiting on, for instance, grids that are fully enclosed\n",
      "although i think we’d still need to create the index\n",
      "so you’d still have to wait for `ds.index` to finish\n",
      "so whatever progressbar appears when it loads an ewah file is approximately instant\n",
      "that's definitely not a bother\n",
      "ah cool, then we’re probably good\n",
      "any chance you can trigger the specific slowdown you’re talking about with one of the data files on <http://yt-project.org/data|yt-project.org/data>?\n",
      "to give us a benchmark :slightly_smiling_face:\n",
      "i shall see what i can do\n",
      "if these are your test datasets i can see why nobody has been bothered by this behavior before :p\n",
      "i think the largest one is gizmo_cosmology_plus\n",
      ":fire:\n",
      "i might just be way towards one end of the spectrum :shrug:\n",
      "it looks to me like the snapshots in gizmo_cosmology_plus are the largest in the available downloads. is that correct? the thing i'm complaining about is hardly noticable with snapshots of that size. again, i'm reminded that i should nag people to get a massivefire snapshot contributed to the yt data examples\n",
      "gizmo_cosmology_plus is 363 mb each, my snapshots are 13 gb each\n",
      "i don't know enough about this area to know if i could stick together a bunch of copies of one of your snapshots so you can reproduce this. hm\n",
      "it’s cool, can you by any chance do a profile of you doing an operation with one of your data files?\n",
      "and share the script and profile data?\n",
      "`python -m profile -o prof.out my_script.py`\n",
      "and you can share `prof.out`\n",
      "so _that_ is how you use `profile`\n",
      "and then i can run the script with e.g. `gizmo_cosmology_plus` and try to make the bit that’s slow in your script faster :slightly_smiling_face:\n",
      "but yeah, as is so often the case with scaling issues, you only notice with sufficiently big data, thank you for the report :slightly_smiling_face:\n",
      "i think this is a decent enough demo script to point at the relative difference, but profile incoming\n",
      "```\n",
      "import yt\n",
      "import h5py\n",
      "import time\n",
      "\n",
      "ds = yt.load('gizmo_cosmology_plus/snap_n128l16_131.hdf5')\n",
      "data = ds.all_data()\n",
      "\n",
      "start = time.time()\n",
      "data[('parttype0', 'coordinates')]\n",
      "print('yt', time.time() - start)\n",
      "\n",
      "\n",
      "with h5py.file('gizmo_cosmology_plus/snap_n128l16_131.hdf5') as f:\n",
      "    start = time.time()\n",
      "    data = f['parttype0/coordinates'][:]\n",
      "    print('h5py', time.time() - start)\n",
      "```\n",
      "this is 1.0 seconds for yt and 0.02 for h5py. if it were 1.0 seconds on my full snapshots i'd be quite content with the current status :stuck_out_tongue:\n",
      "\n",
      "and what are the timing differences for your dataset?\n",
      "419 seconds in _count_particles_chunks, that’s probably not necessary\n",
      "yt ~16 seconds, h5py 1 second (the script gets more complicated without yt because it's a multi-file snapshot)\n",
      "interesting, that timing doesn’t agree with the profile, i guess there’s some profiling overhead\n",
      "anyway, thanks, will try to see if there’s an easy way to short-circuit this…\n",
      "fun thread to follow!\n",
      "the profile is probably tracking cpu time\n",
      "i ran the profile on a login node with 32 cores\n",
      "like under mpi?\n",
      "i don’t think anything in that script is threaded\n",
      "the machinery inside `data[('....` is threaded in yt\n",
      "nope\n",
      "at least i don’t think so :slightly_smiling_face:\n",
      "```\n",
      "real    0m40.207s\n",
      "user    14m2.574s\n",
      "sys     0m9.497s\n",
      "```\n",
      "this is `time` on my demo script. this looks like threading to me (but i'll admit i've seen spooky stuff come out of `time` before)\n",
      "i guess it could be numpy if you’re using the mkl numpy build from anaconda\n",
      ":shrug:\n",
      "reproduced it locally with `gadgetdiskgalaxy`\n",
      "\n",
      "pyinstrument is nifty ^\n",
      "i'm probably using the mkl numpy build\n",
      "we can shortcut `_count_particles_chunks` for `all_data()`, since we can get the counts directly from the output\n",
      "desika has advised me to share a simba snapshot with yt as an example of a large dataset. where/how do i deliver the file?\n",
      "if it’s less than 5 gb, then “yt upload some_file.tar.gz”\n",
      "if it’s bigger than that then a google drive link should work or if you have access to a public webspace you can just put it there and share a link\n",
      "please also make a pull request to the website adding the file, let me find an example to look at\n",
      "<https://github.com/yt-project/website/pull/39>\n",
      "one of the yt developers who have access (probably me) will need to manually stage the file on the webserver running <http://yt-project.org/data|yt-project.org/data> before the pull request gets merged\n",
      "it's 30 gb\n",
      "i'll see what i can do with google drive\n",
      "here's a link to the google drive upload <https://drive.google.com/open?id=1ug5d6g62qrrpl4kv7fec7mhf7fbh-ykc>\n",
      "i'll send you a pr tomorrow\n",
      "<@udxdkjcdt> has joined the channel\n",
      "hi! i’m trying to generate light ray through a dense region in a halo. when the ray path hit the dense region, everything works fine, but if i try to let it go through a less dense region, then the ray[‘t’] field can’t be called, with a bug saying:\n",
      "\n",
      "is this an sph dataset?\n",
      "it’s an enzo dataset\n",
      "hmm, this might be a <@u042s6y2g> question\n",
      "can we also see the rest of the traceback?\n",
      "if you can make an example using one of the test enzo datasets on <http://yt-project.org/data|yt-project.org/data> that triggers this issue that would also help\n",
      "i don’t know off-hand if there are any zoom-in sims there though\n",
      "(she’s trying to upload something but we have crap wifi here)\n",
      "i’m uploading a file as a code snippet, but the wifi it’s not happy… i’ll copy the rest into this window\n",
      "so this is the code i use:\n",
      "ray_start = np.array([halo_center[0]-0.001/5, halo_center[1]+0.001/5, halo_center[2]-0.001])\n",
      "ray_end = np.array([halo_center[0]-0.001/5., halo_center[1]-0.001/5, halo_center[2]+0.001])\n",
      "print(ray_start, ray_end)\n",
      "\n",
      "rs = ds.arr(ray_start, ‘code_length’)\n",
      "re = ds.arr(ray_end, ‘code_length’)\n",
      "\n",
      "ray = ds.ray(rs, re)\n",
      "and the error when i call ray[‘t’]:\n",
      "\n",
      "52 53\n",
      "52 53\n",
      "---------------------------------------------------------------------------\n",
      "valueerror                                traceback (most recent call last)\n",
      "&lt;ipython-input-70-893c526a201d&gt; in &lt;module&gt;\n",
      "----&gt; 1 ray[‘t’]\n",
      "\n",
      "/anaconda3/lib/python3.6/site-packages/yt/data_objects/data_containers.py in __getitem__(self, key)\n",
      "    250             if f in self._container_fields:\n",
      "    251                 self.field_data[f] = \\\n",
      "--&gt; 252                     self.ds.arr(self._generate_container_field(f))\n",
      "    253                 return self.field_data[f]\n",
      "    254             else:\n",
      "\n",
      "/anaconda3/lib/python3.6/site-packages/yt/data_objects/selection_data_containers.py in _generate_container_field(self, field)\n",
      "    248             return self._current_chunk.dtcoords\n",
      "    249         elif field == “t”:\n",
      "--&gt; 250             return self._current_chunk.tcoords\n",
      "    251         else:\n",
      "    252             raise keyerror(field)\n",
      "\n",
      "/anaconda3/lib/python3.6/site-packages/yt/geometry/geometry_handler.py in cached_func(self)\n",
      "    269             tr = self._accumulate_values(n[1:])\n",
      "    270         else:\n",
      "--&gt; 271             tr = func(self)\n",
      "    272         if self._cache:\n",
      "    273\n",
      "\n",
      "/anaconda3/lib/python3.6/site-packages/yt/geometry/geometry_handler.py in tcoords(self)\n",
      "    378     @cached_property\n",
      "    379     def tcoords(self):\n",
      "--&gt; 380         self.dtcoords\n",
      "    381         return self._tcoords\n",
      "    382\n",
      "\n",
      "/anaconda3/lib/python3.6/site-packages/yt/geometry/geometry_handler.py in cached_func(self)\n",
      "    269             tr = self._accumulate_values(n[1:])\n",
      "    270         else:\n",
      "--&gt; 271             tr = func(self)\n",
      "    272         if self._cache:\n",
      "    273\n",
      "\n",
      "/anaconda3/lib/python3.6/site-packages/yt/geometry/geometry_handler.py in dtcoords(self)\n",
      "    391             gdt, gt = obj.select_tcoords(self.dobj)\n",
      "    392             if gt.size == 0: continue\n",
      "--&gt; 393             ct[ind:ind+gt.size] = gt\n",
      "    394             cdt[ind:ind+gdt.size] = gdt\n",
      "    395             ind += gt.size\n",
      "\n",
      "valueerror: could not broadcast input array from shape (31) into shape (29)\n",
      "<@udxdkjcdt> and what did you change that made it work? just the ray_start / ray_end?\n",
      "i changed the ray_start and ray_end to point to the densest region of the halo, and everything turns out fine\n",
      "so this definitely seems like a bug to me\n",
      "it’s amr so “empty” regions shouldn’t really be empty\n",
      "i’d need to poke around with an example triggering the error to say more, sorry about the trouble!\n",
      "it’s so much easier when you just tell us we’ve done something wrong :wink:\n",
      "and ray[‘x’] is good, it’s just ray[‘t’] and ray[‘dts’] that give me bug\n",
      "‘t’ has special handling, but that is a good hint\n",
      "what’s the shape of ray[‘x’]?\n",
      "(629, )\n",
      "and ray[‘density’] or other things look good as well\n",
      "you’re using yt 3.5.0?\n",
      "yes, just installed yesterday\n",
      "for some reason, for one grid, `select_tcoords` isn’t returning an array of the correct shape\n",
      "what does that mean?\n",
      "this is something that’s happening deep inside yt\n",
      "for some reason you’ve hit a corner case in a low-level routine that is causing the error you’re seeing\n",
      "hmmm, anything i can fix in the meantime?\n",
      "i don’t know offhand, my first approach here would be to fix the bug\n",
      "i’ve never seen an error like this before\n",
      "we can make a slice with the ray location overplotted and also overplot the grid locations---since this is a foggie simulation i’m not actually sure if this means we have relatively few or relatively many grids for the number of cells\n",
      "yeah, i’m sorry i don’t know what kind of corner case you’re hitting\n",
      "i’d probably need to insert print statements into a cython function to understand what’s happening\n",
      "you could as well but that might be a big ask\n",
      "the particular function that i suspect is buggy is `rayselector.get_dts`, which is defined in `yt/geometry/selection_routines.pyx`\n",
      "have you tried slightly changing the ray start and end positions?\n",
      "we’ll just decide that a spectrum through that part of the cgm is unimportant :snobby_sniff:\n",
      "lol\n",
      "again, sorry for the trouble, if you can make this particular dataset available for me to download i can try to poke at it, if you can find a public dataset that triggers this behavior that would also work\n",
      "that might be hard though\n",
      "it’s like 26gb and an output we haven’t published from yet :confused:\n",
      "sure, i understand it might be hard to share\n",
      "i’m not going to scoop you fwiw :slightly_smiling_face:\n",
      "i know :slightly_smiling_face:\n",
      "ok, dumb wifi… it’s not allowing me to upload any files or figures to show you where the start and end points are … let me change the coordinates a bit to see how it goes.\n",
      "literally the easiest way to get the output to yong was for her to fly to baltimore and plug one of my external harddrives into her laptop.\n",
      "fun!\n",
      "hmmm, when i change the x position by 1e-4, then everything turns out fine!\n",
      "yeah, i bet the position you chose was e.g. exactly on a grid boundary\n",
      "or something along those lines\n",
      "<@udxdkjcdt> but if you change by 1e-6 or 1e-5 it doesn’t work? i think in code units 1e-4 is pretty big for this run.\n",
      "ok, now it works when i switch the ray direction by 1e-6 in a certain axies (not always work, but most do).\n",
      "i’m gonna take it as the solution for now. thanks! <@u042fh0rb>\n",
      "and a dumb question, how do i get the total length of the whole ray in physical unit from the start to end point?\n",
      "the cartesian distance between the start and end point?\n",
      "yes\n",
      "just calculate it?\n",
      "oh there is not built-in function they would do that?\n",
      "np.sqrt((end_point - start_point)**2.sum()).to(‘kpc’)\n",
      "missed a sqrt\n",
      "got it, thanks!\n",
      "this comes up often enough it’s probably worth it do implement a `dl` field for rays\n",
      "true\n",
      "that would be awesome. so far there is no such thing call ray[‘dl’] right?\n",
      "yes, just dt\n",
      "which is normalized to the length of the ray\n",
      "dts?\n",
      "yeah\n",
      "cool! thanks a lot!\n",
      "if you keep on hitting this issue i’d still really like to debug it :slightly_smiling_face:\n",
      "seems like my slack is still working hard to upload the two files i was trying to, so please ignore them if they do make it through this crappy wifi …\n",
      "heh\n",
      "yes, i’ll report again!\n",
      "thanks <@u042fh0rb>!\n",
      "i have an octree that's produced by a bespoke piece of code.\n",
      "is it possible to feed this into yt? or is it likely to be easier for me to write my own projection code to make plots\n",
      "yes, with `yt.load_octree`\n",
      "oh huh it’s not in the api docs\n",
      "<https://github.com/yt-project/yt/blob/master/yt/frontends/stream/data_structures.py#l1541>\n",
      "for an example of how to use that, see <https://github.com/yt-project/yt/blob/master/yt/testing.py#l480>\n",
      "and see this function for how to create the octree mask <https://github.com/yt-project/yt/blob/master/yt/testing.py#l455>\n",
      "let me issue a pr to include `load_octree` in the api docs\n",
      "i have some index files that look _way_ too big. can i twiddle the numbers in the index filename to improve the situation?\n",
      "ah; there's an `index_order` argument to `yt.load`. are there any potential hazards with adjusting this parameter?\n",
      "<@u91855pa9> a bit -- it's tough to balance the first and second orders.  the compression *should* help with this; they're stored in a form of run-length encoding, so if they're *humongous* that might be worth figuring out why.\n",
      "the first order being the order of decomp for the top level files and the second order being for any collisions.\n",
      "what are the consequences of picking index orders that are too small? for my common usage patterns the index is just a nusicance; are there examples that exercise it?\n",
      "you can set it to zero, and it won't do any indexing at all -- indexing helps for when doing memory-lazy ops, or for when doing any type of subselection\n",
      "i'm not sure what you mean by a memory-lazy op\n",
      "<@u91855pa9> ah, so i mean, any time that the full dataset doesn't have to be in memory. but not indexing only hurts if you're trying to do spatial subselections.\n",
      "interesting. i think i'll just turn off indexing in my application\n",
      "<@uk4mcs5j7> has joined the channel\n",
      "hi all---is it possible in `yt-4.0` to set an entire `ytoctree` as a field parameter and access it from within a field function? i'm hoping to use it like below,\n",
      "\n",
      "```\n",
      "ds = yt.load('./snapshot_134.hdf5')\n",
      "ad = ds.all_data()\n",
      "octree = ds.octree(n_ref=32)\n",
      "ad.set_field_parameter('octree', octree)\n",
      "\n",
      "def _gassmootheddensity(field, data):      \n",
      "    octree = data.get_field_parameter('octree')              \n",
      "    return octree[('parttype0', 'density')]  \n",
      "\n",
      "ds.add_field(('gassmootheddensity'), function=_gassmootheddensity, units='g/cm**3', sampling_type='particle')\n",
      "```\n",
      "\n",
      "but `octree` appears to be a `float` and not an actual `ytoctree` when accessed through the `gassmootheddensity` function. in the docs it says field parameters must be `ytarray` objects, but something like:\n",
      "\n",
      "```\n",
      "ad.set_field_parameter('octree', octree)\n",
      "myoctree = ad.get_field_parameter('octree')\n",
      "```\n",
      "\n",
      "accesses the complete octree object just fine. i was hoping this would extend to accessing it via a field-defining function---is it possible to do so?\n",
      "<@uk4mcs5j7> this is something that *should* be possible, but likely isn't just because of some silly technical oversight on my part.  what i'd recommend doing, which makes me feel a tad bit gross, is to either use a global, or assign it to the `ds.parameters` dict and access it there; this would have to happen before `add_field` is called\n",
      "<@u042hlt7u>  ah great, thanks for the help!\n",
      "<@uk4mcs5j7> there may still be an issue, in which case i'll prep an example of how to force it to work\n",
      "out of curiosity, is there a way to smooth a projection/slice (yielding a fixed resolution buffer) using e.g. a gaussian filter ?\n",
      "i know you can just use the fixed resolution buffer object to get a numpy array and then smooth it, but then you can't use the yt plotting interface anymore\n",
      "<@u37dtbl6n> i believe this was implemented by <@u042f73r7> but to be honest the specifics have slipped my mind\n",
      "hey folks, question - is arepo’s data format supported by yt?  i assume it’s similar to that of gadget (possibly identical?), but i don’t see it explicitly listed on <https://yt-project.org/docs/dev/reference/code_support.html>\n",
      "<@u1dlem6kw> no, there’s an open pr from <@u042j7xjp> though\n",
      "<@u37dtbl6n> i don’t think it’s hooked up to the plotting interface, no\n",
      "would be a nice improvement to wire that up\n",
      "ok thanks! maybe this could be added as a callback?\n",
      "ah ok, thanks\n",
      "<@u37dtbl6n> yup!\n",
      "<@ugpsvm3gw> has joined the channel\n",
      "<@ugmsyv789> has joined the channel\n",
      "<@ugnaz0jn7> has joined the channel\n",
      "<@ugnmqtdgs> has joined the channel\n",
      "hi, is there a way to save data generated by `yt.offaxissliceplot` ? like, pull the x, y, z data arrays from the slice and do some calculation myself? thanks!\n",
      "you can get the image from the `frb` attribute of the plot:\n",
      "\n",
      "```image = plot.frb[plotted_field_name]```\n",
      "i think `frb.bounds` is the bounds of the image? i don’t remember the api off-hand and am on my phone\n",
      "thanks <@u042fh0rb>! yeah, both of the code lines work\n",
      "there's no vr for particle data yet\n",
      "i think most people who do similar things use sph smoothing onto a uniform grid and then volume render the uniform grid\n",
      "<@uggk0erpa>  -- <@u042fh0rb> wrote this script (or some version of it at least) a while ago that i sometimes modify for pretty pics:\n",
      "\n",
      "```\n",
      "#written by nathan golbaum\n",
      "import yt\n",
      "import numpy as np\n",
      "from yt.units import kpc\n",
      "ds = yt.load('gadgetdiskgalaxy/snapshot_200.hdf5')\n",
      "m, c = ds.find_max(('gas', 'density'))\n",
      "le = c - 25*kpc\n",
      "re = c + 25*kpc\n",
      "ag = ds.arbitrary_grid(le, re, [256]*3)\n",
      "fields = ['density', 'temperature', 'velocity_x', 'velocity_y', 'velocity_z']\n",
      "data = {}\n",
      "for f in fields:\n",
      "    print(f)\n",
      "    data[f] = ag[('gas', f)]\n",
      "    print(data[f].shape)\n",
      "bbox = np.array(list(zip(le, re)))\n",
      "ds2 = yt.load_uniform_grid(data, [256]*3, length_unit=<http://ds.length_unit.to|ds.length_unit.to>('kpc'),\n",
      "bbox=bbox)\n",
      "sc = yt.create_scene(ds2)\n",
      "sc.camera.zoom(2)\n",
      "sc.save()\n",
      "```\n",
      "oh dang, thank you! my volume rendering dreams are coming true :smile:\n",
      "what's the argument to `yt.load` that sets the resolution for the index? it's a 2-tuple, but i can't remember what the keyword argument is and the documentation and source code are failing me\n",
      "it doesn’t ring a bell, though i’m not too experienced with the various ways one can use this function. i wouldn’t be surprised if this argument was frontend-exclusive.\n",
      "have you looked at the arguments of `&lt;nameofyourcode&gt;dataset.__init__` ?\n",
      "as far as i can tell, all such functions just punt a huge pile of kwargs somewhere else\n",
      "ah ah ah it's in a different part of the help page. got it. thanks!\n",
      "then out of curiosity, care to share the solution ? :)\n",
      "`index_order`\n",
      "i want to poke at the parttype0, coordinates field for a bunch of snapshots. if i do it through `ds.all_data()`, yt-4.0 spends about 2 or 3 minute generating these indexes which are probably super helpful for things that i'm not doing. is there a way to get at my arrays without generating those indexes?\n",
      "(i can rig something up with h5py but yt is just so nice)\n",
      "no unfortunately, the way it’s set up right now you need to generate the bitmap index before we do any i/o\n",
      "i guess a hack would be to make a trivial index\n",
      "e.g. index_order = (1, 1)\n",
      "<@u91855pa9> you might be able to speed it up by setting the level of the index to very low order\n",
      "is that an argument to yt.load?\n",
      "yup\n",
      "yup\n",
      "sighs\n",
      "we should add some discussion about this to <https://github.com/yt-project/yt/blob/yt-4.0/doc/source/yt4differences.rst>\n",
      "it’s discussed in <https://ytep.readthedocs.io/en/latest/yteps/ytep-0032.html> though\n",
      "hm think that made it slower\n",
      "how big are these datasets?\n",
      "massivefire\n",
      "how many particles or size on disk?\n",
      "13 gb on disk\n",
      "and do you have write access to the directories the files are in?\n",
      "e.g. does yt write .ewah files?\n",
      "if so it only actually generates the index on the first load, the second time it just reads it from the .ewah files\n",
      "so that might explain why it’s slower with index_order=(1,1)\n",
      "oooooo\n",
      "but this is a good point, i wonder if it’s possible to delay generating the bitmap index until we *really* need it\n",
      "awesome, thanks.\n",
      "desika suggested i rig something up with symlinks. if i yt.load my symlinks instead of his files the indexes load instantaneously\n",
      "hmm, interesting\n",
      "because there are ewah files generated next to the symlinks\n",
      "that should do it, yeah\n",
      "it would make sense to have loadable ewah files, too\n",
      "what do you mean matt?\n",
      "from other locations\n",
      "ah yeah, that came up yesterday\n",
      "not that i am encouraging lots more kwargs\n",
      "i think <@u9ce5d9lz> wanted to wire something up\n",
      "e.g. have a configurable path to search for those files\n",
      "or write them\n",
      "but that’s a good hack, using symlinks\n",
      "agreed\n",
      "yeah, i also thought about symlinks but my first attempt failed as i symlink-ed the folder and not the files, so i still had the same issue. symlinks fixed it for me eventually\n",
      "i think it would be cool to put into a config though and then i can have the .ewah files generate in my data folder and not my home folder on cosma\n",
      "yeah it's a bit funky to have all the individual snapshot file symliinks in one huge directory but hey it works and yt still figures it out\n",
      "<@u91855pa9> this kind of feedback is really valuable :slightly_smiling_face:\n",
      "aw thanks. i'm never sure if simple questions like this are annoying or useful ux feedback\n",
      "assume the latter :slightly_smiling_face:\n",
      "is there an article or paper where i can read about the technique yt uses to make a density projection plot?\n",
      "so ash is planning on writing a paper, but for now probably the splash paper\n",
      "at least for scatter projections\n",
      "<http://adsabs.harvard.edu/cgi-bin/bib_query?arxiv:0709.0832>\n",
      "i’m not actually sure offhand whether ash got around to doing gather projections\n",
      "i haven't actually, but iirc then i've added the machinery to the neighbor finding to only use 2 of 3 spatial dimensions, so in principal it should be pretty straight forward. if someone has a need for it, i'm happy to have a stab. my schedule is a lot calmer starting tomorrow\n",
      "(that wasn’t me saying what you should work on btw :wink: )\n",
      "i know haha!\n",
      "the research group i'm in uses a halo finder+cool tools package caesar that identifies particles by indexes into the array returned by `ds.all_data()['parttype0', 'coordinates']`. from some experimentation, i think the order of the particles in that array is changing every time i yt.load my dataset. does this sound right?\n",
      "no, that doesn’t sound right\n",
      "oh dear\n",
      "if you can make a runnable example demonstrating the behavior you’re seeing i’m happy to take a look\n",
      "perferably with a not-huge dataset :slightly_smiling_face:\n",
      "hm; i suspect this is because the dataset is split across multiple files\n",
      "so i'm not sure how to repro this with a small amount of data\n",
      "well, you don’t need 13 gb\n",
      "this is gizmo hdf5?\n",
      "yeah\n",
      "if i could lop off everything but one particle in each file i bet that could reproduce this but i don't know how to do that\n",
      "it looks like we don’t actually have a multi-file gizmo test dataset on <http://yt-project.org/data|yt-project.org/data>\n",
      "maybe a smaller dataset that you can share with us?\n",
      "or that’s publicly available?\n",
      "i'll try the ones at /data\n",
      "gizmo_64 and gizmo_cosmology_plus are both time series\n",
      "one file per snapshot\n",
      "and fire_m12i_ref11 is one file\n",
      "oh\n",
      "i thought you said _we actually have_\n",
      "you could also share the 13 gb one\n",
      "split over a few files or something\n",
      "i think the upload size limit for `yt upload some_file` is a couple gb?\n",
      "i’m gonna go home soonish but will take a look tomorrow, if you get a script that triggers the issue you’re seeing please create an issue on github\n",
      "as a mental health thing i no longer have slack installed on my phone so i don’t normally look outside of business hours\n",
      "okay, thanks. i'll poke around on my own and check in tomorrow\n",
      "i seem to recall i may have also bumped into something like this before \n",
      "i can have a check with a dataset and see if i can see it \n",
      "i can confirm i also experience this <@u91855pa9>\n",
      "as a temp fix, you could pass in the data by sorting by a particular index, i.e `particleid's` if present. that should ensure your consistency\n",
      "thanks for the sanity check. now to see what i can do about this i guess\n",
      "i'm trying to take advantage of the hyperion grid constructor that can load in amr grids (such as enzo) from a yt dataset:\n",
      "\n",
      "<http://docs.hyperion-rt.org/en/stable/api/hyperion.grid.amrgrid.html#hyperion.grid.amrgrid.from_yt>\n",
      "\n",
      "an issue i'm running into is that i'd like to be able to pass in a subregion of the original dataset.   generating a `region` doesn't work, though, since the `amrgrid.from_yt` function in hyperion wants access to the derived_field_list (and perhaps other issues..but this was the first error thrown).\n",
      "\n",
      "since regions don't have the full attributes of a ds (i.e.\n",
      "\n",
      "```(pdb) reg.derived_field_list\n",
      "*** attributeerror: 'ytregion' object has no attribute 'derived_field_list'```\n",
      "is it possible to construct a ds-like object that is just a subregion of the original loaded ds (via a bounding box, or something like that)?\n",
      "yes, by using `region.save_as_dataset`\n",
      "and then reloading the saved ytdata dataset you'll get from that and passing that to hyperion\n",
      "(note i've never done that before)\n",
      "oh cool - i never heard of this! i'll check it out right now\n",
      "thanks <@u042fh0rb>!  this definitely saves, though it doesn't seem like all the fields get written  - for example:\n",
      "\n",
      "```import yt\n",
      "\n",
      "ds = yt.load('galaxy0030')\n",
      "ad = ds.all_data()\n",
      "print(ad[('all', 'creation_time')])\n",
      "\n",
      "#set up a region\n",
      "box_len = ds.quan(0.1,'code_length')\n",
      "center = ds.domain_center\n",
      "min_region = [center[0]-box_len,center[1]-box_len,center[2]-box_len]\n",
      "max_region = [center[0]+box_len,center[1]+box_len,center[2]+box_len]\n",
      "\n",
      "reg = ds.region(center,min_region,max_region)\n",
      "\n",
      "\n",
      "#now save the region to disk as a ds\n",
      "reg.save_as_dataset('temp_enzo.h5')\n",
      "\n",
      "#load the region back up\n",
      "ds1 = yt.load('temp_enzo.h5')\n",
      "ad1 = ds1.all_data()\n",
      "print(ad1[('all', 'creation_time')])```\n",
      "returns:\n",
      "\n",
      "```(pd3env) [desika.narayanan@login2 galaxy0030]$ python reg2ds.py\n",
      "yt : [info     ] 2020-01-16 15:54:45,046 parameters: current_time              = 0.0060000200028298\n",
      "yt : [info     ] 2020-01-16 15:54:45,046 parameters: domain_dimensions         = [32 32 32]\n",
      "yt : [info     ] 2020-01-16 15:54:45,046 parameters: domain_left_edge          = [0. 0. 0.]\n",
      "yt : [info     ] 2020-01-16 15:54:45,046 parameters: domain_right_edge         = [1. 1. 1.]\n",
      "yt : [info     ] 2020-01-16 15:54:45,046 parameters: cosmological_simulation   = 0.0\n",
      "parsing hierarchy : 100%|██████████████████████████████████████████████████████████████████| 173/173 [00:00&lt;00:00, 11512.76it/s]\n",
      "yt : [info     ] 2020-01-16 15:54:45,071 gathering a field list (this may take a moment.)\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.37082069e+16\n",
      " 1.37189892e+16 1.37880524e+16] s\n",
      "yt : [info     ] 2020-01-16 15:54:47,169 saving field data to yt dataset: temp_enzo.h5.\n",
      "yt : [info     ] 2020-01-16 15:54:47,393 parameters: current_time              = 0.0060000200028298 code_time\n",
      "yt : [info     ] 2020-01-16 15:54:47,393 parameters: domain_dimensions         = [2 2 2]\n",
      "yt : [info     ] 2020-01-16 15:54:47,393 parameters: domain_left_edge          = [0. 0. 0.] code_length\n",
      "yt : [info     ] 2020-01-16 15:54:47,393 parameters: domain_right_edge         = [1. 1. 1.] code_length\n",
      "yt : [info     ] 2020-01-16 15:54:47,394 parameters: cosmological_simulation   = 0.0\n",
      "yt : [info     ] 2020-01-16 15:54:47,431 allocating for 0.000e+00 particles (index particle type 'all')\n",
      "yt : [info     ] 2020-01-16 15:54:47,432 identified 1.000e+00 octs\n",
      "traceback (most recent call last):\n",
      "  file \"reg2ds.py\", line 22, in &lt;module&gt;\n",
      "    print(ad1[('all', 'creation_time')])\n",
      "  file \"/home/desika.narayanan/yt/yt/data_objects/data_containers.py\", line 249, in __getitem__\n",
      "    f = self._determine_fields([key])[0]\n",
      "  file \"/home/desika.narayanan/yt/yt/data_objects/data_containers.py\", line 1335, in _determine_fields\n",
      "    finfo = self.ds._get_field_info(ftype, fname)\n",
      "  file \"/home/desika.narayanan/yt/yt/data_objects/static_output.py\", line 739, in _get_field_info\n",
      "    raise ytfieldnotfound((ftype, fname), self)\n",
      "yt.utilities.exceptions.ytfieldnotfound: could not find field '('all', 'creation_time')' in temp_enzo.h5.```\n",
      "is there maybe an argument i want to pass when saving the region, or calling `region.save_as_dataset` that would save all of the fields?\n",
      "i don't know offhand, maybe look at the docs for the ytdata frontend or the docstring of save_as_dataset?\n",
      "will do\n",
      "this slight modification works:\n",
      "\n",
      "```reg.save_as_dataset('temp_enzo.h5',fields=[('all','creation_time')])```\n",
      "\n",
      "thanks for the pointer to `region.save_as_dataset` <@u042fh0rb>!\n",
      "very quick question: how do you save a yt figure to pdf ?\n",
      "`plot.save('my_figure.pdf')`\n",
      "of course :face_palm::skin-tone-2:\n",
      "thanks :smile:\n",
      "while i’m at it, how do you update the frb’s resolution again ?\n",
      "`plot.set_buff_size`\n",
      "that’ll do it, thanks a lot !\n",
      "note that the actual resolution of the figure saved by matplotlib will be determined by the dpi of the figure and the figure size\n",
      "yup, this much i know already, but thanks for pointing it out :slightly_smiling_face:\n",
      "does anyone know a way of finding the velocity dispersion of the gas particles in a particular galaxy plotted against radius?\n",
      "im using ramses and hop catalogs\n",
      "i don't know if the\n",
      "```velocity_divergence_absolute```\n",
      "parameter has anything to do with it\n",
      "in the derived field quantities\n",
      "basically really confused as to how to go about getting this (just for the gas mesh (not particles sorry))\n",
      "no, that’s the divergence not the velocity dispersion\n",
      "i did something like this for my phd\n",
      "it was easiest for me to interpolate the amr data onto a uniform resolution grid and then calculate the velocity dispersion on that uniform grid\n",
      "<https://hg.sr.ht/~ngoldbaum/galaxy_analysis/browse/default/galanyl/galaxy_analyzer.py#l750|https://hg.sr.ht/~ngoldbaum/galaxy_analysis/browse/default/galanyl/galaxy_analyzer.py#l750>\n",
      "the velocity dispersion would be very tricky to add into yt in a way that makes sense for a galaxy sim, since you need to subtract away the local streaming velocity before calculating the dispersion locally\n",
      "but that’s relatively straightforward to calculate if you know where your galaxy is and what its angular momentum vector is\n",
      "for 3d volume rendering, is there a way to specify the aspect ratios? my simulation box is a very long rectangle (1:1:50).\n",
      "no not really\n",
      "do you think that would be hard to add? if that's a simple thing, i could try and make a pr\n",
      "i'm not sure how you'd do it\n",
      "i can think of hacky ways to do it (e.g. by manually overriding the values of `ds.domain_width`, `ds.domain_left_edge` and `ds.domain_right_edge`)\n",
      "but i don't know if they would work, offhand\n",
      "i don't think they do, <@u043bna00> already suggested this to me and i tried, without success. i ended up modifying all metadata in our output.\n",
      "i guess another way to do it would be to make your image much narrower along your long axis, you can volume render to an image with an arbitrary axis ratio\n",
      "oh how can i do that? the only way i know is by playing with the resolution, but i'm afraid that would result in a ridiculous resolution, like 512*(512*50)\n",
      "yeah, that's what i mean\n",
      "you could do it at very very high resolution along the coarse axes i guess?\n",
      "sorry to not have an easy fix for you\n",
      "ok no problem, i'll keep my scripts to modify the metadata then :slightly_smiling_face:\n",
      "<@up08thq5a> has joined the channel\n",
      "<@u042fh0rb> <@uppavuhsm> wouldn't it be possible to compute the velocity dispersion in the 27 neighbouring cells using ghost zones?\n",
      "yes\n",
      "how would i do this/where is the literature on ghost zones? \n",
      "so yt does have the ability to generate fields that need ghost zones, i didn't suggest that because you'd have to write the fields (which is hard) you'd also have to subtract off the local streaming velocity which might be hard to do.\n",
      "look inside yt for fields that use `validatespatial` with a parameter set to a number not equal to zero\n",
      "but if you're only interested in the local velocity dispersion, why do you want to remove the local streaming velocity? it shouldn't matter that there's a bulk motion\n",
      "`validatespatial` is how yt's field system generates field data with spatial locality\n",
      "<@u37dtbl6n> i found it did when i did this?\n",
      "the velocity dispersions in disk galaxies are like 10 km/s in the atomic ism, but the circular velocity might be 100 km/s for a mw-mass galaxy\n",
      "but how do you define the local streaming velocity?\n",
      "the local circular velocity\n",
      "it's all in my paper, one sec\n",
      "oh right, that makes sense\n",
      "i guess it depends how you define the velocity dispersion :slightly_smiling_face:\n",
      "<@uppavuhsm> if you want to use ghost zones with ramses, you'll have to use my branch (see the pull-request here <https://github.com/yt-project/yt/pull/2425>) that adds support for ghost zones in ramses\n",
      "appendix a of <https://ui.adsabs.harvard.edu/abs/2015apj...814..131g/abstract>\n",
      "is the ghost zones method the only one <@u37dtbl6n> \n",
      "<@u042fh0rb> \n",
      "so for ramses data corentin just recently added support for yt to generate ghost zones\n",
      "corentin was linking to the pr that implements that feature he added\n",
      "it's not yet merged\n",
      "so if you want to go that route you'd (for now) need to use a version of yt with his pr applied\n",
      "ok so im looking through this pr and it seems like its just making a box and then creating a projection plot of the density gradient? so not sure how this is going through the neighbouring cells/not sure what about this is the 'ghost zone'\n",
      "to calculate a density gradient you need ghost zones\n",
      "yt can generate ghost zones, any field that needs non-local data uses that functionality\n",
      "corentin's pr adds that functionality for oct amr data, it's already there for patch amr data\n",
      "like, here's an example of a field in yt that is generated using ghost zones: <https://github.com/yt-project/yt/blob/bcf05515b7b12579c88977a7d225b7dfc36d6195/yt/fields/fluid_vector_fields.py#l88>\n",
      "the x-component of the vorticity\n",
      "it's calculated using a finite difference\n",
      "normally fields in yt are fully local, and aren't calculated using spatial data\n",
      "optionally, they can be (e.g. in the linked field)\n",
      "corentin's pr makes it possible to use these fields for oct amr codes\n",
      "in principle you could write a velocity dispersion field that uses yt's ghost zone functionality\n",
      "is there a way to get an frb from a `phaseplot` object in the same way you can from a `projectionplot` or `sliceplot`?\n",
      "it doesn’t appear so, but i just wanted to double check if there were a hack to get the image as a numpy array.\n",
      "<@u042j5bn6> yes, it's in an attribute. i believe it is .data\n",
      "oh interesting.\n",
      "i was trying .frb; i’ll give that a shot.  thanks!\n",
      "right; it's not exactly a frb, so it's called a different thing. note that the x and y bins are exposed as x and y on the profile object itself. not sure about on the plot object.\n",
      "hmm…not seeing attribute hanging off it with `data` or any other suggestive name, nor on the `phase.profile` object.\n",
      "maybe i’m doing this wrong.\n",
      "it's somewhere in there, and i'm reasonably certain it's in the docs somewhere, but i'm away from my computer. keep looking. might have an underscore. likely accessible via dict like access.\n",
      ".profile\n",
      "it’s a `profile2d` instance\n",
      "<@u042j5bn6> ^\n",
      "<https://gist.github.com/ngoldbaum/7ed9bb5d6944ad49a5e9614b02925acb>\n",
      "you can visualize it using `pcolormesh`\n",
      "oh ok, great!\n",
      "thanks, guys!\n",
      "hi, i'm having some difficulty with getting yt to run. i've recently started porting stuff over to python 3 and my attempts to test have all been marred by fnv_hash? it says that there is no module named that. within the yt.utilities.lib there are some files called fnv_hash.c/pdx/pyx but none of those seem to be the one it's looking for. does anyone have a fix in mind for this? fresh installations of yt seem to maintain this problem.\n",
      "how did you install yt?\n",
      "if it was from source did you build the cython extensions with e.g. `pip install -e .`\n",
      "the fnv_hash is one of the c extensions in yt and those need to be compiled\n",
      "if you install yt from conda-forge or via pip you’ll get a binary distribution where all of these c bits have already been compiled\n",
      "ah, gotcha, i forgot to build\n",
      "whoops\n",
      "wait, no i did use pip to install yt\n",
      "unless you're not supposed to do pip install yt anymore?\n",
      "here, i'll include some screenshots of my yt folder\n",
      "no, you are\n",
      "or you can anyway\n",
      "what specific commands did you enter to install yt?\n",
      "pip install yt\n",
      "\n",
      "here are the two pictures\n",
      "\n",
      "ok, it looks like it installed correctly, on windows the compiled code is in the pyd files\n",
      "what errors do you get when you try to import yt?\n",
      "if you can copy/paste them or screenshot them that would help me to see exactly what you’re doing and what’s going wrong\n",
      "so, running pip from the command line give me\n",
      "error: command errored out with exit status 1:\n",
      "     command: 'c:\\users\\alberto\\appdata\\local\\programs\\python\\python38\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'c:\\\\users\\\\alberto\\\\appdata\\\\local\\\\temp\\\\pip-install-k664m7qu\\\\yt\\\\setup.py'\"'\"'; __file__='\"'\"'c:\\\\users\\\\alberto\\\\appdata\\\\local\\\\temp\\\\pip-install-k664m7qu\\\\yt\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'c:\\users\\alberto\\appdata\\local\\temp\\pip-install-k664m7qu\\yt\\pip-egg-info'\n",
      "         cwd: c:\\users\\alberto\\appdata\\local\\temp\\pip-install-k664m7qu\\yt\\\n",
      "    complete output (26 lines):\n",
      "    traceback (most recent call last):\n",
      "      file \"c:\\users\\alberto\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\setuptools\\msvc.py\", line 489, in _find_latest_available_vc_ver\n",
      "        return self.find_available_vc_vers()[-1]\n",
      "    indexerror: list index out of range\n",
      "but running from a different command line works fine\n",
      "which is the yt files i'm using now\n",
      "it looks like setuptools is broken in what you copy/pasted\n",
      "do you have more than one python environment? it sort of sounds like you installed yt with one version of python but are importing it with another\n",
      "just a guess though, it’s hard to debug stuff like this over the internet\n",
      "that's probably the case, i'm trying to get python 3.8 working but maybe the installation was done with my 3.7 environment instead\n",
      "in which case, how should i handle the fact that pip doesn't seem to want to work for python 3.8?\n",
      "worse comes to worse i can just go back to 3.7\n",
      "is what you copy/pasted all you see?\n",
      "pretty much\n",
      "googling your error message leads to threads where there’s also a mention about needing msvc c++ 14.0\n",
      "ah yeah\n",
      "yt doesn’t have python3.8 wheels yet do we\n",
      "ah, well in that case i'll just drop back to 3.7\n",
      "not a big deal\n",
      "so you’re not getting a wheel and pip is dying when it tries to build yt from source because you don’t have the compilers installed\n",
      "if you install the compilers it should work, fwiw\n",
      "gotcha. alright, well, thanks for the help\n",
      "have a nice weekend\n",
      "<https://visualstudio.microsoft.com/visual-cpp-build-tools/|https://visualstudio.microsoft.com/visual-cpp-build-tools/>\n",
      "you too\n",
      "<@u94rac2ah> has joined the channel\n",
      "hi, i'm trying to load gizmo data into yt, but when i try to do anything with the loaded data, i get a zerodivisionerror: float division . here's the full traceback:\n",
      "hi <@u94rac2ah> is this error reproduceable with one of the yt sample datasets?\n",
      "\n",
      "<http://yt-project.org/data/>\n",
      "\n",
      "also do you mind sharing the full script please?\n",
      "sure, here you go.\n",
      "heres a picture of the rest of it. i'm not sure if it's reproducible with yt sample data, i'll try.\n",
      "here's the data set, if that will help\n",
      "ah! what version of yt are you using?   i'm using yt 4.x (which is still in devlepment mode) and am able to get this snapshot to load:\n",
      "\n",
      "```\n",
      "in [4]: ds.field_list\n",
      "yt : [info     ] 2019-03-13 09:35:07,767 allocating for 2.700e+04 particles\n",
      "yt : [info     ] 2019-03-13 09:35:07,767 bounding box cannot be inferred from metadata, reading particle positions to infer bounding box\n",
      "yt : [info     ] 2019-03-13 09:35:07,774 load this dataset with bounding_box=[[-0.99254775 -0.99533606 -0.99219495], [0.99507153 0.99014074 0.9967863 ]] to avoid i/o overhead from inferring bounding_box.\n",
      "out[4]:\n",
      "[('parttype0', 'coordinates'),\n",
      " ('parttype0', 'density'),\n",
      " ('parttype0', 'internalenergy'),\n",
      " ('parttype0', 'masses'),\n",
      " ('parttype0', 'particlechildidsnumber'),\n",
      " ('parttype0', 'particleidgenerationnumber'),\n",
      " ('parttype0', 'particleids'),\n",
      " ('parttype0', 'smoothinglength'),\n",
      " ('parttype0', 'velocities'),\n",
      " ('all', 'coordinates'),\n",
      " ('all', 'density'),\n",
      " ('all', 'internalenergy'),\n",
      " ('all', 'masses'),\n",
      " ('all', 'particlechildidsnumber'),\n",
      " ('all', 'particleidgenerationnumber'),\n",
      " ('all', 'particleids'),\n",
      " ('all', 'smoothinglength'),\n",
      " ('all', 'velocities')]\n",
      "```\n",
      "so i wonder if there was an issue that was resolved in one of the commits between where you are and the code version i'm using\n",
      "oh that\n",
      "that's great! i'm using 3.5.1, and the same thing happened with the yt sample dataset \"gizmo_mhd_mwdisk\"\n",
      "awesome! i don't know what version this issue was fixed in, but the first thing i would try to do is update yt really quickly to see if that fixes it\n",
      "i can also dig up the 4.x installation instructions too\n",
      "yeah, could i have the 4.x installation instructions? 3.5.1 is the latest version, at least according to anaconda\n",
      "i think this is it!\n",
      "\n",
      "<https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb>\n",
      "i think through step 4 is sufficient\n",
      "awesome, thanks so much!!\n",
      "cool hopefully this works let me know!\n",
      "ok, i will after i get through these annoying installation problems...\n",
      "hi - just coming into this discussion now\n",
      "let me try and download your test dataset and see if i can reproduce the issue\n",
      "if this is happening with yt 3.5.1 we should try to fix it there\n",
      "yt 4.0 hasn’t been released yet so i don’t think we should direct people to that in this channel unless they really need it\n",
      "or they’re interested in helping out with development\n",
      "ah i see, so this is the expected behavior in yt 3.x, and it is something that was fixed in 4.0\n",
      "for non-cosmological gadget datasets, you need to specify the bounding box when you load the dataset\n",
      "in yt 4.0 we added a thing to do an i/o pass over the data and compute a sufficiently large bounding box\n",
      "so yeah, in principle we could fix this in master but probably not worth the effort since it would be a fair bit of work\n",
      "it looks like this working in yt 3.x:\n",
      "```\n",
      "ds = yt.load('snapshot_000.hdf5', bounding_box=[[-1, 1], [-1, 1], [-1, 1]])\n",
      "```\n",
      "<@u94rac2ah> ^\n",
      "ah sorry - i had missed that this wasn't a cosmological data set..i should have seen taht\n",
      "awesome!! thanks! \n",
      "hi guys! hopefully a quick one. i am trying to find the percent of the plt file the corresponds to each level of amr. is there a quick way of doing this? i was trying to use `create_profile` to attempt this, but i can’t find a way for the field to be profile to be something like “count” as opposed to a field variable.\n",
      "<@uc6l85lbb> yup, you can probably come up with a quicker estimate (which *includes* overlapping/refined zones) by doing `ds.index.print_stats()` but a profile of `ones` with `weight_field=none` will also show you an unweighted summation\n",
      "(but not overlapping/refined!)\n",
      "you could also look at the `grid_level` field\n",
      "`weight_field=none` means that the profile is a sum over field values\n",
      "and not a weighted average\n",
      "so you could make a 1d profile where the x bins are the `grid_level` field and the profiled field is `ones`\n",
      "oh a profile of ones, that would make sense! how would implement that exactly? right now i have something like : `dx_prof = yt.create_profile(ad, \"dx\", \"x\", weight_field=none, nbins=4, fractional=true)` but what would i change the `\"x\"` to to get ones\n",
      "`yt.create_profile(ad, 'grid_level', 'ones', weight_field=none)`\n",
      "perfect, i will give that a try. thanks!\n",
      "you might need to set `nbins` or the `extrema` to get the binning you want :slightly_smiling_face:\n",
      "probably want to make grid_level be non-logged, too, i imagine ... :slightly_smiling_face:\n",
      "yeah, that too :slightly_smiling_face:\n",
      "`profile.set_log('grid_level', false)`\n",
      "yep! i got those adjustments!\n",
      "hey,\n",
      "i'm trying to install yt-4.0\n",
      "using :\n",
      "```git clone <https://github.com/yt-project/yt.git>\n",
      "cd yt\n",
      "git checkout yt-4.0\n",
      "git pull\n",
      "pip install --user -e .\n",
      "cd ..```\n",
      "i'm attaching the error message that comes when i do the `pip install`\n",
      "any ideas why this is happening?\n",
      "thanks!\n",
      "hi ! i would suggest trying to upgrade conda `conda upgrade conda` and/or pip `conda upgrade pip` though tbf i’m not sure what’s causing your error\n",
      "i am no expert in sfr in yt, but isn't it because the smallest lookback time is `dt/2`? this is because you bin stellar particles by formation time, and the `lookback_time` is the central value of each bin, so anything that formed between `0` and `dt` ago falls in the same bin, which is assigned a `lookback_time` of `dt/2`.\n",
      "there has been a lot of confusion in the trident community about how to best install yt-4 from scratch, so i’m updating the installation instructions for both trident and yt4.  it used to be that yt4 required the `cykdtree` package, where one had to download the github repository and install it locally.  i see there is now a pip package for `cykdtree`.  is that currently a dependency of yt4 or has that code been brought directly into yt?  i just want to know if i should have users install it explicitly.  thanks in advance!\n",
      "or maybe someone has already updated the yt installation docs for the yt-4 branch?  if that’s the case, can someone just point me to them?\n",
      "yeah, as i’m trying to build yt-4 from scratch on a vanilla ubuntu install, i’m getting failures even seemingly unrelated to cykdtree.  so i’d love to chat with people about the proper yt4 installation procedures.  i’ve been going off modifications on what used to be the gold standard for installing yt4, the trident demesh instuctions:  <https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb>\n",
      "is there something messing with the slice plot function?\n",
      "```>sp3 = ds.sphere(center=[0.487410,0.475657,0.497294], radius=(500, \"kpc\"))\n",
      ">proj3 = yt.projectionplot(ds, \"y\", (\"gas\", \"density\"), center=sp3.center, width=(80, \"kpc\"), data_source=sp3)```\n",
      "yields:\n",
      "whereas:\n",
      "```>gas3slice = yt.sliceplot(ds, \"y\", (\"gas\", \"density\"), center=sp3.center, width=(80, \"kpc\"), data_source=sp3)```\n",
      "yields:\n",
      "and this image comes out whether or not im setting \"data_source=sp3\" or using the center as \"center=sp3.center\"\n",
      "and similar things are happening for a lot of other galaxies\n",
      "this is a ramses dataset\n",
      "<@u013buz2xh7> has joined the channel\n",
      "<@u0565nsc0> yeah, right nowit's hard.  i think we can make it better, but i don't know if i have the bandwidth to lead it right now.  want to chat about it a bit and see if it's in your wheelhouse?\n",
      "thank you for addressing the issue!\n",
      "<@umlnt6wds> has joined the channel\n",
      "<@u042fh0rb> is there a way to query a unit registry object for all units of the same dimensionality that already exist?\n",
      "other than iterating and checking if they're equivalent, i mean\n",
      "this seems to work:\n",
      "`[k for k, v in unit_registry.lut.items() if v[1] is dimension]`\n",
      "where `dimension` is imported from yt.units.dimensions\n",
      "ah, that's very concise!  thank you.\n",
      "i’d take a pr that added that as a method on the `unitregistry` class if you want to make that a helper method or something\n",
      "i will indeed do that!\n",
      "<@u046k2qnk> unk, but would be worth doing.  if you have sample data i can whip something up after next weds.\n",
      "oh cool!  i don't yet but am working with mike tremmel to see if he's got any equivalent data sets that are both tipsy and nchilada format (for powderday input) -- i'll definitely pass on when i get them from him!\n",
      "<@ulbk1pvsr> has joined the channel\n",
      "is the image in this paragraph supposed to have a quiver field on it? <https://yt-project.org/doc/visualizing/callbacks.html#off-axis-data-sources>\n",
      "<@u91855pa9> it is and it used to! see <https://yt-project.org/docs/3.4.1/visualizing/callbacks.html#off-axis-data-sources>  looks like a regression\n",
      "<@u0139ssa38f> has joined the channel\n",
      "<@uc6l85lbb> i have just started using yt for the past few weeks. i am trying to extract data points along a line between 2 points\n",
      "```import yt\n",
      "from yt.units import kpc\n",
      "\n",
      "ds = yt.load(\"/users/n22/dummy/precision_4/15/plt-gv\")\n",
      "ds.print_stats()\n",
      "ds.field_list\n",
      "print (ds.field_list)\n",
      "#point_obj = ds.point([10.0, 0.019, 0.95]*kpc)\n",
      "#density_at_point = point_obj['duration']\n",
      "ray = ds.ray((8., 0, 0.8), (9., 0.5, 0.8))\n",
      "print(ray[\"duration\"])\n",
      "#print(density_at_point)```\n",
      "i am not sure what i am doing wrong in here\n",
      "it doesn't print the data. it just says dimensionless.\n",
      "```yt : [info     ] 2020-05-15 09:58:25,582 parameters: current_time              = 30.0\n",
      "yt : [info     ] 2020-05-15 09:58:25,583 parameters: domain_dimensions         = [1001   39   51]\n",
      "yt : [info     ] 2020-05-15 09:58:25,583 parameters: domain_left_edge          = [ 0.  0. -1.]\n",
      "yt : [info     ] 2020-05-15 09:58:25,583 parameters: domain_right_edge         = [20.    0.75  0.  ]\n",
      "level   # grids        # cells       # cells^3\n",
      "----------------------------------------------\n",
      "  0        136         1990989             126\n",
      "----------------------------------------------\n",
      "           136         1990989\n",
      "\n",
      "\n",
      "t = 3.00000000e+01 = 3.00000000e+01 s = 9.50642634e-07 years\n",
      "\n",
      "smallest cell:\n",
      "        width: 6.475e-27 mpc\n",
      "        width: 6.475e-21 pc\n",
      "        width: 1.336e-15 au\n",
      "        width: 1.998e-02 cm\n",
      "[('boxlib', 'g'), ('boxlib', 'v'), ('boxlib', 'duration'), ('boxlib', 'ex'), ('boxlib', 'ey'), ('boxlib', 'ez')]\n",
      "[] dimensionless```\n",
      "i want to extract the data for \"duration\" along a line between 2 points\n",
      "aren’t your points out of the domain?\n",
      "```yt : [info     ] 2020-05-15 09:58:25,583 parameters: domain_left_edge          = [ 0.  0. -1.]\n",
      "yt : [info     ] 2020-05-15 09:58:25,583 parameters: domain_right_edge         = [20.    0.75  0.  ]```\n",
      "\n",
      "ah, my bad. works good. is there a way to print corresponding co-ordinates?\n",
      "what do you mean with “corresponding”? you can use `ds.domain_left_edge` and `ds.domain_right_edge` i believe\n",
      "the ray function spits out the following data. i would like to extract the corresponding x y z co-ordinates for each data point\n",
      "```[1.49536445 1.3892176  1.23081842 1.05004701 0.86141738 0.67979444\n",
      " 0.52100306 0.38255975 0.2683003  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]```\n",
      "<@u0139ssa38f> ah, that's a good question.  the ray will give you the 't' along its length as well, which is the parameter (along the start-to-end, scaled 0 ... 1) at which it enters a given cell.\n",
      "so if you query `x` or `y` or `z` that is the cell *center*, which means they won't necessarily monotonically increase/decrease along the ray, but you can also compute the specific location by taking `t` and multiplying it by the `end - start` and adding `start`\n",
      "ok, let me check. in this case, all i want to do is to check the length of the ray where the data point becomes zero\n",
      "hi, may i ask you a question about the code below? what does this error mean, please?\n",
      "\n",
      "error: the data dict appears to be invalid. the data dictionary must map from field names to (numpy array, unit spec) tuples.\n",
      "\n",
      "and how to correct it?\n",
      "\n",
      "many thanks\n",
      "\n",
      "```import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import array\n",
      "from scipy.interpolate import regulargridinterpolator,interpn  \n",
      "from scipy.integrate import solve_ivp                   # runge-kutta method \n",
      "import matplotlib as mpl\n",
      "from mpl_toolkits.mplot3d import axes3d                 # 3d graph\n",
      "from mpl_toolkits.mplot3d import proj3d                 # 3d graph\n",
      "from matplotlib.patches import fancyarrowpatch\n",
      "import matplotlib.patches as mpatches\n",
      "import math\n",
      "from matplotlib import patches\n",
      "import code\n",
      "import yt\n",
      "from yt import ytarray  # arrays in yt module\n",
      "from yt.visualization.api import streamlines  # force lines\n",
      "import matplotlib.pylab as pl\n",
      "\n",
      "# configuration file\n",
      "from configparser import configparser\n",
      "cfg = configparser()\n",
      "cfg.read('setting.cfg')\n",
      "\n",
      "# font setting\n",
      "import font\n",
      "\n",
      "# constants\n",
      "q     =-1.6e-19      # electron charge\n",
      "m     = 9.1e-31      # electron mass\n",
      "v_par = 2000         # parallel volocity \n",
      "v_per = 2000         # perpendicular velocity\n",
      "num   = 150          # grid resolution \n",
      "\n",
      "x0=np.zeros(6)      # vector - 6 dim, give zeros into it (3 components - positions, 3 components - velocity)\n",
      "\n",
      "def get_b(grid):\n",
      "    b_theta = 0\n",
      "    if grid[2].any() &lt; 0:\n",
      "        b_z = grid[0]\n",
      "        b_r = 0\n",
      "    elif grid[2].any() &lt; np.pi:\n",
      "        b_z = 0.25 * grid[0] * np.cos(grid[2]) + 0.75 * grid[0]\n",
      "        b_r = (0.25/3) * grid[0] * grid[0] * np.sin(grid[2])\n",
      "    else:\n",
      "        b_z = 0.5 * grid[0]\n",
      "        b_r = 0\n",
      "    return (b_r, b_theta, b_z)\n",
      "\n",
      "import numpy as np\n",
      "# grid limits\n",
      "rmin = 0.000001\n",
      "rmax = 1\n",
      "theta_min = 0\n",
      "theta_max = 2*np.pi\n",
      "zmin =0\n",
      "zmax = 2\n",
      "\n",
      "# definition of space\n",
      "r = np.linspace(rmin, rmax, num = num)\n",
      "theta = np.linspace(theta_min, theta_max, num = num)\n",
      "z = np.linspace(zmin, zmax, num = num)\n",
      "\n",
      "b_r, b_theta, b_z = get_b(grid=np.meshgrid(r, theta, z))\n",
      "\n",
      "print(b_r)\n",
      "print(type(b_r))\n",
      "print(b_r.shape)\n",
      "\n",
      "# choose point in field where force line will be integrated\n",
      "r_point = 0.025\n",
      "theta_point = 0.05*np.pi\n",
      "z_point = 0\n",
      "\n",
      "# dictionary of numpy arrays - magnetic field data\n",
      "data = dict(b1=b_r, b2=b_theta, b3=b_z) \n",
      "b_r = data[\"b1\"]\n",
      "b_theta = data[\"b1\"]\n",
      "b_z = data[\"b1\"]\n",
      "\n",
      "print(data)\n",
      "\n",
      "# 3d array of dipole magnetic field \n",
      "bbox = np.array([[-0.15, 0.15], [0, 0.2], [-0.1, 0.1]])                               # border\n",
      "\n",
      "\n",
      "ds = yt.load_uniform_grid(data, b_r.shape, length_unit=\"mpc\", bbox=bbox, nprocs=100) # data, dimenze```\n",
      "in `data = dict(b1=b_r, b2=b_theta, b3=b_z)` your `b_theta` is equal to a scalar 0\n",
      "thank you very much.\n",
      "<@u013rfd5l9y> has joined the channel\n",
      "and this please? i got an error: one of the requested xi is out of bounds in dimension 0.\n",
      "the script creates a magnetic fields and then should compute streamlines with yt in given point, and then, it should compute the trajectory of the particle which initial position and velocities are in the list x0. x0[0] means radius, and this value is negative (row 168). is the problem in the setting of bbox for streamlines? many thanks\n",
      "<https://paste.ofcode.org/iakgwrtftqhu2ppcku7ay2>\n",
      "<@ugpuvhm7w> has joined the channel\n",
      "yes, let's do tuesday 10am pst\n",
      "ok sorry for making this difficult, but i'm tied up at 10am tomorrow too now with a student. after that the quarter is over, though. later tuesday (afternoon) would work for me\n",
      "no worries!  tomorrow later in the day is pretty bad, but weds i could do around 3:15pm\n",
      "er, that's central, which would be 1:15pm your time\n",
      "ok, sounds good. my number is +19167492940 \n",
      "in ytree, is there a way to add a vector analysis fields? i'd like to calculate and save density and temperature profiles for some halos in the arbor, if possible. <@u042s6y2g>\n",
      "hi, i am trying to use feed vtr data into yt which i have read and parsed. the data is from a finite difference cfd simulation, fluid field. the cfd mesh is rectilinear. i have an array of grid point positions and field values at that position. what would be the best way to load the data into yt? i am trying to use load_particles but i get the following error:\n",
      "\n",
      "this is a single uniform resolution grid? if so you want to use `yt.load_uniform_grid`\n",
      "if it’s an amr simulation you want to use `yt.load_amr_grids`\n",
      "sometimes i use grid stretching, so the grid points are not uniformly distributed\n",
      "its not amr\n",
      "ah ok, then `load_hexahedral_mesh`\n",
      "there’s a helper function to calculate the connectivity if you don’t already have it\n",
      "<http://yt-project.org/doc/examining/loading_data.html#semi-structured-grid-data>\n",
      "thanks <@u042fh0rb>\n",
      "hi all, i would like to know if it is possible to plot a line on a phaseplot?\n",
      "i can pass the data to load_hexagonal_mesh fine - but i cannot seem to plot the field properly\n",
      "\n",
      "how can i probe the data stored in ds ?\n",
      "hello all, i have a question regarding assigning a colorbar to an rgb image. if i recall correctly, then whatever i get from the projection operation in yt is an image. and i do not have any issues assigning a colormap to the result. now when reading in an rgb image, what is the best way to assign a color map to this, because the colormap that i plot for it is disconnected from the rgb data. what is the difference between the yt image that i got through projection (proj.to_frb) and the rgb image?\n",
      "<@udn6acn6n> this is 2d data? can you give it some extent in z?\n",
      "<@uaurgnee6> to get an rgb image using a colorbar from a grayscale image, you can do something like this:\n",
      "\n",
      "also sorry but small gripe: screenshots of jupyter notebooks make me sad\n",
      "you can share notebooks with github gists or with nbviewer\n",
      "hello! i was wondering how i could best view a .dat file as an image?\n",
      "also i tried to use yt.load to load a dataset but it doesn't recognise the format? what can i do\n",
      "what sort of data is this?\n",
      ".dat is a file extension, we need a bit more info to help, where did the file come from? what \"style\" of data is it (gridded, particle, unstructured mesh)?\n",
      "when you say you want to view it as an image, you mean to want to make a slice through a 3d grid, or does the original data file represent a 2d image?\n",
      "<@uqannp709> has joined the channel\n",
      "<@u042hlt7u> , is this a bug and should i report this?\n",
      "please do, although i'm not 100% sure it's a bug.  i think a bug report would be a good place to walk through it, however.\n",
      "hi all, i am having a difficult time with gradient fields in yt. particularly, i am looking at the divergence of velocity, and using yt vs. the computed within the code (amrex based). i would expect the fields to be just about exactly the same since both yt and the code use simple centered differences.\n",
      "\n",
      "the first one, divu.png is from the code and the second one, velocity_divergence.png is from yt.\n",
      "any thoughts on what this could be?\n",
      "hi all, how do i change the font size of my legend and axis labels? i found the solution suggested before through the use of the label in my_labels loop. however, it does not work once i use `p.set_unit`. i tried changing the order as well but to no avail. can someone help?\n",
      "`p.set_font(24)`?\n",
      "i think if you do `p._setup_plots()` before applying your customizations it would also work, so long as your customizations are the last thing that happen before you call `p.save()`\n",
      "you can also construct the plot manually using matplotlib and the profile objects, sometimes that’s simpler, `profileplot` is there mostly for making quick plots\n",
      "`athena++` has the ability to output 2-d slices through the simulation domain, in a specific variable (e.g., density).  loading such a slice into `yt` gives a dataset with `ds.domain_dimensions` returning the dimensions of the entire 3-d mesh while `ds.domain_left_edge` and `ds.domain_right_edge` return the full extent of the simulation domain, where really, the slice only contains data through the midplane (z=0) of the simulation.  my question: is there a way to plot the x and y density gradients from such a 2-d slice of density? i tried setting `ds.periodicity = (true, true, true)` but `yt` reports\n",
      "\n",
      "```needsgridtype: (1, [('gas', 'density')]) ```\n",
      "when attempting `yt.sliceplot(ds, 'z', density_gradient_x)` .\n",
      "i think by adding support in yt for data like this, cc <@u042j7xjp>, dunno offhand if there’s a workaround without modifying the athena++ frontend in yt\n",
      "thanks, <@u042fh0rb>~\n",
      "<@uhkuhfybf> is there an example file you can share?\n",
      "attached!\n",
      "\n",
      "<@u0565nsc0> has joined the channel\n",
      "having a little trouble building the docs, not sure what it isn't finding? output: <https://pastebin.com/uljmeguw>, log file: <https://pastebin.com/ypwqrmtv>\n",
      "so it’s dying in the config-help extension, not sure why it’s doing that\n",
      "but you can turn it off in the sphinx conf.py file\n",
      "```\n",
      "extensions = ['sphinx.ext.autodoc', 'sphinx.ext.intersphinx',\n",
      "              'sphinx.ext.mathjax', 'sphinx.ext.viewcode',\n",
      "              'sphinx.ext.napoleon', 'yt_cookbook', 'yt_colormaps',\n",
      "              'config_help', 'yt_showfields']\n",
      "```\n",
      "just delete `config_help`\n",
      "that’s in `doc/source/conf.py`\n",
      "awesome, that worked thanks\n",
      "off to the ncsa staff picnic, if you run into more issues i’ll get back to you eventually :slightly_smiling_face:\n",
      "question about how yt handles memory: if i want to, say, make a projection, do i need to have available at least as much memory as the size of that output? slices presumably take less?---is there a good way to estimate how much memory i’ll need for slice?\n",
      "for enzo data?\n",
      "yes\n",
      "slices need to read in data from the grids the slice intersects with\n",
      "projections need to read in data from every grid (unless you use a `data_source`)\n",
      "right, but does it happen all at once in the projection?\n",
      "no, there’s a loop: <https://github.com/yt-project/yt/blob/master/yt/data_objects/construction_data_containers.py#l336>\n",
      "i’m not sure offhand how much ram the `quadtree` object needs though\n",
      "i guess it might be a lot depending on your amr hierarchy\n",
      "the `quadtree` is a 2d representation of the projected amr hierarchy\n",
      "it’s a resolve-everywhere cosmological box, not a zoom.\n",
      "how many grids are there?\n",
      "`len(ds.index.grids)`\n",
      "lots\n",
      "each chunk in that loop over chunks should be a collection of grids\n",
      "i don’t often work with enzo data that big, it’s possible there’s a scaling issue you’re running into, i’d need to measure\n",
      "it looks like there’s a debug logging statement you could look at which says how much memory the quadtree is using\n",
      "i haven’t even tried to do anything yet because it’s a big dataset and i need to figure out how many nodes, etc., to request.\n",
      "ah i see\n",
      "(waiting on an interactive node to get a more precise estimate of number of grids)\n",
      "yeah, if the interactive node is bigmem you could measure by looking at top\n",
      "i mean, if it has a ton of ram, like 512 gb or something, enough to hold the whole field in memory\n",
      "whatever field you’re projecting\n",
      "oh, no, it doesn’t.\n",
      "oooh that’s a good point though. if i’m just projecting say density, i don’t have to read the entire thing into memory? just the fraction that is density info?\n",
      "yeah, just the one field\n",
      "we also need to have the amr hierarchy in memory\n",
      "but if it’s less than a million grids that doesn’t matter\n",
      "i think it’s several hundred thousand? (i had to increase max-subgrids to 400000) any guess for a regular enzo dataset what the fraction of the total data volume would be one field?\n",
      "it depends on e.g. how many species fields there are\n",
      "maximum a fifth or so i guess?\n",
      "density, vx, vy, vx, internal energy\n",
      "and then however many species fields you have\n",
      "and particle fields too\n",
      ":nod:\n",
      "and if you have a ton of dark matter particles the particle fields can actually be the bulk of the data on-disk\n",
      "i probably do have a ton of dark matter particles\n",
      "many minutes later: i have 261406 grids. but this helps me estimate how much i need; thanks!\n",
      "that’s pretty big :slightly_smiling_face:\n",
      "it just started :smile:\n",
      "projections should be memory conservative. but the large number of grids can be a bottleneck like nathan said. it's possible there is a caching of coordinate info somewhere, which i'll take a look at. that could also lead to memory overhead\n",
      "<@uf6pvg3v4> has joined the channel\n",
      "i am wondering why the points in the plot that do not correspond to my grid domain as showing up in purple and not just transparant. they seem to be set to zero values. my color bar lower range is 265 km/s.\n",
      "any way i can remove this? i tried\n",
      "`s1 = yt.sliceplot(ds_sph, axis='theta', fields='velocity_1')`\n",
      "`s1.set_background_color('velocity_1', color='white')`\n",
      "as the documentation says “if unset, background color is set to the bottom value of the color map”, which seems to correspond with what i have, but that doesn’t seem to do anything though. i don’t have any more ideas to try.\n",
      "<@usj3smag2> interestingly, i *think* that it has changed in very recent versions so that the polar plots have *white* by default for the background.\n",
      "weird. but the set_background_color should normally work?\n",
      "it should, yes -- can you try re-creating this using a dataset formed by `ds = yt.testing.fake_amr_ds(geometry='polar')` (or spherical)?\n",
      "i currently don’t have access to my laptop. i will try when i am home.\n",
      "<@uv0d2uckf> has joined the channel\n",
      "short question…is there an easy way to rotate a projection / slice plot…?\n",
      "you mean at an arbitrary angle, or you just want to flip the axes?\n",
      "for the former, use an off-axis slice/projection plot and set the north_vector\n",
      "for the latter, something like this:\n",
      "```\n",
      "in [1]: ds.coordinates.x_axis['z'] = 1\n",
      "\n",
      "in [2]: ds.coordinates.x_axis[2] = 1\n",
      "\n",
      "in [3]: ds.coordinates.y_axis['z'] = 0\n",
      "\n",
      "in [4]: ds.coordinates.y_axis[2] = 0\n",
      "\n",
      "in [5]: plot = yt.sliceplot(ds, 'z', 'density')\n",
      "\n",
      "in [6]: plot.save()\n",
      "out[6]: ['galaxy0030_slice_z_density.png']\n",
      "```\n",
      "<https://i.imgur.com/ljpof7o.png>\n",
      "here x -&gt; 0, y -&gt; 1, z -&gt; 2\n",
      "<@u8fuk8kcl> ^\n",
      "thanks!\n",
      "does sliceplot have any internal resolution setting?\n",
      "e.g., does it make a frb along the way\n",
      "yes\n",
      "slc.set_buff_size() lets you control the resolution of the frb\n",
      "thanks!\n",
      "slc.frb is a reference to the frb object\n",
      "i'm 12288 zones across\n",
      "so i want more pixels\n",
      "also keep in mind that matplotlib doesn’t let us directly control the resolution of the image you get back from `slc.save()`\n",
      "sure\n",
      "so you also would need to set the dpi (via the `mpl_kwargs` keyword argument to `save`) and the figure size via `slc.set_figure_size`\n",
      "i embiggened the figure\n",
      "or you can just get the image buffer from the frb and handle that manually\n",
      "also mike it’s so effing humid in new york today\n",
      "your state hates me\n",
      "yeah\n",
      "where are you?\n",
      "nyc\n",
      "currently at my grandmother’s apartment\n",
      "(i thought for sure matt would like my embiggen reference)\n",
      "tragically there are no simpsons emojis installed on this slack group  :smile:\n",
      "there is :hypnotoad:\n",
      "<@u04rl26hx> perfectly cromulant reaction emoji\n",
      "<@ud2365cuv> has joined the channel\n",
      "hey <@u042s6y2g>, i wanted to come back to this if you have a chance.  i have a working version of the conversion that steps through tree by tree, then node by node within each tree.  because it’s a triple (quad?) nested for loop though, it’s kinda unbearably slow.\n",
      "\n",
      "i think that at the end of the day what i need is just a way to expose the raw data in all the nodes across all the trees.  is this stored anywhere in ytree?  i can get a list of all the nodes by doing\n",
      "```\n",
      "all_nodes = arbor.select_halos('tree[\"tree\", \"uid\"]&gt;=0')\n",
      "```\n",
      "but this seems hacky and like there should be a better way.  additionally, i’d need to do something like:\n",
      "```\n",
      "vals = np.array([node[key] for node in all_nodes])\n",
      "```\n",
      "in order to build the type of array that i’m looking for.\n",
      "\n",
      "ideally these arrays would be sorted by `uid ` (which does seem unique across the entire arbor), such that `vals[uid]` gives you the value for halo `uid`, but i can handle the sorting if need be.\n",
      "\n",
      "is the data for all of the nodes exposed anywhere in this way?\n",
      "<@u7ku54sg5> i suspect i know what the deal is with this, but won't be able to look for a few hours.  my guess is that it's reading an empty chunk (not sure why) and perhaps it just needs to \"continue\"\n",
      "hi folks, quick question.\n",
      "at one point it was easy to plot a 1d phase plot (i.e., line plot) over a 2d phase plot\n",
      "so one could show, e.g., both the distribution and the mean values\n",
      "i cannot find that in the docs any more.  is this feature gone?\n",
      "i mean, you can always go back to raw matplotlib and do it, but if there’s a very simple way to do it i’m not seeing it\n",
      "i don’t think it was ever documented?\n",
      "i also don’t thin it would be hard to add a helper that does that\n",
      "gotcha, one of my grad students was asking me about it and i wasn’t seeing it anywhere\n",
      "thanks!\n",
      "i’m pretty sure if anyone did it before they did it manually with matplotlib\n",
      "my memory is definitely failing me, then.\n",
      "thanks!\n",
      "i want to do an operation between a ytarray and a numpy.ndarray. i know the units are right. is there a better way than just passing the ytarray to `np.array` to remove the units?\n",
      "<@u91855pa9> there's a shorthand for getting raw ndarray out of ytarray: `.d`\n",
      "`array_name.d`\n",
      "what’s the difference between `.d` and `.v`?\n",
      ".d returns a view\n",
      ".v returns a copy\n",
      "`.d` is short for `.ndarray_view()`\n",
      "and `.v` is short for `.to_ndarray()`\n",
      "i see!\n",
      "<@ubtpbpwcf> has joined the channel\n",
      "<@u8fuk8kcl> has joined the channel\n",
      "<@ubu23q9hv> has joined the channel\n",
      "<@ubu4wppqf> has joined the channel\n",
      "thanks! i think my netcdf4 package is up-to-date. my workaround was indeed to run the script on a machine which did not have netcdf4 installed.\n",
      "<@ucmqab2el> has joined the channel\n",
      "hi all! should i expect to be able to run the clump finder on data from ramses? i'm getting `runtimeerror: amrkdtree does not support particle or octree-based data.` from `find_clumps()`, and before i spend too much time debugging i wanted to check if what i'm trying to do just isn't supported.\n",
      "oh no, that won’t work, sorry :disappointed:\n",
      "one way to do this is to save the data as a covering grid or arbitrary grid and reload that resampled data using `load_uniform_grid`\n",
      "but yes, sorry, the clump finder only works with block amr data (like enzo or flash), not octree amr data\n",
      "the `load_uniform_grid` approach basically reloads your data as a patch amr dataset\n",
      "i think you could use `covering_grid.save_as_dataset` as well\n",
      "britton will probably chime in with ideas :slightly_smiling_face:\n",
      "nathan, do you have a sense for what it would take to make this work straight away with octree data?\n",
      "saving the covering grid with `save_as_dataset` should work for the clump finder, though i haven’t tested it\n",
      "it will definitely be helpful for preserving the unit systems of the dataset\n",
      "ok great, i'll give that a go :slightly_smiling_face: thanks\n",
      "<@u042s6y2g> we need a way to generate ghost zones for octree amr data\n",
      "<@u37dtbl6n> has ideas specifically for ramses data\n",
      "we really need ghost zones because the “block”s are so small, basically the entire volume needs data from neighboring blocks to get decent face-centered data estimates\n",
      "ah, i see, ok\n",
      "in matplotlib i can write multiple plots to a single pdf document using a pdfpages. is there a way to do this with a projectionplot?\n",
      "first i’ve heard of `pdfpages` - maybe i could hack together an example?\n",
      "i was kinda hoping there'd be some underlying matplotlib handle i could grab but i can't quite follow the inheritance structure here. it smells like there's a matplotlib figure or something involved but i can't find it\n",
      "ah cool, this appears to work:\n",
      "```\n",
      "plot = yt.sliceplot(ds, 'z', ['density', 'temperature'])\n",
      "with pdfpages('multipage.pdf') as pdf:\n",
      "    pdf.savefig(figure=plot.plots['density'].figure)\n",
      "    pdf.savefig(figure=plot.plots['temperature'].figure)\n",
      "```\n",
      "<@u91855pa9> ^\n",
      "i was a little worried that it only worked with pyplot, but `pdf.savefig` takes a `figure` keyword argument, so we just need to pass a reference to the matplotlib figure containing the yt plot\n",
      "since this isn’t going through yt’s save function there might be some small amount of rendering weirdness\n",
      "in particular i doubt the math fonts will come out correctly unless you also use the matplotlib style context manager yt uses to override matplotlib’s defaults\n",
      "call you point me to this context manager?\n",
      "it’s `yt.funcs.matplotlib_style_context`, here’s the place in the code it gets used: <https://github.com/yt-project/yt/blob/0cd79d9aefb72abbff465bd22cae3f40b9c49624/yt/visualization/base_plot_types.py#l156>\n",
      "you would say, `with matplotlib_style_context(): pdf.savefig(figure=...)`\n",
      "i wish it wasn’t necessary to do that\n",
      "but unfortunately matplotlib doesn’t have an api to set that without going through the style system\n",
      "<https://github.com/matplotlib/matplotlib/issues/6518>\n",
      "at one point we had a `pdfpages` thing that worked with old-school `plotcollection`, i think.  but it was not as nice as what you're doing here.\n",
      "i'm trying to fiddle around and it's taking an hour to generate a single projectionplot. any suggestions on what i can do to cut that down? doesn't have to be pretty\n",
      "just trying to get a feel for things here\n",
      "use a smaller test dataset?\n",
      "e.g. `isolatedgalaxy`\n",
      "use a region to select only a subset of the data\n",
      "is this an sph dataset?\n",
      "if it is, you could try the yt-4.0 branch\n",
      "which will be like two orders of magnitude faster depending on the data size\n",
      "gizmo so... yes? i'll check out the branch\n",
      "it seems like projectionplot is making a projection of the whole simulation even though i just want 5x5 kpc\n",
      "yeah, then definitely check out yt-4.0 for gizmo processing.  it’s the way to go. <@u91855pa9>\n",
      "if you need help on installing, check out this: <https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb>\n",
      "also see ytep-0032 and my and meagan lang’s scipy 2017 talk\n",
      "we also chat about demeshening stuff in the <#c046hvb59|particles> channel\n",
      "(the scipy talk is on youtube)\n",
      "wow that makes a huge difference\n",
      "pretty much two orders of magnitude\n",
      "1 hour -&gt; 4 minutes\n",
      "<@u91855pa9> i've been using 4.0 for mostly everything (or at least trying to) -- it's great\n",
      "yes, i use yt-4.0 exclusively for my fire analysis.\n",
      "also, for reference, the ytep and scipy talks are linked in the demesh notebook.\n",
      "<@u9ce5d9lz> has a way to make it o(ncpus) faster by using openmp :)\n",
      "thanks for your reply! editing rockstar.py and the rockstar interface worked for me. but yes, yt can't read the particle data.\n",
      "<@u046k2qnk>, pygadgetreader seems to be capable of doing so, thank you for letting me know!\n",
      "awesome glad to hear!\n",
      "when i use np.dot() on two ytarrays the units don't seem to combine correctly. for example, calling <http://np.ar|np.>dot(array_with_velocity_units, array_with_length_units) produces an array with velocity units, not an array with l**2/s. i was wondering what i was doing wrong?\n",
      "nevermind, i think i figured it out!\n",
      "<@uqutv5kgc> has joined the channel\n",
      "<@uf14fnztq> has joined the channel\n",
      "question related to `ds.r`. if i have a simulation that is 64^3 with one level of amr, `ds.covering_grid(level=1, left_edge=[0,0.0,0.0], dims=[128,128,128])` and `ds.r[xlo:xhi:128j, ylo:yhi:128j, zlo:zhi:128j]` should return the same thing, right?\n",
      "<@uc6l85lbb> yes, although there may be very minor differences around single precision level.\n",
      "assuming xlo/xhi etc are 0 and 1, that is.\n",
      "okay, thanks <@u042hlt7u>! i am writing up some code to do an analysis that can change between the two functions simply. this way, i can see if scaling up the number of cores and running out of ram is the actual cause for `ds.r` not giving all the data but also not producing an error.\n",
      "<@uc6l85lbb> ahh, great idea\n",
      "i should note, there is also one very significant difference -- `ds.r` will always generate an `arbitrary_grid` object, which is why it will be different.  `covering_grid` is generated differently.\n",
      "thanks. but the resulting data should be the same, minus some differences about 1e-8 level for single precision and maybe needing to transpose data, right?\n",
      "should be\n",
      "okay cool. i’ll keep you updated!\n",
      "<@ukx34dxt6> has joined the channel\n",
      "do you know an easy way to get planck's constant and the speed of light from yt without having to define them myself?\n",
      "hi <@u012n308pch>,\n",
      "in yt version 3.6, you can get physical constants from yt.units.physical_constants. for instance\n",
      "```import yt\n",
      "h_planck = yt.units.physical_constants.planck_constant_cgs\n",
      "c_light  = yt.units.physical_constants.c```\n",
      "in previous versions, they are in yt.units instead of yt.units.physical_constants\n",
      "alternatively, you can use the standalone unyt module which was derived from yt’s unit system <https://unyt.readthedocs.io/en/stable/>\n",
      "i'm trying to render 2 different fields with different colormaps defined in the transfer function, but the resulting image always seems to use a single colormap\n",
      "here's an image:\n",
      "i made 2 regions, and the density is in region 1 on the left and temperature in region 2 on the right\n",
      "here's the code:\n",
      "any suggestions on how to get it to use different colormaps in each region?\n",
      "nevermind...\n",
      "turns out i didn't understand the colormaps i was using\n",
      "it is doing the proper thing\n",
      "carry on\n",
      "<@u012raulp7t> has joined the channel\n",
      "ah sorry, <@u042fh0rb>, i accidentally signed out of yt slack last week. will try to see if the bug still exists with the test datasets and send along an example script. thanks!\n",
      "hi. i'm currently the only person working on bringing `yt`to `mpi-amrvac`, and the task proved too vast for myself alone given the time i can afford to dedicate to it. i'm planning on selling `yt` out too my dev community next time we gather to discuss our code, so i was wondering if there exists some material designed to that end, like a letter titled _why you should use `yt` and not rewrite the wheel for the nth time in your career\"_ or something... if not at least i know i won't be wasting my time when i write a presentation on this very matter :slightly_smiling_face:\n",
      "\n",
      "of course the website in itself is a wonderful place to start, but i'm looking for something more developer oriented, that will make them feel like it's a burden out of their workflow instead of an additional task they will never complete.\n",
      "a few of us have given public talks about yt with slides available on the web. maybe something like that would be a good thing to show. i think <@u042fh0rb> gave one pretty recently. i have one from a few years ago now. i can try to dig it up.\n",
      "this is quite out of date. i think nathan’s would be better it’s around.\n",
      "<https://figshare.com/articles/the_yt_project/1391957>\n",
      "i don't know how it's out of date but it sure is some quality material, i definitely can use this, thank you very much :ok_hand:\n",
      "<@u042fh0rb> i’ve reported the issue on github using one of the test datasets :slightly_smiling_face:\n",
      "thanks!\n",
      "you’re very welcome. it’s out of date in that there are more supported codes and quite a bit more/improved functionality since i gave this about 3.5 years ago. i think the philosophy is still the same, i.e., that we take many different data formats and allow you to think of physically meaningful structures, etc.\n",
      "i’m uploading my slides, you might also want to look at matt’s paper on community building, which i think is relevant\n",
      "<https://arxiv.org/abs/1301.7064>\n",
      "aaaaah you outquicked me nathan, i was nearly there :smile:\n",
      "you already sent me this a few weeks back, very interesting paper indeed, and very relevant\n",
      "<https://figshare.com/articles/numfocus_summit_talk/7390463>\n",
      "thank you both very much\n",
      "i'll keep you tuned\n",
      "i’ve got some slides for a presentation that i gave for an astro visualization conference 6 months ago.  i’m happy to share them with you if you want.\n",
      "it’s a bunch of cool images and movies, with some explanation as to the methods, so maybe not exactly what you’re looking for.\n",
      "but it’s ~140 mb so let me know if you are interested and i’ll upload them to you.  no worries if not, though.\n",
      "iirc, <@u042hlt7u>’s talk at the 2015 stsci conference mocking the universe <https://webcast.stsci.edu/webcast/detail.xhtml?talkid=4692&amp;parent=1> probably touched on many of these ideas\n",
      "thanks, molly!\n",
      "\n",
      "one thing to note about that talk is that i misspoke and did not mention cameron when i was talking about folks there that had contributed. (i think his name is there in the slides but i messed up with verbal attribution.)\n",
      "<@ud9l1d44t> i'd be happy to share my slides from various talks, or chat, too - i am on a phone but will send them up here when i get out a keyboard. :)\n",
      "sorry to bring this up again after so many weeks, didn't have time to work on it for a while.\n",
      "but i was wondering if you have found any other problems.\n",
      "i'm still not convinced things are working properly. the normalization is still not right, i get 0.6 of cosmic mean at z = 2 and 0.26 at z = 3. i've tested this with both illustris and tng, in both cases taking the smoothing length from (m/rho)^(1/3), and increasing or decreasing the number of grid cells does not make much of a difference\n",
      "hi robin, yes- i have been working on this.\n",
      "\n",
      "my initial results so far seem to suggest `gather` smoothing with `use_sph_normalization = false`  should return answers which are *very* close to the cosmic mean.\n",
      "\n",
      "however, i've identified a few issues with `scatter` but i need to think about the best way to fix them\n",
      "the answer will never be exactly equal to the value from the simulations as the simulations solve an equation iteratively for density and smoothing length (see eq. 3.98 of <https://arxiv.org/pdf/1007.1245.pdf>) we don't do that. we just take the smoothing length to be the distance to the 32nd nearest neighbour. we also use a different kernel. as such, it is not clear the answers should ever perfectly agree.\n",
      "\n",
      "i think if `gather` smoothing is enabled then things should be close.\n",
      "i'm tracking the issue here (<https://github.com/yt-project/yt/issues/2571>) and i've been spending a bit of time on it. switching to eq. 9 of the splash paper may also improve agreement - but i'm yet to implement that\n",
      "thank you for continuing to work on this.\n",
      "although i agree that it will never come out exactly the same as the critical density for the reasons you mentioned, i would expect the difference to be more on the order of a few percent, not factors of ~2-6 that i am still getting with gather smoothing. it's also a bit worrisome that it gets worse with increasing redshift.\n",
      "\n",
      "which simulation are you using for the tests? and have you tried it on multiple redshift snapshots?\n",
      "hey! what is the procedure one should use to make a fixed-grid interpolation of data from an sph simulation?\n",
      "i think typically you use sph `smoothing`  and you can use either the `gather`  technique or the `scatter` . the `gather`  is more sph in nature, e.g., no chance of zero density anywhere but is a little slower. the `arbitrary_grid`  in yt can do this\n",
      "and what if i want to deposit the dark matter particles?\n",
      "oh nevermind, it just works\n",
      "in a smoothed way, or summed?\n",
      "i wanted to do the same as cic deposition for dm particle onto a fixed-resolution mesh for a gadget dataset.\n",
      "but i guess this code does this, doesn't it?\n",
      "```import yt\n",
      "ds = yt.load('/some/path/foo.hdf5')\n",
      "grid = ds.r[::128j, ::128j, ::128j]\n",
      "grid['parttype0', 'particle_mass']```\n",
      "yeah or maybe you can force with `(\"deposit\", \"parttype1_cic\")`\n",
      "following <https://yt-project.org/doc/analyzing/fields.html#deposited-particle-fields>\n",
      "i guess you want `parttype0`  but yeah\n",
      "aren't `parttype0` gas particles?\n",
      "yeah - i just used them as that's what you pasted in your example\n",
      "i guess `(\"deposit\", \"parttype1_cic\")`  is what you want\n",
      "correct, i made a typo... sorry for the confusion. i was inquiring about dm particles, so the `parttype0` should have been `parttype1`\n",
      "btw, it it normal that for `parttype0` (i.e. gas particles), a keyerror is raised when i try to do `grid['deposit', 'parttype0_cic']`, with error message `keyerror: 'deposit is not a sph particle type!'`\n",
      "yeah - i expect that\n",
      "because if we detect a `parttype0`  we default to sph smoothing\n",
      "we should probably add something which allows that to be overridden\n",
      "and lets gas be treated like any other particle if the user really wants that\n",
      "gotcha!\n",
      "well, for sph newbies like me it's probably for the best that you can't easily use gas particles as regular, point-like ones, otherwise i would have just used them this way and would have missed all the amazing stuff you coded for sph particles\n",
      "maybe it would be worth changing the error to\n",
      "```keyerror: \"expected a sph particle type, got `deposit` (from ('deposit', 'parttype0_cic')\"```\n",
      "wdyt?\n",
      "yeah agreed and we should probably add a way to force them to point particles if the user is certain they want that!\n",
      "it just got me super confused because for me `deposit` is _not_ a particle type, so i am not expecting yt to treat it as such.\n",
      "and so i wrongly assumed their was a bug or something, instead of going forth and trying `grid['deposit', 'parttype1_cic']`\n",
      "yeah its a bit wierd in the internals. the `('deposit', 'parttype0_cic')` gets turned into something else and its the something else which is then getting error'd\n",
      "this function is where it happens <https://github.com/yt-project/yt/blob/yt-4.0/yt/data_objects/construction_data_containers.py#l648>\n",
      "it sort of sniffs out a `parttype0`  and then *wrongly* assumes the input is in the format\n",
      "`(parttype0, &lt;some field&gt;)`\n",
      "so another question, how would one deposit dm particle _as_ sph particles?\n",
      "is `grid['parttype1', 'particle_mass']` doing the trick?\n",
      "i think so, but i think you'd have to do this first,\n",
      "```# reload dm particles into a stream dataset\n",
      "ad = ds.all_data()\n",
      "pt = 'parttype1'\n",
      "fields = ['particle_mass'] + [f'particle_position_{ax}' for ax in 'xyz']\n",
      "data = {field: ad[pt, field] for field in fields}\n",
      "ds_dm = yt.load_particles(data, data_source=ad)\n",
      "\n",
      "# generate the missing sph fields\n",
      "ds_dm.add_sph_fields()```\n",
      "following <https://github.com/yt-project/yt/pull/2186>\n",
      "thanks! that seems to be working :slightly_smiling_face:\n",
      "awesome - <@u0860sxlk> worked really hard on this stuff!\n",
      "is there a way to run rockstar halo finder on different snapshots one by one and save the halo catalogs in the same directory, instead of overwriting them?\n",
      "thank you! this is very helpful!\n",
      "<@u010zdngdpa> where did you clone it to? you could check if `sys.path` contains the folder where you installed it to, and if not, append it to it (as a test)\n",
      "<@u042hlt7u> has joined the channel\n",
      "<@u042hlt7u> set the channel purpose: getting help with yt\n",
      "good morning! i want to make a movie that shows two slice plots side-by-side. how can i combine two slice plots into one?\n",
      "<@ulbk1pvsr> my usual way is to use fixed resolution buffers directly and operate on them in matplotlib to create image i want\n",
      "<https://yt-project.org/doc/visualizing/manual_plotting.html#using-the-manual-plotting-interface>\n",
      "<@u042f73r7> thanks!\n",
      "if you already used sliceplot or projectionplot, they also expose the buffer via .frb attribute\n",
      "<@ulld7b8gk> has joined the channel\n",
      "<@u7483nd6y> has joined the channel\n",
      "thank u guys for ur help. i will\n",
      "<@ujt0fc0mr> unfortunately, i don't know the answer to your question. the best to ask is <@u042s6y2g> i guess, though some people may be away during august\n",
      "<@ulqt4pxbp> you can load a uniform grid into yt (see details here <https://yt-project.org/doc/examining/generic_array_data.html>) which you can then use to do volume rendering (see details here <https://yt-project.org/doc/visualizing/volume_rendering.html>)\n",
      "hi <@ujt0fc0mr>, i’m happy to try and help out. i’m around generally but i’m in the uk so there will probably be a delay in getting back to you if it’s late here.\n",
      "as to your question, this is mostly in the domain of rockstar/consistent-trees, but i can relay my experiences. consistent-trees corrects various issues relating the linking of ancestors and descendants, but it can often change the properties of the halos. one thing that might help is that the consistent-trees file should have a field called ‘halo_id’, which is the id of the halo within the rockstar catalog. you should be able to use this to then load that halo catalog and get its position from there. not the best, but i think it’ll work. let me know if you want to talk this over some more.\n",
      "<@ufggcp9r6> has joined the channel\n",
      "does anybody know if xray intensity fields are still supported in “yt.add_xray_emissivity_field” ? in addition to xray emissivity fields, it used to also return “xray_intensity_{e_min}_{e_max}_kev”, but no longer does. has this functionality been moved elsewhere?\n",
      "<@u042j7xjp> ^\n",
      "<@ua068e10d> this only works if you provide a non-zero redshift\n",
      "but i think it's always been that way\n",
      "<http://yt-project.org/doc/analyzing/analysis_modules/xray_emission_fields.html>\n",
      "i see! that makes sense… in retrospect, it only “broke” when i started using a z = 0 output. thanks!\n",
      "in reality it probably makes sense to provide an option to set a distance, so you can do this for an object that isn't necessarily cosmological\n",
      "but we don't have that yet\n",
      "is that module still supported? i thought it had been removed\n",
      "i think you’re thinking of the photon simulator, which is now a separate package\n",
      "it’s now <http://hea-www.cfa.harvard.edu/~jzuhone/pyxsim/>\n",
      "but these xray emission fields are still in yt\n",
      "<@usj3smag2> has joined the channel\n",
      "in yt4.x there seems to be a (derived?) field for enzo `('gas','mass')` which doesn't exist in yt3.x.  however, in yt3.x there is `('gas','matter_mass')`  (which also exists in 4.x).  interestingly, the quantities aren't equivalent to one another.  does anyone know what the difference in the two is?\n",
      "for example:\n",
      "```import yt\n",
      "import numpy as np\n",
      "\n",
      "ds = yt.load('galaxy0030')\n",
      "ad = ds.all_data()\n",
      "print(np.sum(ad[('gas', 'matter_mass')])/np.sum(ad[('gas', 'mass')]))```\n",
      "for the yt example galaxy galaxy0030 results in:\n",
      "```12.784961670719257 dimensionless```\n",
      "\n",
      "`matter_mass` in yt3 is definitely gas+particles and i’m pretty sure `mass` in yt4 is gas mass alias that would alias `cell_mass` in grid codes\n",
      "ah this makes sense -- thanks <@u042s6y2g>!\n",
      "no prob. ok, source code seems to confirm this, too.\n",
      "<@usxcqaylc> has joined the channel\n",
      "i created a pull request for this: <https://github.com/yt-project/yt/pull/2358>\n",
      "<@uqpmcdsf7> has joined the channel\n",
      "<@u042hlt7u> is correct.  you must manually integrate rays by multiplying the path length of each element of a ray (`dts`) by the field in question and then summing.\n",
      "oh! there's also a unit conversion, right? because the sum of dts should be 1.0 regardless of the distance?\n",
      "<@ug37nt2rm> has joined the channel\n",
      "i’m sure there are method to rotate a plot_2d, but all i can find in the cookbook about that relates to 3d volume rendering. could someone point me the right direction ?\n",
      "you mean to flip the x and y axes?\n",
      "try this:\n",
      "```\n",
      "ds.coordinates.x_axis[1] = 0\n",
      "ds.coordinates.x_axis['y'] = 0\n",
      "ds.coordinates.y_axis[0] = 1\n",
      "ds.coordinates.y_axis['y'] = 1\n",
      "```\n",
      "this comes up often enough we should make a cookbook recipe or something\n",
      "ah, that actually answers another question i was wondering, but here i mean an arbitrary angle rotation\n",
      "i.e i would like to replot in a (x’, y’) frame with\n",
      "x’ = x cos(t) + y sin(t)\n",
      "y’ = x sin(t) - y cos(t)\n",
      "hi all, i'm working with an amrex dataset that has temperature, pressure, etc. but no viscosity. i'd like to use cantera through python to calculate viscosity as a derived field in yt. however, i'm running into the issue that `gas.tpx` in cantera expects scalar values for t and p, while i have these as fields in yt (`data[\"temperature\"]`, etc.), which wants derived fields to work as ufuncs according to the documentation. i've tried using `np.vectorize` and `np.frompyfunc` to pass in the t and p fields to cantera, but am getting errors that look like `keyerror: &lt;ufunc '? (vectorized)'&gt;` and `unit_operator = self._ufunc_registry[ufunc]` . any advice on how to best accomplish this? i've never tried something like this so i'm a little lost.\n",
      "can you make a bug report on github?\n",
      "please include a short script we can run to reproduce the error locally\n",
      "if you can, make that script use one of the test datasets on <http://yt-project.org/data|yt-project.org/data>\n",
      "also the derived fields don’t need to be ufuncs themselves\n",
      "fwiw\n",
      "i can probably provide a workaround if i have some code to look at\n",
      "<@ujeeuv7lh> ^\n",
      "<@u042fh0rb> yeah let me see if i can reproduce it with one of the test datasets and i'll post some simplified code to github.\n",
      "<@uba7lu7uj> has joined the channel\n",
      "thanks! neutrinoceros (and awesome name!)\n",
      "anytime :smile:\n",
      "it looks like `ds.covering_grid` and `ds.smoothed_covering_grid` aren't implemented for python3 yet?\n",
      "\n",
      "i get a \"notimplemented\" error for the following code with python3 both yt 3.5.1 and 3.6.0 (just installed 3.6.0 to check it). on the bright side i still have yt 3.2.1 for python2 which works :)\n",
      "\n",
      "`ds = yt.load(filename)`\n",
      "`level = 4`\n",
      "`data = ds.smoothed_covering_grid(level, left_edge=ds.domain_left_edge,`\n",
      "                 `dims=ds.domain_dimensions*2**level)`\n",
      "`#data = ds.covering_grid(level, left_edge=ds.domain_left_edge,` \n",
      "`#            dims=ds.domain_dimensions*2**level)`\n",
      "is it possible to view the web documentation from an older version? or would i need to read source code?\n",
      "the docs from the main release versions are on the site, e.g. <https://yt-project.org/docs/3.1/>, just replace it with x.y if you're looking for a specific version.\n",
      "<@ume7t58a2> has joined the channel\n"
     ]
    }
   ],
   "source": [
    "for m in ts_id_dict.values():\n",
    "    print(m[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('powderday', \"in `` i'm trying to write code that will automagically check what `yt` version we're on, and then deal with the octrees accordingly.   is this reasonable as a check, or is there a better way to discern (in code) if we're on 3.x or 4.x?\\n\\n\\n```if  yt.__version__ == '4.0.dev0':\\n   blah```\")\n"
     ]
    }
   ],
   "source": [
    "test = user_text[2]\n",
    "test1 = get_code(test, '`', '`', '`')\n",
    "# print(test)\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['powderday']\n"
     ]
    }
   ],
   "source": [
    "print(all_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[('3.x or 4.x', 'DATE')]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('LooseVersion', 'ORG')]\n",
      "[]\n",
      "[('the Rockstar Halo Finder', 'FAC'), (\"the Rockstar User's Guide\", 'ORG')]\n",
      "[('Benedikt', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('gas_tracer', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('MeshIdentifier', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('MeshIdentifier', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('```\\n&gt', 'ORG'), ('mp/cm**3', 'ORG'), ('Npart=11180202 Ncell=1089077', 'PERCENT'), ('Npart=11180202 Ncell=1089077', 'TIME'), ('Npart=11180202 Ncell=1089077', 'TIME'), ('Npart=11180202 Ncell=1089077', 'WORK_OF_ART'), ('2.52232177e-05', 'TIME'), ('2.57807361e-05', 'DATE'), ('8.77901157e-05', 'MONEY'), ('8.77901157e-05', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('gas_tracer', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('0.18909443', 'CARDINAL')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('789', 'CARDINAL'), ('max\\n\\xa0\\xa0rv', 'PERSON'), ('File', 'PERSON'), ('755', 'CARDINAL'), ('542', 'CARDINAL'), ('self).__call__(fields', 'PERSON'), ('non_zero', 'GPE'), ('69', 'CARDINAL'), ('549', 'CARDINAL'), ('process_chunk', 'GPE'), ('1165', 'DATE'), ('determine_fields\\n\\xa0\\xa0', 'LAW'), ('798', 'CARDINAL'), ('redshift0045', 'GPE')]\n",
      "[('Shai', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot.set_antialias', 'PERSON')]\n",
      "[('False', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('https://pastebin.com/Czbb9nWB', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[('100644', 'DATE'), ('self.field_parameters[param', 'PRODUCT'), ('self.requested_parameters.append(param', 'ORG'), ('self.ds.arr', 'GPE'), ('self.ds.arr', 'PERSON'), ('KeyError', 'ORG'), ('self.ds.arr(np.random.random(3', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('vy', 'PERSON'), ('vz', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('create_vector_fields', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2D', 'CARDINAL'), ('4', 'CARDINAL'), ('zgrid =', 'PERSON')]\n",
      "[('<@UDN6ACN6N', 'ORG')]\n",
      "[('3d', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('RGBA', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[]\n",
      "[('img_ex', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('scipy years ago', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('https://youtu.be/ywHqIEv3xXg?t=1938', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.5', 'CARDINAL')]\n",
      "[('2013', 'DATE')]\n",
      "[]\n",
      "[('2.7', 'CARDINAL'), ('3.5', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL'), ('2.7 one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('setup.py', 'ORG')]\n",
      "[]\n",
      "[('five years', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UDN6ACN6N', 'ORG')]\n",
      "[]\n",
      "[('<@UDN6ACN6N', 'ORG')]\n",
      "[]\n",
      "[('https://www.vtk.org/wp-content/uploads/2015/04/file-formats.pdf', 'ORG')]\n",
      "[('Athena', 'PERSON')]\n",
      "[]\n",
      "[('second', 'ORDINAL'), ('a month', 'DATE'), ('CFD', 'ORG')]\n",
      "[('VTK', 'ORG'), ('https://www.kitware.com/products/books/VTKTextbook.pdf', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('t_ff', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[('MPL', 'ORG')]\n",
      "[('ytree/rockstar/consistent trees', 'ORG'), ('a[#][\"prog', 'WORK_OF_ART'), ('TreeNode', 'WORK_OF_ART'), ('TreeNodes', 'NORP')]\n",
      "[('Numpy', 'PRODUCT'), ('1', 'CARDINAL')]\n",
      "[('<@UHWBQ8VPB', 'ORG')]\n",
      "[('ray.integrate', 'WORK_OF_ART')]\n",
      "[('<@UBJU11GJU', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('load_particles', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@UH593L8TU', 'PERSON'), ('last week', 'DATE')]\n",
      "[('2.91482979e-01', 'DATE'), ('7.08714933e-01', 'QUANTITY'), ('-4.43966940e-01', 'GPE'), ('5.31101502e-01', 'QUANTITY'), ('8.47308108e-01', 'CARDINAL'), ('data.has_field_parameter(\"rotation_vectors', 'EVENT'), ('2019-03-26 14:26:14,139', 'DATE'), ('2019-03-26', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Gaia', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt.load_particles', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('smoothing_length', 'ORG'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[('yt.load', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[('@UH593L8TU', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@UH593L8TU', 'PERSON'), ('GIZMO', 'ORG'), ('SPH', 'ORG'), ('gather', 'PERSON'), ('@U9CE5D9LZ', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AREPO', 'ORG'), ('SPH', 'ORG')]\n",
      "[('GIZMO', 'ORG')]\n",
      "[]\n",
      "[('@UH593L8TU', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[('Haha', 'ORG')]\n",
      "[('@UH593L8TU', 'PERSON')]\n",
      "[('0.0', 'CARDINAL'), ('0.0', 'CARDINAL'), ('0.99', 'CARDINAL'), ('0.99', 'CARDINAL'), ('0.99', 'CARDINAL'), ('dims=[128', 'DATE'), ('128', 'DATE'), ('128', 'CARDINAL'), ('135', 'CARDINAL'), ('128', 'CARDINAL'), ('3', 'CARDINAL')]\n",
      "[('left_edge', 'ORG'), ('right_edge', 'WORK_OF_ART'), ('1', 'CARDINAL')]\n",
      "[('<@UC6L85LBB', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('```File \"', 'ORG'), ('30', 'CARDINAL'), ('283', 'CARDINAL'), ('520', 'CARDINAL'), ('284', 'CARDINAL'), ('725', 'CARDINAL'), ('tag=0', 'DATE'), ('1156', 'DATE'), ('174', 'CARDINAL'), ('mpi4py', 'GPE'), ('File \"mpi4py/', 'PERSON'), ('91', 'CARDINAL'), ('1040', 'DATE'), ('n', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG'), ('AMReX.', 'ORG'), ('AMReX', 'ORG'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG'), ('vertex/cell/face/edge', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('AMReX', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Eq', 'PERSON'), ('9', 'CARDINAL'), ('tomorrow', 'DATE')]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('get(code_length', 'ORG')]\n",
      "[]\n",
      "[('0', 'CARDINAL'), ('slc =', 'PERSON'), ('SlicePlot(ds', 'ORG'), ('width=(20', 'ORG')]\n",
      "[]\n",
      "[('ParaView', 'ORG'), ('first', 'ORDINAL'), ('second', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[('out.e', 'DATE')]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('FYI', 'ORG'), ('two', 'CARDINAL')]\n",
      "[]\n",
      "[('code_length', 'WORK_OF_ART')]\n",
      "[('code_length', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('ds.quan(1', 'GPE')]\n",
      "[('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Tanfastic', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('vy', 'PERSON'), ('``\\n\\ncauses problems downstream (in, e.g., projections) unless you', 'WORK_OF_ART'), ('YTArray', 'ORG'), ('vy', 'PERSON')]\n",
      "[('YTArray', 'ORG'), ('YTArray([vx', 'GPE'), ('vy', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U043BNA00', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('<@UBJU11GJU', 'ORG'), ('PR', 'ORG')]\n",
      "[('Miniconda', 'ORG'), ('h5py', 'ORG'), ('119', 'CARDINAL'), ('299', 'CARDINAL'), ('107', 'CARDINAL'), ('317', 'CARDINAL'), ('finalize_options', 'FAC'), (\"LooseVersion('1.10.4\", 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('today', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Caltech', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('One', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Miniconda', 'ORG'), ('Installing h5py', 'WORK_OF_ART'), ('REPORT', 'PERSON'), ('119', 'CARDINAL'), ('299', 'CARDINAL'), ('107', 'CARDINAL'), ('317', 'CARDINAL'), ('finalize_options', 'FAC'), (\"LooseVersion('1.10.4\", 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2.7', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('```\\n&gt;&gt;&gt', 'ORG')]\n",
      "[('chummels@wheeler', 'PRODUCT'), ('Loaded Modulefiles', 'PERSON'), ('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('today', 'DATE')]\n",
      "[]\n",
      "[('today', 'DATE')]\n",
      "[]\n",
      "[('Cluster', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('e.g. install', 'PERSON'), ('miniconda3', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[('first', 'ORDINAL')]\n",
      "[('anaconda manually', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Trident/yt-4.0', 'ORG'), ('install_script', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('pip install', 'PERSON')]\n",
      "[('miniconda', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('numpy.py', 'ORG')]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[('install_script', 'ORG')]\n",
      "[]\n",
      "[('FRB', 'ORG'), ('PhasePlot', 'WORK_OF_ART'), ('PlotWindow', 'ORG'), ('PhasePlot', 'WORK_OF_ART'), ('Matplotlib', 'ORG')]\n",
      "[('FRB', 'ORG'), ('PhasePlot', 'WORK_OF_ART')]\n",
      "[('Gotcha', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('<@UDDTT4WSZ', 'ORG')]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[('<http://yt-project.org/docs/dev/examining/spherical_data.html', 'ORG')]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UBJU11GJU', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt.add_field', 'PERSON')]\n",
      "[('pressure1', 'WORK_OF_ART'), ('ds.derived_field_list', 'GPE'), ('ds.derived_field_list', 'GPE'), ('ds.add_field((\"gas\",\"pressure3', 'GPE'), ('ds.derived_field_list', 'GPE')]\n",
      "[('pressure2', 'WORK_OF_ART'), ('pressure1', 'PRODUCT'), ('pressure3', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('<@ULDDR26EL', 'ORG')]\n",
      "[('yt.off_axis_projection', 'CARDINAL'), ('bitmap', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[('150', 'CARDINAL'), ('kpc', 'ORG'), ('75', 'CARDINAL'), ('75', 'DATE'), ('snapshot_172.0.hdf5', 'ORG'), ('print(np.sort(distance_components.flatten', 'PERSON')]\n",
      "[('32.5', 'CARDINAL'), ('32.5', 'CARDINAL'), ('32.5', 'CARDINAL'), ('0', 'CARDINAL'), ('0', 'CARDINAL')]\n",
      "[('SPH', 'ORG'), ('2', 'CARDINAL'), ('Gadget', 'ORG'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UGS4TDHQF', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('save_as_dataset', 'PERSON'), ('``\\n', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('2D', 'CARDINAL'), ('AMR', 'ORG')]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3', 'CARDINAL'), ('ProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[('CIC', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[('> morning', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('matt', 'PERSON')]\n",
      "[]\n",
      "[('<@UC6L85LBB', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot(ds', 'GPE'), ('normal=[0,1,0', 'ORG')]\n",
      "[('@U8FUK8KCL', 'ORG')]\n",
      "[('Vadlamani', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('arepo', 'PERSON'), ('TNGHalo', 'WORK_OF_ART'), ('arepo', 'PERSON'), ('``\\n', 'WORK_OF_ART'), ('5', 'CARDINAL'), ('2019-05-03', 'DATE'), ('1.000e+00', 'CARDINAL'), ('4.356e+17 seconds', 'TIME'), ('2019-05-03 15:08:42,039', 'DATE'), ('kpc', 'ORG'), ('2019-05-03', 'DATE'), ('Parameters', 'NORP'), ('4.355810528213309e+17 s', 'QUANTITY'), ('2019-05-03', 'DATE'), ('Parameters', 'NORP'), ('2019-05-03', 'DATE'), ('Parameters', 'NORP'), ('2019-05-03', 'DATE'), ('Parameters', 'NORP'), ('205000', 'CARDINAL'), ('205000', 'CARDINAL'), ('2019-05-03', 'DATE'), ('Parameters', 'NORP'), ('1', 'CARDINAL'), ('2019-05-03', 'DATE'), ('Parameters', 'NORP'), ('2019-05-03', 'DATE'), ('Parameters', 'NORP'), ('2019-05-03', 'DATE'), ('Parameters', 'NORP'), ('0.3089', 'CARDINAL'), ('2019-05-03', 'DATE'), ('Parameters', 'NORP'), ('0.6774', 'CARDINAL'), ('6', 'CARDINAL')]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[('4', 'CARDINAL')]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('arepo', 'PERSON'), ('yt-4.0', 'NORP')]\n",
      "[('this summer', 'DATE'), ('yt4.0', 'GPE')]\n",
      "[('only 4.0', 'CARDINAL')]\n",
      "[]\n",
      "[('2D', 'QUANTITY'), ('``RuntimeError: Error: yt attempted', 'WORK_OF_ART'), ('0', 'CARDINAL'), ('```grad_defined_region = ds.r[1:9, 1:9, 1:9]  ', 'WORK_OF_ART'), ('0 to 10', 'CARDINAL'), ('Dataset.r', 'CARDINAL'), ('2D', 'CARDINAL'), ('``YTDimensionalityError: Dimensionality', 'WORK_OF_ART'), ('3', 'CARDINAL'), ('2', 'CARDINAL'), ('``\\nmeanwhile', 'WORK_OF_ART'), ('2d', 'CARDINAL'), ('1:9', 'CARDINAL'), ('2', 'CARDINAL'), ('3', 'CARDINAL')]\n",
      "[('1:9', 'CARDINAL'), ('3rd', 'ORDINAL')]\n",
      "[('the last couple days', 'DATE'), ('Blue Waters', 'ORG')]\n",
      "[('ytree', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('cantera', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('TPX', 'ORG'), ('1000', 'CARDINAL'), ('Python', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AttributeError', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('``def _', 'WORK_OF_ART'), ('nu(field', 'PERSON'), ('2', 'CARDINAL'), ('1', 'DATE'), ('dyn_visc', 'GPE'), ('dyn_visc', 'PERSON'), ('dyn_visc', 'PERSON')]\n",
      "[('myarray.view(numpy.ndarray', 'PERSON')]\n",
      "[]\n",
      "[('mass/(length*time', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('3.6.dev0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('https://github.com/yt-project/yt/blob/master/yt/frontends/artio/fields.py#L34', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('like 90%', 'PERCENT'), ('slightly_smiling_face', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot', 'ORG'), ('6.35001', 'CARDINAL'), ('3.8100001', 'CARDINAL'), ('240', 'CARDINAL'), ('int(periodic', 'ORG'), ('64', 'CARDINAL'), ('np.float64_t', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('friday', 'DATE'), ('today', 'DATE')]\n",
      "[('``def get_mu(Temp,Pres', 'WORK_OF_ART'), ('Solution(\"air.cti', 'CARDINAL'), ('TPX', 'ORG'), ('O2:1.0', 'PERSON'), ('nu(field', 'PERSON'), ('np.frompyfunc(get_mu', 'ORG'), ('2', 'CARDINAL'), ('1', 'DATE'), ('vfunc(data[\"Temp\"].d', 'CARDINAL'), ('dyn_visc', 'PERSON'), ('``ds.add_field((\"gas\",\"nu', 'WORK_OF_ART'), ('function=_nu', 'ORG'), ('sampling_type=\"cell', 'PERSON'), ('SlicePlot', 'ORG'), ('6.35001', 'CARDINAL'), ('3.8100001', 'CARDINAL')]\n",
      "[('later tonight', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[(\"ProjectionPlot(ds,'z',('gas','density'),center='m',width=(50,'kpc\", 'PERSON'), ('DM', 'ORG')]\n",
      "[]\n",
      "[('yt4.x', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('Japan', 'GPE'), ('the week', 'DATE'), ('select_halos', 'GPE'), ('this week', 'DATE'), ('select_halos', 'ORG'), (\"``\\n    tsize = np.array([t['tree'].size\", 'WORK_OF_ART'), ('0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('all day', 'DATE'), ('tomorrow', 'DATE'), ('US', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Two', 'CARDINAL'), ('particle_type', 'ORG'), ('2', 'CARDINAL'), ('2', 'CARDINAL')]\n",
      "[('second', 'ORDINAL'), ('two', 'CARDINAL'), ('particle_type', 'ORG'), ('Enzo', 'LOC'), ('particle_type', 'ORG'), ('particle_type', 'ORG'), ('field_type', 'ORG'), ('ptype=\"stars', 'WORK_OF_ART')]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL'), ('<http://use.yt/upload/99e20269', 'CARDINAL'), ('one', 'CARDINAL'), ('yt-4.0', 'NORP'), ('SPH', 'ORG'), ('yt.load(dsname', 'PERSON'), ('needed_ptype', 'ORG'), ('310', 'CARDINAL'), ('np.empty(3', 'QUANTITY'), ('311', 'CARDINAL'), ('312', 'CARDINAL'), ('313', 'CARDINAL'), ('314', 'CARDINAL'), ('zero', 'CARDINAL')]\n",
      "[('z&gt;2', 'ORG')]\n",
      "[('cm**2', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('about 4 years ago', 'DATE')]\n",
      "[('GPL', 'ORG'), ('GPL', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('WIP', 'ORG')]\n",
      "[('repo', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[('@UJN82HE20', 'ORG')]\n",
      "[('yt.load_uniform_grid', 'ORG'), ('cosmological_parameters', 'ORG')]\n",
      "[('1', 'CARDINAL'), ('Mpccm', 'WORK_OF_ART')]\n",
      "[('@U8FUK8KCL', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Hi Sam', 'PERSON'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('arrays', 'DATE')]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('ValidateSpatial', 'ORG'), ('3D', 'CARDINAL')]\n",
      "[('1', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('<@UEJKUJ2DB', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('@UCYBQ5KPA', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMRVAC', 'ORG'), ('2D', 'CARDINAL'), ('second', 'ORDINAL')]\n",
      "[('AMRVAC', 'ORG'), ('AMRVAC', 'ORG'), ('Cartesian', 'NORP'), ('yt.load(\"your', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('FYI', 'ORG'), ('non Cartesian', 'NORP')]\n",
      "[]\n",
      "[('ART', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yup', 'ORG'), ('a few years', 'DATE')]\n",
      "[]\n",
      "[('3D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('1', 'CARDINAL'), ('0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<https://github.com/yt-project/yt/blob/master/yt/fields/field_detector.py#L126', 'PERSON')]\n",
      "[]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('DXL', 'ORG')]\n",
      "[('test_yt_works.py', 'CARDINAL'), ('Dropbox', 'PERSON'), ('about 275 Mb', 'CARDINAL'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[('this today', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON'), ('14', 'CARDINAL')]\n",
      "[]\n",
      "[('ProfilePlot(ds', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('second', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('Trident', 'PRODUCT'), ('Lagrangian', 'NORP'), ('626', 'CARDINAL'), ('627 seconds', 'TIME'), ('h5py', 'ORG'), ('<5 seconds', 'TIME'), ('Trident', 'PRODUCT'), ('1.3.dev1', 'CARDINAL'), ('4.0.dev0', 'CARDINAL'), ('947ebdc22c48', 'CARDINAL')]\n",
      "[]\n",
      "[('all_data', 'GPE')]\n",
      "[('h5py', 'ORG'), ('h5py', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('Trident', 'WORK_OF_ART'), ('Lagrangian', 'NORP'), ('ds.ray', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('cProfile', 'ORG')]\n",
      "[]\n",
      "[('cProfile', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('1e12', 'CARDINAL'), ('L*', 'PRODUCT')]\n",
      "[]\n",
      "[('bitmap', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[('matt', 'PERSON')]\n",
      "[]\n",
      "[('create_averaged_field', 'WORK_OF_ART')]\n",
      "[('Simon', 'PERSON'), ('3D', 'CARDINAL'), ('nx-2', 'GPE'), ('ny-2', 'GPE'), ('nz-2', 'GPE')]\n",
      "[('cell_mass', 'ORG')]\n",
      "[]\n",
      "[('ValidateSpatial', 'ORG'), ('3D', 'CARDINAL')]\n",
      "[('YT', 'ORG')]\n",
      "[('first', 'ORDINAL')]\n",
      "[('3.5', 'CARDINAL'), ('4.0', 'CARDINAL'), ('4.0', 'CARDINAL')]\n",
      "[('GIZMO', 'ORG'), ('3.5.1', 'LAW')]\n",
      "[('3.5.1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('sampling_type=\"particle', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('yt.funcs.mylog.setLevel(SOME_HIGH_NUMBER_HERE', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProfilePlot(ad', 'PERSON'), ('plot_spec', 'ORG'), (\"dict(color='red\", 'ORG'), ('####', 'MONEY'), ('####', 'MONEY')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('mpl_kwargs', 'WORK_OF_ART')]\n",
      "[('yt.units import Msun', 'ORG'), ('ProfilePlot(ad', 'PERSON'), ('weight_field= None', 'PERSON'), ('accumulation= True', 'PERSON'), ('####', 'MONEY'), ('####', 'MONEY')]\n",
      "[('yt.units import Msun', 'ORG'), ('ProfilePlot(ad', 'PERSON'), ('####', 'MONEY'), ('####plot.set_unit(\"cell_mass\",\"Msun', 'MONEY')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('sec', 'ORG')]\n",
      "[('ProfilePlot(ad', 'PERSON'), ('weight_field= None', 'PERSON'), ('accumulation= True', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('set_xlim(1e8,1e7', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('10', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('xlim(1e8,1e7', 'ORG')]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('between 1 and 10', 'CARDINAL')]\n",
      "[('ylim', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[('Matt', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProfilePlot', 'ORG'), ('sec', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProfilePlot', 'ORG'), ('#', 'CARDINAL'), ('prof[(\"gas', 'CARDINAL')]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[('@U011Y3XM493', 'PERSON')]\n",
      "[('ur time', 'ORG')]\n",
      "[]\n",
      "[('np', 'ORG'), ('create_profile', 'GPE'), ('number_density', 'WORK_OF_ART'), ('#', 'CARDINAL'), ('cell_mass', 'ORG'), ('#', 'CARDINAL'), ('cell_mass', 'ORG'), ('np.flipud(data', 'GPE'), ('#', 'CARDINAL'), ('#', 'CARDINAL'), ('Plot', 'ORG'), ('ProfilePlot(prof_ds.data', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('over_refine_factor &gt', 'ORG'), ('1', 'CARDINAL'), (\"yt.load('snapshot_094.hdf5',over_refine_factor=0\", 'CARDINAL')]\n",
      "[]\n",
      "[('ds.octree(left', 'PRODUCT'), ('``\\n\\n', 'WORK_OF_ART')]\n",
      "[('SlicePlot', 'PRODUCT'), ('2407793', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.5.1', 'CARDINAL')]\n",
      "[]\n",
      "[('5', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[('http://yt-project.org/doc/installing.html#installing-yt-from-source', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[('<@UC6L85LBB', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('<@UC6L85LBB', 'ORG')]\n",
      "[(\"a[0]['tree\", 'PERSON'), ('342', 'CARDINAL'), ('342', 'CARDINAL')]\n",
      "[]\n",
      "[('YTArray', 'ORG'), ('units=', 'ORG'), (\"a[0]['tree\", 'PERSON'), ('np.arange(a.size', 'CARDINAL')]\n",
      "[('Cartesian', 'NORP'), ('YT Disk', 'ORG'), ('0.0', 'CARDINAL'), ('0.0', 'CARDINAL'), ('1.0', 'CARDINAL'), ('8', 'CARDINAL'), ('5', 'CARDINAL'), ('```\\n\\nBut the bounds of my profile', 'WORK_OF_ART'), ('0', 'DATE'), ('8', 'CARDINAL'), ('``\\n\\n```\\nYTArray([14.27000956', 'WORK_OF_ART'), ('14.3696463', 'CARDINAL'), ('14.46997873', 'CARDINAL'), ('14.57101171', 'CARDINAL'), ('14.67275012', 'CARDINAL'), ('14.7751989', 'CARDINAL'), ('14.87836299', 'CARDINAL'), ('14.98224741', 'CARDINAL'), ('15.08685717', 'CARDINAL'), ('15.19219734', 'CARDINAL'), ('15.29827303', 'CARDINAL'), ('15.40508936', 'CARDINAL'), ('15.51265151', 'CARDINAL'), ('15.62096468', 'CARDINAL'), ('15.73003413', 'CARDINAL'), ('15.83986512', 'CARDINAL'), ('15.95046298', 'CARDINAL'), ('16.06183306', 'CARDINAL'), ('16.17398076', 'CARDINAL'), ('16.2869115', 'CARDINAL'), ('16.40063075', 'CARDINAL'), ('16.51514402', 'CARDINAL'), ('16.63045684', 'CARDINAL'), ('16.74657481', 'CARDINAL'), ('16.86350354', 'CARDINAL'), ('16.9812487', 'CARDINAL'), ('17.09981598', 'CARDINAL'), ('17.21921113', 'CARDINAL'), ('17.33943993', 'CARDINAL'), ('17.46050819', 'CARDINAL'), ('17.58242179', 'CARDINAL'), ('17.70518661', 'CARDINAL'), ('17.82880861', 'CARDINAL'), ('17.95329376', 'CARDINAL'), ('18.07864811', 'CARDINAL'), ('18.20487771', 'CARDINAL'), ('18.33198867', 'CARDINAL'), ('18.45998715', 'CARDINAL'), ('18.58887936', 'CARDINAL'), ('18.71867152', 'CARDINAL'), ('18.84936992', 'CARDINAL'), ('18.98098088', 'CARDINAL'), ('19.11351079', 'CARDINAL'), ('19.24696606', 'CARDINAL'), ('19.38135314', 'CARDINAL'), ('19.51667855', 'CARDINAL'), ('19.65294883', 'CARDINAL'), ('19.79017059', 'CARDINAL'), ('19.92835046', 'CARDINAL'), ('20.06749514', 'CARDINAL'), ('20.20761136', 'CARDINAL'), ('20.34870591', 'CARDINAL'), ('20.49078561', 'CARDINAL'), ('20.63385735', 'CARDINAL'), ('20.77792805', 'CARDINAL'), ('20.92300469', 'CARDINAL'), ('21.06909429', 'CARDINAL'), ('21.21620393', 'CARDINAL'), ('21.36434072', 'CARDINAL'), ('21.51351184', 'CARDINAL'), ('21.6637245', 'CARDINAL'), ('21.81498599', 'CARDINAL'), ('21.96730363', 'CARDINAL'), ('22.12068478', 'CARDINAL'), ('cylindrical_radius', 'ORG'), ('``\\n\\n```\\nYTArray([0.06382166', 'WORK_OF_ART'), ('0.06904446', 'CARDINAL'), ('0.07469467', 'CARDINAL'), ('0.08080727', 'CARDINAL'), ('0.08742008', 'GPE'), ('0.09457405', 'MONEY'), ('0.10231346', 'GPE'), ('0.11068622', 'CARDINAL'), ('0.11974416', 'GPE'), ('0.12954336', 'GPE'), ('0.14014446', 'MONEY'), ('0.1516131', 'GPE'), ('0.16402026', 'CARDINAL'), ('0.17744276', 'GPE'), ('0.19196368', 'CARDINAL'), ('0.20767292', 'MONEY'), ('0.2246677', 'GPE'), ('0.24305325', 'CARDINAL'), ('0.26294337', 'GPE'), ('0.28446118', 'CARDINAL'), ('0.30773989', 'MONEY'), ('0.3329236', 'CARDINAL'), ('0.3601682', 'DATE'), ('0.38964235', 'DATE'), ('0.4215285', 'DATE'), ('0.45602403', 'MONEY'), ('0.49334248', 'GPE'), ('0.53371486', 'GPE'), ('0.5773911', 'GPE'), ('0.62464155', 'GPE'), ('0.67575872', 'MONEY'), ('0.73105903', 'GPE'), ('0.7908848', 'GPE'), ('0.85560638', 'CARDINAL'), ('0.92562442', 'DATE'), ('1.00137233', 'MONEY'), ('1.08331904', 'CARDINAL'), ('1.1719718', 'CARDINAL'), ('1.2678794', 'CARDINAL'), ('1.37163555', 'GPE'), ('1.48388252', 'MONEY'), ('1.60531516', 'CARDINAL'), ('1.73668516', 'CARDINAL'), ('1.87880574', 'CARDINAL'), ('2.03255668', 'CARDINAL'), ('2.19888973', 'MONEY'), ('2.37883455', 'CARDINAL'), ('2.57350504', 'CARDINAL'), ('2.78410628', 'CARDINAL'), ('3.01194194', 'CARDINAL'), ('3.2584224', 'MONEY'), ('3.52507344', 'CARDINAL'), ('3.8135457', 'CARDINAL'), ('4.12562492', 'GPE'), ('4.46324295', 'GPE'), ('4.82848976', 'CARDINAL'), ('5.22362631', 'CARDINAL'), ('5.65109863', 'CARDINAL'), ('6.11355288', 'CARDINAL'), ('6.61385181', 'CARDINAL'), ('7.15509239', 'MONEY'), ('7.74062507', 'CARDINAL'), ('8.37407447', 'CARDINAL'), ('9.05936181', 'CARDINAL')]\n",
      "[]\n",
      "[('cylindrical_radius', 'ORG')]\n",
      "[('first', 'ORDINAL')]\n",
      "[(\"my_disk['cylindrical_radius\", 'PERSON'), (\"my_disk.set_field_parameter('center\", 'PERSON')]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[(\"my_disk.set_field_parameter('center\", 'FAC')]\n",
      "[]\n",
      "[('7', 'CARDINAL'), ('10', 'CARDINAL'), ('8', 'CARDINAL')]\n",
      "[]\n",
      "[('cylinrical_radius', 'ORG')]\n",
      "[('9', 'CARDINAL'), (\"sp['cylindrical_radius'].to('kpc').min\", 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@pdmullen', 'ORG'), ('GitHub', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AttributeError', 'ORG')]\n",
      "[]\n",
      "[('```File \"xyz.py\"', 'ORG'), ('18', 'CARDINAL'), ('1915', 'DATE'), ('1762', 'DATE'), ('export_ply', 'LANGUAGE')]\n",
      "[('@UCNRC0276', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('pip uninstall yt', 'WORK_OF_ART'), ('clone', 'ORG'), ('repo', 'GPE'), ('pip install -e', 'WORK_OF_ART'), ('repo', 'GPE'), ('http://yt-project.org/doc/installing.html#installing-yt-from-source', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('zero', 'CARDINAL'), ('1850', 'MONEY')]\n",
      "[]\n",
      "[]\n",
      "[('1850', 'MONEY')]\n",
      "[]\n",
      "[]\n",
      "[('greif 2012', 'DATE')]\n",
      "[('1e10gcm-3', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('```p.annotate_sphere([0.5', 'WORK_OF_ART'), ('0.5', 'CARDINAL'), ('0.5', 'CARDINAL'), ('radius=(2', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Ben Kimock', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[('2154', 'CARDINAL'), ('1909', 'DATE'), ('N', 'ORG'), ('~S/N', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('jupyter', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('the anaconda navigator', 'FAC'), ('Source For', 'ORG'), ('Conda', 'GPE'), ('Installs', 'ORG'), ('https://yt-project.org/doc/installing.html#installing-yt-using-anaconda', 'ORG')]\n",
      "[('zero', 'CARDINAL')]\n",
      "[('first', 'ORDINAL')]\n",
      "[('python/conda', 'ORG')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('the week', 'DATE')]\n",
      "[]\n",
      "[('AHF', 'ORG'), ('Amiga', 'ORG'), ('Halo Finder', 'PERSON'), ('GADGET', 'ORG'), ('GADGET', 'ORG')]\n",
      "[]\n",
      "[('one day', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('UnitRegistry', 'ORG'), ('UnitSystem', 'ORG')]\n",
      "[]\n",
      "[('UnitRegistry', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('UnitRegistry', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('UnitRegistry', 'ORG')]\n",
      "[('@U91855PA9', 'PERSON'), ('Python', 'ORG'), ('One', 'CARDINAL'), ('https://github.com/qobilidop/gadgeteer', 'CARDINAL')]\n",
      "[('unit_objs', 'ORG')]\n",
      "[('enh, n/m', 'ORG')]\n",
      "[('unit_objs', 'ORG')]\n",
      "[]\n",
      "[('https://wwwmpa.mpa-garching.mpg.de/gadget/users-guide.pdf', 'CARDINAL'), ('6', 'CARDINAL'), ('1', 'CARDINAL'), ('6.1', 'CARDINAL'), ('2', 'CARDINAL'), ('6.2', 'CARDINAL'), ('AHF', 'ORG')]\n",
      "[('<@U042HLT7U', 'ORG'), ('unit_objs', 'FAC')]\n",
      "[]\n",
      "[('AHF', 'ORG'), ('32', 'CARDINAL')]\n",
      "[]\n",
      "[('11', 'CARDINAL')]\n",
      "[('12', 'CARDINAL'), (\"C**2/N/m**2'\", 'ORG'), (\"J/kg'\", 'ORG'), (\"N/A**2'\", 'ORG'), ('cm**2', 'ORG'), (\"cm**3/g/s**2'\", 'ORG'), (\"cm/s'\", 'ORG'), (\"cm/s**2'\", 'ORG'), (\"dyne/cm**2'\", 'ORG'), (\"erg/K'\", 'ORG'), (\"erg/cm**2/s**1/K**4'\", 'ORG'), (\"erg/g'\", 'ORG'), ('ft**2', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('.log', 'PRODUCT'), ('1', 'CARDINAL')]\n",
      "[('32', 'CARDINAL')]\n",
      "[('0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U014SHC71A4', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('<@U014SHC71A4', 'ORG')]\n",
      "[]\n",
      "[('3H²/8piG', 'PRODUCT'), ('1+z)³', 'CARDINAL'), ('a few percent', 'PERCENT')]\n",
      "[]\n",
      "[('Yuxuan', 'PERSON')]\n",
      "[('set_zlim', 'PRODUCT'), ('max', 'PERSON')]\n",
      "[]\n",
      "[('slc`', 'PERSON'), ('AttributeError', 'ORG')]\n",
      "[]\n",
      "[('psi_m', 'CARDINAL'), ('0.15', 'CARDINAL')]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG'), ('3D', 'FAC'), ('2', 'CARDINAL'), ('3D', 'ORG'), ('min', 'ORG'), ('max', 'PERSON')]\n",
      "[('max', 'PERSON'), ('min', 'PERSON'), ('` or `', 'WORK_OF_ART'), ('2D', 'CARDINAL')]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[('two', 'CARDINAL'), ('PNG', 'ORG'), ('JPEG', 'ORG')]\n",
      "[('Python', 'ORG')]\n",
      "[('@UD4ECST8F', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ParticleProjectionPlot', 'ORG')]\n",
      "[(\"yt.load('gizmo_64\", 'GPE'), ('ParticleProjectionPlot(ds', 'PERSON'), ('particle_mass', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ParticleImageBuffer', 'CARDINAL')]\n",
      "[('YTEP', 'PRODUCT')]\n",
      "[]\n",
      "[('two weeks later', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[(\"g/cm**3'\", 'ORG'), ('density_alias', 'ORG'), ('density_alias', 'ORG'), (\"units='g/cm**3'\", 'ORG')]\n",
      "[]\n",
      "[('<http://yt-project.org/doc/reference/configuration.html#the-plugin-file', 'PERSON')]\n",
      "[]\n",
      "[('i.e', 'ORG'), ('15', 'CARDINAL'), ('15', 'CARDINAL'), ('15', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('ds.load_particle', 'ORG'), ('```To', 'WORK_OF_ART'), ('particle_mass', 'ORG'), ('one', 'CARDINAL')]\n",
      "[('SPH', 'ORG')]\n",
      "[('bili', 'PERSON')]\n",
      "[]\n",
      "[('particle_mass', 'ORG'), ('particle_position', 'ORG'), ('smoothing_length', 'ORG')]\n",
      "[('bili’s PR', 'ORG')]\n",
      "[]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[('0.0', 'CARDINAL'), ('0.0', 'CARDINAL'), ('0.99', 'CARDINAL'), ('0.99', 'CARDINAL'), ('0.99', 'CARDINAL'), ('dims=[128', 'DATE'), ('128', 'DATE'), ('128', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('0.0', 'CARDINAL'), ('0.0', 'CARDINAL'), ('0.99', 'CARDINAL'), ('0.99', 'CARDINAL'), ('dims=[128', 'DATE'), ('128', 'DATE'), ('128', 'CARDINAL')]\n",
      "[('kpc', 'ORG'), ('left_edge', 'CARDINAL'), ('0', 'CARDINAL'), ('.99', 'GPE'), ('.99', 'GPE'), ('ds.arbitrary_grid(left_edge', 'CARDINAL'), ('right_edge', 'ORG')]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('128', 'CARDINAL'), ('128', 'CARDINAL'), ('128', 'CARDINAL'), ('135', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('MCVE', 'ORG')]\n",
      "[('MCVE', 'ORG')]\n",
      "[]\n",
      "[('MCVE', 'ORG'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3', 'CARDINAL'), ('R3', 'GPE'), ('TRIDENT', 'PERSON'), ('29.95018959', 'CARDINAL'), ('30.56563377', 'CARDINAL'), ('3.83224448', 'CARDINAL'), ('3.91099298 603.83302643', 'MONEY'), ('kpc', 'PERSON'), ('kpc', 'ORG'), ('0.49916983', 'CARDINAL'), ('29.95018959', 'CARDINAL'), ('30.56563377', 'CARDINAL'), ('TRIDENT', 'PERSON')]\n",
      "[('https://github.com/yt-project/yt/blob/master/yt/data_objects/static_output.py#L1084', 'PERSON')]\n",
      "[]\n",
      "[('i.e. 1', 'ORG')]\n",
      "[('ds.quan(1', 'ORG')]\n",
      "[]\n",
      "[(\"``print ds.quan(1,'unitary').to('kpc'\", 'WORK_OF_ART'), ('7.677235835829029', 'CARDINAL'), ('kpc', 'ORG'), ('7.67723584 7.67723584 7.67723584', 'MONEY'), ('kpc', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[('BoxSize', 'PRODUCT')]\n",
      "[]\n",
      "[('Nathan', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Clayton', 'PERSON')]\n",
      "[]\n",
      "[('<@UT20CS337', 'ORG')]\n",
      "[('3', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('0 to', 'CARDINAL'), ('1', 'CARDINAL'), ('0', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('YT', 'ORG'), ('GIZMO', 'ORG'), ('particle_mass', 'GPE'), ('<http://yt-project.org/doc/analyzing/analysis_modules/halo_catalogs.html#extra-halo-analysis', 'FAC'), ('first', 'ORDINAL'), ('halos_ds', 'CARDINAL'), ('2019-02-26', 'DATE')]\n",
      "[]\n",
      "[('hc.create', 'PERSON')]\n",
      "[('AHF', 'ORG')]\n",
      "[('2019-02-26', 'DATE'), ('2019-02-26', 'DATE'), ('particle_mass', 'ORG'), ('2019-02-26', 'DATE'), ('2019-02-26', 'DATE'), ('2019-02-26', 'DATE'), ('2019-02-26', 'DATE')]\n",
      "[('AHF', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Python 3.7.1', 'PRODUCT'), ('3.5', 'CARDINAL')]\n",
      "[('3.7.1', 'CARDINAL'), ('3.5.1', 'CARDINAL')]\n",
      "[('print(halos_ds.field_list', 'WORK_OF_ART')]\n",
      "[('britton', 'ORG'), ('bili', 'PERSON'), ('the west coast', 'LOC')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('busy day ahead', 'DATE')]\n",
      "[]\n",
      "[('AHF', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('astro_analysis', 'ORG')]\n",
      "[]\n",
      "[('yt.analysis_modules', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('HaloCatalog', 'ORG'), (\"yt.load('gizmo_64\", 'GPE'), ('halos_ds', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt_astro_analysis', 'NORP')]\n",
      "[]\n",
      "[('yt_astro_analysis', 'DATE')]\n",
      "[('pip install -e', 'WORK_OF_ART')]\n",
      "[('yt_astro_analysis', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('the next hour', 'TIME')]\n",
      "[]\n",
      "[('<@UA5GP3NBH', 'ORG')]\n",
      "[]\n",
      "[('ds.quan(3.5', 'ORG')]\n",
      "[('3.5*ds.units.kpccm', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('code_length', 'ORG')]\n",
      "[('FRB', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[('yup', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('xmin', 'PERSON'), ('xmax', 'PERSON'), ('ymin', 'ORG'), ('ymax', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.x', 'DATE'), ('``\\n\\n', 'WORK_OF_ART'), ('``\\nTraceback', 'WORK_OF_ART'), ('20', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('desika.narayanan/miniconda3/envs/mypy2env/lib/python2.7', 'ORG'), ('second', 'ORDINAL'), ('np.dtype(float).type', 'ORG'), ('2018-09-11', 'DATE'), ('9.756e-01', 'QUANTITY'), ('4.263e+17 seconds', 'TIME'), ('2018-09-11', 'DATE'), ('kpc', 'ORG'), ('2018-09-11', 'DATE'), ('Parameters', 'NORP'), ('4.263438511011124e+17 s', 'QUANTITY'), ('2018-09-11', 'DATE'), ('4 4 4', 'DATE'), ('2018-09-11', 'DATE'), ('Parameters', 'NORP'), ('0', 'CARDINAL'), ('2018-09-11', 'DATE'), ('50000', 'CARDINAL'), ('50000', 'CARDINAL'), ('50000', 'CARDINAL'), ('2018-09-11', 'DATE'), ('1', 'CARDINAL'), ('2018-09-11', 'DATE'), ('Parameters', 'NORP'), ('0.02499995903287333', 'CARDINAL'), ('2018-09-11', 'DATE'), ('Parameters', 'NORP'), ('2018-09-11', 'DATE'), ('Parameters', 'NORP'), ('2018-09-11', 'DATE'), ('Parameters', 'NORP'), ('0.68', 'CARDINAL'), ('2018-09-11', 'DATE'), ('2018-09-11 08:55:45,393', 'DATE'), ('2018-09-11 13:03:57,411', 'DATE'), ('Max Value', 'PERSON'), ('5.20419e-20', 'CARDINAL'), ('28073.1499195098876953 22261.768579483032', 'TIME'), ('2266 28530.7705402374267578', 'DATE'), ('2018-09-11', 'DATE'), ('2018-09-11', 'DATE'), ('22181.340925 22320.740919', 'TIME'), ('2018-09-11', 'DATE'), ('28461.739948 28601.139942', 'DATE'), ('2018-09-11', 'DATE'), ('22181.340925 22320.740919', 'TIME'), ('2018-09-11', 'DATE'), ('28461.739948 28601.139942', 'DATE'), ('2018-09-11', 'DATE'), ('800', 'CARDINAL'), ('20', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('```(', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('ProjectionPlot', 'ORG')]\n",
      "[('set_zlim', 'PRODUCT')]\n",
      "[('YT Covering', 'ORG'), ('YTDataSelectorNotImplemented', 'PRODUCT')]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[('3.5+AMR', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[('save_as_dataset', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('btw', 'ORG')]\n",
      "[('3.5', 'CARDINAL')]\n",
      "[('demesh yt', 'PERSON'), ('``\\nsc =', 'WORK_OF_ART'), ('``\\n\\n', 'WORK_OF_ART'), ('dpi=300', 'GPE')]\n",
      "[]\n",
      "[('<@U37DTBL6N', 'ORG')]\n",
      "[('1600, 1600', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Nathan', 'ORG')]\n",
      "[('1850', 'MONEY')]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON'), ('3.4', 'CARDINAL')]\n",
      "[('<@UD6CG0FL4', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2048', 'MONEY')]\n",
      "[]\n",
      "[]\n",
      "[('kacper', 'ORG')]\n",
      "[]\n",
      "[('1850', 'MONEY')]\n",
      "[('3.5.dev0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.5.dev0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('pip install -e', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('britton', 'ORG'), ('PR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3', 'CARDINAL')]\n",
      "[]\n",
      "[('info_items', 'ORG')]\n",
      "[('2048', 'DATE')]\n",
      "[('tonight', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('pip install -e', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@UF1UC363Z', 'ORG')]\n",
      "[('3.4', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PR', 'ORG'), ('AHF', 'ORG')]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[('nchilada', 'ORG')]\n",
      "[(\"'a nchilada'\", 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@UTULQHWKE', 'ORG')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[('https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb', 'PERSON')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('pip install', 'PERSON'), ('cykdtree', 'CARDINAL')]\n",
      "[]\n",
      "[('repo', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.5', 'CARDINAL'), ('4', 'CARDINAL')]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('pip install -e', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('PP=\"${HOME}/yt/$1/', 'FAC'), ('PP', 'PERSON'), ('${PP} doesn\\'t exist\" &amp;&amp', 'WORK_OF_ART'), ('YTNAME=$1\\n    setprompt', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('SPH', 'ORG')]\n",
      "[('yt.load_particles', 'PERSON')]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG'), ('smoothing_length', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMReX', 'ORG')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[('SPLASH', 'ORG'), ('SPLASH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('^.', 'ORG'), ('GitHub', 'NORP')]\n",
      "[('<https://github.com/danieljprice/splash.git', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('https://github.com/yt-project/yt/blob/yt-4.0/yt/utilities/lib/pixelization_routines.pyx#L965', 'PERSON')]\n",
      "[]\n",
      "[('itab', 'PERSON')]\n",
      "[('SPH', 'ORG')]\n",
      "[('pixel', 'ORG'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('4.3.1', 'ORG')]\n",
      "[('dan price', 'PERSON'), ('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[('at least one', 'CARDINAL')]\n",
      "[('3.4', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('atm', 'ORG'), ('this morning', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[('about 30', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('smoothing_length', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('136', 'CARDINAL')]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('5', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('DatasetSeries([\"DD0030/DD0030', 'LAW')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('n_bins=\\n\\n', 'PERSON'), ('y2', 'PRODUCT'), ('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('HaloAnalysis', 'ORG')]\n",
      "[('Doug', 'PERSON')]\n",
      "[]\n",
      "[(\"sphere['star\", 'ORG')]\n",
      "[('HaloCatalog', 'PERSON')]\n",
      "[]\n",
      "[('Britton', 'ORG')]\n",
      "[('data_object', 'ORG')]\n",
      "[('data_object', 'ORG')]\n",
      "[('first', 'ORDINAL')]\n",
      "[('one', 'CARDINAL'), ('R500/R2500', 'ORG')]\n",
      "[]\n",
      "[('R200', 'FAC'), ('R500/R2500', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('virial_quantities', 'ORG')]\n",
      "[('yt_astro_analysis', 'NORP'), ('HaloAnalysis', 'ORG')]\n",
      "[('1 hc.create', 'TIME'), ('yt_astro_analysis/halo_analysis/halo_catalog.py', 'ORG'), ('save_halos', 'ORG'), ('335', 'CARDINAL'), ('336', 'CARDINAL'), ('337', 'CARDINAL'), ('338', 'CARDINAL'), ('339     ', 'QUANTITY'), ('njobs=-1', 'GPE'), ('parallel_tools/parallel_analysis_interface.py', 'ORG'), ('barrierize(*args', 'GPE'), ('kwargs', 'GPE'), ('299     def', 'QUANTITY'), ('kwargs', 'GPE'), ('300', 'CARDINAL'), ('301', 'CARDINAL'), ('func(*args', 'DATE'), ('kwargs', 'GPE'), ('302', 'CARDINAL'), ('303', 'CARDINAL'), ('run(self', 'GPE'), ('save_halos', 'ORG'), ('451', 'CARDINAL'), ('452                     ', 'QUANTITY'), ('453', 'CARDINAL'), ('new_halo.quantities[key', 'MONEY'), ('454', 'CARDINAL'), ('455', 'CARDINAL'), ('RuntimeError', 'ORG'), ('37', 'CARDINAL'), ('38     ', 'QUANTITY'), ('39', 'CARDINAL'), ('40', 'CARDINAL'), ('1', 'CARDINAL'), ('2', 'CARDINAL')]\n",
      "[('hc.create', 'FAC')]\n",
      "[('doc', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('halos_ds', 'CARDINAL'), (\"sphere['star\", 'FAC'), ('2.0', 'CARDINAL')]\n",
      "[]\n",
      "[('halos_ds', 'CARDINAL'), (\"sphere['star\", 'ORG'), ('2.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('HaloCatalog', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('HaloCatalog', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('add_callback', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('GIZMO', 'ORG'), ('particle_mass', 'ORG'), (\"sphere['star\", 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('HaloCatalog', 'ORG')]\n",
      "[('this this week', 'DATE'), ('today', 'DATE'), ('slightly_smiling_face', 'PERSON')]\n",
      "[]\n",
      "[('ProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[('@U91855PA9', 'PERSON'), (\"new_particle_name='filtered_particle\", 'GPE'), ('data[(pfilter.filtered_type', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[('<@UD6CG0FL4', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('<@UD6CG0FL4', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('the second half of 2019', 'DATE')]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[('July 2019', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('July 2019', 'DATE'), ('yt-4.0', 'NORP')]\n",
      "[('1.1.1', 'CARDINAL')]\n",
      "[]\n",
      "[('20.0.2', 'CARDINAL'), ('3.6', 'CARDINAL')]\n",
      "[('pip', 'ORG'), ('conda', 'ORG')]\n",
      "[('conda', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('conda', 'ORG'), ('first', 'ORDINAL')]\n",
      "[('pip', 'ORG'), ('slightly_smiling_face', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('conda', 'ORG'), ('pip', 'ORG')]\n",
      "[('conda install -c conda', 'ORG')]\n",
      "[]\n",
      "[('pip', 'ORG')]\n",
      "[('the last six months', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('yt-4 merges', 'PERSON'), ('pip/conda limbo', 'ORG')]\n",
      "[]\n",
      "[('Ben', 'PERSON'), ('OffAxisProjectionPlot', 'ORG'), ('data_source', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PR', 'ORG')]\n",
      "[('<@U042HLT7U', 'ORG'), ('3.5.1', 'ORG'), ('Monday', 'DATE')]\n",
      "[('3.6', 'CARDINAL'), ('the coming days', 'DATE'), ('weeks', 'DATE')]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('sto.result', 'WORK_OF_ART')]\n",
      "[('Mike', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('HPC', 'ORG'), ('DoD', 'ORG')]\n",
      "[('HPC', 'ORG')]\n",
      "[]\n",
      "[('module', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('HPC', 'ORG'), ('conda', 'ORG')]\n",
      "[('the next hour', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('mpi4py', 'ORG'), (\"home/mime5507/yt-conda/src/yt_conda'\", 'ORG'), ('AttributeError', 'ORG'), ('module', 'PERSON')]\n",
      "[]\n",
      "[('yt_conda', 'ORG'), ('yt.__file', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('One', 'CARDINAL'), ('conda', 'GPE'), ('pip', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('OMPI', 'ORG'), ('OMPI', 'ORG'), ('MPI_ERRORS_ARE_FATAL', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('HPC', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('python/mpi4py', 'ORG'), ('pip', 'PERSON')]\n",
      "[('offhand', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('mass_sph', 'WORK_OF_ART')]\n",
      "[('3526ad0f76cccff9825f4ee25b5a8758fa45c48b', 'CARDINAL'), ('17 00:00:00 2001', 'DATE'), ('Corentin Cadiou &lt;contact@cphyc.me&gt', 'ORG'), ('Sun', 'PERSON'), ('5 May 2019', 'DATE'), ('19:02:08', 'TIME'), ('data_structures.py | 1', 'PRODUCT'), ('2', 'CARDINAL'), ('3', 'CARDINAL'), ('insertions(+', 'PRODUCT'), ('1', 'CARDINAL'), ('12979474e 100644', 'DATE'), ('#', 'CARDINAL'), ('mass_sph', 'WORK_OF_ART'), ('9cc8278ed', 'CARDINAL'), ('a3235580a 100644', 'DATE'), ('3', 'CARDINAL'), ('7', 'DATE'), ('5', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('2.21.0', 'CARDINAL')]\n",
      "[]\n",
      "[('2D', 'CARDINAL'), ('SlicePlot', 'ORG'), ('annotate_quiver', 'PRODUCT'), ('SlicePlot', 'ORG'), ('nlc_x', 'DATE'), ('nlc_y', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('nlc_x', 'PERSON')]\n",
      "[('@UCC3WAXE3', 'ORG')]\n",
      "[('ParticlePlot', 'ORG'), ('OffAxisProjectionPlot', 'ORG')]\n",
      "[('yt.load_particles', 'PERSON')]\n",
      "[('ParticlePlot', 'ORG'), ('yt-4.0', 'NORP')]\n",
      "[('<@UC6L85LBB', 'ORG'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('https://yt-project.org/doc/cookbook/opengl_vr.py', 'ORG')]\n",
      "[('IDV', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('RA', 'ORG'), ('DEC', 'ORG')]\n",
      "[('FITS', 'ORG')]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[('IDV', 'ORG')]\n",
      "[]\n",
      "[('GitHub', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('http://yt-project.org/doc/installing.html#installing-yt-from-source', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2014', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('IDV', 'ORG')]\n",
      "[('IDV', 'ORG'), ('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('GitHub', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('today', 'DATE'), ('tomorrow', 'DATE'), ('IDV', 'ORG')]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG'), ('Sorry', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('IDV', 'ORG')]\n",
      "[]\n",
      "[('a couple of years ago', 'DATE')]\n",
      "[]\n",
      "[('<@U37DTBL6N', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('[cambot:~/scratch/Yuguang] chummels% yt pastebin halo.py', 'PERSON'), ('yt-conda/lib/python3.6', 'ORG'), ('is_string_like', 'ORG'), ('2.1', 'CARDINAL'), ('2018-05-16 16:29:33,037', 'DATE'), ('11', 'CARDINAL'), ('1413', 'DATE'), ('237', 'CARDINAL'), ('833', 'CARDINAL'), ('315', 'CARDINAL'), ('211', 'CARDINAL'), ('1112', 'DATE'), ('1452', 'DATE'), ('1154', 'DATE'), ('request_body', 'GPE'), ('1166', 'CARDINAL'), ('single_request', 'GPE'), ('request_body', 'GPE'), ('1279', 'DATE'), ('send_request', 'GPE'), ('request_body', 'GPE'), ('1309', 'DATE'), ('send_content', 'MONEY'), ('1234', 'DATE'), ('1026', 'DATE'), ('964', 'CARDINAL'), ('936', 'CARDINAL'), ('self.port', 'ORG'), ('724', 'CARDINAL'), ('err\\n  File', 'LAW'), ('713', 'CARDINAL'), ('ConnectionRefusedError', 'CARDINAL'), ('Errno 61] Connection', 'LAW')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('IP', 'ORG')]\n",
      "[('30', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('an hour', 'TIME')]\n",
      "[('DiG 9.11.1-P3 &lt;&lt;&gt;&gt', 'ORG'), ('QUERY', 'PERSON'), ('NOERROR', 'ORG'), ('34372', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('0', 'CARDINAL'), ('1', 'CARDINAL'), ('EDNS', 'ORG'), ('0', 'CARDINAL'), ('512', 'CARDINAL'), ('paste.yt-project.org', 'CARDINAL'), ('3563', 'CARDINAL'), ('141.142.211.195\\n\\n', 'LOC'), ('Query', 'ORG'), ('17', 'CARDINAL'), ('May 16 18:32:11', 'DATE'), ('65', 'CARDINAL')]\n",
      "[('141.142.211.195', 'CARDINAL')]\n",
      "[]\n",
      "[('`yt pastebin &lt;file&gt', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt version 3.5.1', 'PERSON'), ('python3', 'PRODUCT'), ('MultiFab', 'ORG'), ('MultiFabs', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UT9K5TYNA', 'ORG')]\n",
      "[('geometry=\"cartesian', 'NORP')]\n",
      "[('AMRVAC', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Nathan', 'ORG')]\n",
      "[]\n",
      "[('AMR', 'ORG'), ('2D', 'CARDINAL'), ('3D', 'CARDINAL')]\n",
      "[('Loading_Generic_Array_Data.py', 'CARDINAL'), ('AMR', 'ORG'), ('2D', 'CARDINAL'), ('3D', 'CARDINAL'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('<@UF14FNZTQ', 'ORG'), ('one', 'CARDINAL')]\n",
      "[('HOP', 'ORG')]\n",
      "[]\n",
      "[('1000kpc', 'DATE'), ('1000kpc', 'ORDINAL')]\n",
      "[('1000kpc', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('400kpc', 'GPE')]\n",
      "[]\n",
      "[('Britton', 'PERSON'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('np', 'ORG'), ('yt.load(snapshot', 'CARDINAL'), ('``In', 'WORK_OF_ART'), ('30', 'CARDINAL'), ('EnzoDataset', 'ORG'), ('[31', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL'), ('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('``    ', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('False', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('unigrid', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@UUF099K2N', 'PERSON')]\n",
      "[('3D_Array', 'CARDINAL'), ('1024, 1024, 1024', 'DATE'), ('Initial Theta', 'WORK_OF_ART'), ('length_unit=2*np.pi/1024', 'LOC')]\n",
      "[('ProjectionPlot(ds', 'GPE'), ('2', 'DATE'), ('Initial Theta', 'WORK_OF_ART'), (\"'Initial Theta'\", 'WORK_OF_ART'), ('UniformGridData', 'GPE')]\n",
      "[(\"'Initial Theta'\", 'WORK_OF_ART')]\n",
      "[('Initial Theta', 'WORK_OF_ART')]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Max', 'PERSON')]\n",
      "[('two', 'CARDINAL'), ('One', 'CARDINAL'), ('Maxwell-Juttner', 'ORG'), ('MJ', 'PERSON'), ('two', 'CARDINAL'), ('MJ', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UBJU11GJU', 'ORG')]\n",
      "[]\n",
      "[('<@UBJU11GJU', 'ORG'), ('CIC', 'ORG'), ('ParticlePlot', 'ORG')]\n",
      "[('ParticlePlot', 'ORG')]\n",
      "[('<@UBJU11GJU', 'ORG'), ('CIC', 'ORG'), ('128', 'CARDINAL')]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[('0:3', 'DATE'), ('0:3', 'CARDINAL'), ('zip(i_i.ravel', 'GPE'), ('slice(i', 'NORP'), ('nx-(2-i', 'ORG'), ('slice(j', 'ORG'), ('new_field', 'PERSON'), ('3', 'CARDINAL'), ('3=27', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('27', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[('2D', 'QUANTITY')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2D', 'QUANTITY')]\n",
      "[('first', 'ORDINAL')]\n",
      "[('1D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ray tracing', 'PERSON')]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[('<@UK6KU2KHS', 'ORG')]\n",
      "[('yt.load(&lt;halo_catalog_file&gt', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('arrays', 'DATE')]\n",
      "[]\n",
      "[('about half', 'CARDINAL'), ('MPI', 'ORG')]\n",
      "[('<@U1TMQQB38', 'ORG')]\n",
      "[('@U010LTM19HD', 'ORG')]\n",
      "[('<@U0107MAQWQK', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('EAGLE', 'NORP')]\n",
      "[]\n",
      "[('ds.covering_grid(2', 'CARDINAL'), ('0', 'DATE'), ('16', 'DATE')]\n",
      "[]\n",
      "[('4', 'DATE')]\n",
      "[('oct', 'DATE'), ('oct', 'DATE'), ('SPH', 'ORG'), ('things~', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[(\"covering_grid[('deposit\", 'ORG')]\n",
      "[('SPH', 'ORG')]\n",
      "[('Ash', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('3', 'CARDINAL'), ('4', 'CARDINAL')]\n",
      "[('Rich', 'ORG'), ('yt-4.0', 'NORP')]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[('yt-4 should', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('Desika', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('this week', 'DATE')]\n",
      "[('US', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('https://github.com/conda-forge/netcdf4-feedstock/issues/44', 'PERSON'), ('FLASH', 'ORG')]\n",
      "[('netcdf4', 'PRODUCT'), ('netcdf4', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('` or `weighted_average`', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UDJ5LCUG2', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('HOP', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('thanksgiving', 'DATE')]\n",
      "[('turkey', 'GPE')]\n",
      "[]\n",
      "[('Nathan', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('10%', 'PERCENT')]\n",
      "[]\n",
      "[(\"obj['radius'] &lt;\", 'ORG')]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('&lt;module&gt', 'ORG'), ('1', 'CARDINAL'), ('0.5', 'CARDINAL'), ('1058', 'DATE'), ('= super(YTArray', 'PERSON'), ('self).__getitem__(item', 'PERSON'), ('YTQuantity(ret', 'GPE'), ('boolean', 'NORP'), ('0', 'CARDINAL'), ('boolean', 'NORP'), ('8', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('np.where', 'FAC')]\n",
      "[(\"sph['radius\", 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('this a couple of days', 'DATE'), ('SPH', 'ORG'), ('Arepo', 'GPE'), ('Voronoi', 'GPE')]\n",
      "[]\n",
      "[('Arepo', 'GPE'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[('voronoi mesh', 'GPE')]\n",
      "[('voronoi', 'GPE')]\n",
      "[('3D', 'LAW'), ('voronoi', 'GPE')]\n",
      "[('second', 'ORDINAL')]\n",
      "[('`if distance(particle) &lt; plane[pos', 'ORG')]\n",
      "[]\n",
      "[('voronoi', 'GPE')]\n",
      "[]\n",
      "[('Streamlines', 'LOC'), ('3D', 'CARDINAL'), ('velocity_y', 'PERSON'), ('z', 'PERSON'), ('vy', 'PERSON'), ('vz', 'GPE'), ('Streamlines', 'LOC')]\n",
      "[]\n",
      "[]\n",
      "[('yt.say_hello', 'PERSON')]\n",
      "[('2018-06-26', 'DATE'), ('#', 'CARDINAL'), ('np\\n\\n', 'PERSON'), ('np.random.random(data[\"density\"].shape', 'GPE'), ('say_hello', 'ORG'), ('print(\"hilo', 'ORG')]\n",
      "[('2018-06-26', 'DATE')]\n",
      "[('2018-06-26', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('###################', 'MONEY'), ('np', 'ORG'), ('solve_ivp', 'ORG'), ('Runge-Kutta', 'ORG'), ('YTArray', 'ORG'), ('Streamlines', 'ORG'), ('#', 'CARDINAL'), ('#', 'CARDINAL'), ('Constants', 'ORG'), ('9.1e-31', 'CARDINAL'), ('# electron mass', 'MONEY'), ('2000', 'CARDINAL'), ('#', 'CARDINAL'), ('2000', 'DATE'), ('x0=', 'PRODUCT'), ('np.zeros(6', 'ORG'), ('#', 'CARDINAL'), ('6 dim', 'MONEY'), ('zeros', 'CARDINAL'), ('3', 'CARDINAL'), ('3', 'CARDINAL'), ('Cartesian', 'NORP'), ('0.15', 'CARDINAL'), ('YMIN', 'ORG'), ('ZMAX', 'ORG'), ('0.1', 'CARDINAL'), ('100', 'CARDINAL'), ('XMAX', 'ORG'), ('YMAX', 'ORG'), ('ZMAX', 'ORG'), ('np.meshgrid(x', 'ORG'), ('np.all(y[0,:,0', 'PERSON'), ('assert np.all(z[0,0', 'PERSON'), ('# Cylindric', 'WORK_OF_ART'), ('math.atan2(y_[i', 'ORG'), ('#', 'CARDINAL'), ('def mag_field(grid', 'PERSON'), ('sampling-1', 'PERSON'), ('sampling-1', 'PERSON'), ('np.float', 'PRODUCT'), ('0', 'CARDINAL'), ('0.25', 'CARDINAL'), ('0.75 *', 'PERCENT'), ('0.5', 'CARDINAL'), ('#############################################################################################', 'MONEY'), ('Choose', 'ORG'), ('0.025', 'CARDINAL'), ('Dictionary', 'ORG'), ('# 3d array', 'MONEY'), ('2.5', 'CARDINAL'), ('#', 'CARDINAL'), ('yt.load_uniform_grid(data', 'PERSON'), ('B_R.shape', 'PERSON'), ('length_unit=\"Mpc', 'ORG'), ('bbox=', 'PERSON'), ('bbox', 'GPE'), ('nprocs=100', 'CARDINAL'), ('#', 'CARDINAL'), ('Define', 'MONEY'), ('N', 'ORG'), ('Scale', 'ORG'), ('Streamlines(ds', 'GPE'), ('pos', 'PERSON'), ('streamlines.integrate_through_volume', 'PERSON'), ('0.0', 'CARDINAL'), ('#', 'CARDINAL'), ('print(x_streamline', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('0.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[('@U91855PA9', 'PERSON'), ('Hmm', 'PERSON')]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('https://yt-project.org/docs/dev/_modules/yt/funcs.html#get_hash', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[('1', 'CARDINAL'), ('2', 'CARDINAL'), ('2', 'CARDINAL'), ('0x10e576c88&gt', 'CARDINAL'), ('3', 'CARDINAL'), ('249704968950119223', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('ParticleIndex', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PhasePlot', 'WORK_OF_ART'), ('2d', 'CARDINAL'), ('2d', 'QUANTITY')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.point', 'ORG')]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[('ds.point', 'PERSON'), ('SPH', 'ORG')]\n",
      "[('SPH', 'ORG')]\n",
      "[('ds.point', 'ORG')]\n",
      "[('ds.point', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.point', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yup', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[('<https://github.com/yt-project/yt/blob/yt-4.0/yt/utilities/lib/pixelization_routines.pyx#L1316', 'PERSON')]\n",
      "[]\n",
      "[('@UH593L8TU', 'PERSON')]\n",
      "[]\n",
      "[('@UH593L8TU', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U9CE5D9LZ', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@UH593L8TU', 'PERSON'), ('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.point', 'WORK_OF_ART')]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.point', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG'), ('cython', 'GPE'), ('https://github.com/yt-project/yt/blob/yt-4.0/yt/utilities/lib/pixelization_routines.pyx', 'CARDINAL'), ('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('pip install', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2.7', 'CARDINAL')]\n",
      "[('test_minimal_requirements.txt', 'ORG'), ('appveyor.yaml', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('5.2.2', 'CARDINAL'), ('2.7', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('Gotcha', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('pip install', 'WORK_OF_ART')]\n",
      "[('python_version &lt', 'ORG'), (\"python_version &gt;= '\", 'ORG'), ('test_minimal_requirements.txt', 'LAW')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('4.6.9', 'CARDINAL')]\n",
      "[('4.6 release', 'MONEY')]\n",
      "[('Gadget', 'ORG')]\n",
      "[]\n",
      "[('<@UC8FLF97U', 'ORG')]\n",
      "[('@UC9RGJ4TY', 'ORG')]\n",
      "[('<@UPYHR9L8G', 'ORG')]\n",
      "[('YT 2.x', 'GPE'), ('YT 3.5', 'FAC'), ('particle_mass', 'ORG'), ('cell_mass', 'WORK_OF_ART'), ('particle_mass', 'WORK_OF_ART'), ('Star Particle Mass.', 'ORG'), ('one', 'CARDINAL'), ('Star Particle Mass', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Jupyter Notebook', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('about 10', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('second', 'ORDINAL')]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[('2', 'CARDINAL')]\n",
      "[('only one', 'CARDINAL'), ('one', 'CARDINAL'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U014A7MD2LT', 'ORG')]\n",
      "[]\n",
      "[('Rockstar', 'GPE')]\n",
      "[('Halocatalog', 'NORP'), ('mass &gt', 'ORG'), ('10', 'CARDINAL'), ('13', 'CARDINAL'), ('rockstar_ds', 'PRODUCT'), ('catalog_ds', 'PRODUCT'), ('catalog_ds', 'PRODUCT'), ('rockstar_ds', 'PRODUCT'), ('only half', 'CARDINAL'), ('catalog_ds', 'CARDINAL'), ('rockstar_ds', 'GPE'), ('Halocatalog', 'NORP')]\n",
      "[('<@U042HLT7U', 'ORG'), ('Python', 'ORG')]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('PyNE', 'ORG'), ('d.extract_isocontours(\"ww_n', 'ORG'), ('0.002', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('PyNE', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[('PyNE', 'ORG')]\n",
      "[('one', 'CARDINAL'), ('3D', 'ORG')]\n",
      "[]\n",
      "[('load_uniform_grid', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('first-year', 'DATE'), ('the University of Michigan', 'ORG'), ('1D', 'CARDINAL'), ('ProfilePlot', 'ORG'), ('Python', 'ORG')]\n",
      "[]\n",
      "[('<@UBJU11GJU', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('``\\nconda', 'WORK_OF_ART'), ('``\\n\\n', 'WORK_OF_ART'), ('``\\n\\n', 'WORK_OF_ART'), ('```\\n\\n(h/t to <@U042FH0RB', 'WORK_OF_ART'), ('this a few weeks ago', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('python3', 'PRODUCT')]\n",
      "[('two', 'CARDINAL')]\n",
      "[('setup.py', 'PRODUCT')]\n",
      "[('Matt', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('three thousand', 'CARDINAL'), ('3d', 'CARDINAL'), ('Komolgorov', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3d', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('self.particle_types_raw', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('gas_tracer', 'PERSON'), ('star_tracer', 'PERSON')]\n",
      "[]\n",
      "[('get_data', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('YTDataContainer.get_data', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('YTDataContainer.get_data', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Index._split_fields', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL'), ('two', 'CARDINAL')]\n",
      "[('two', 'CARDINAL'), ('VolumeSource', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yup', 'ORG')]\n",
      "[('two', 'CARDINAL')]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2', 'CARDINAL'), ('2018-08-15', 'DATE'), ('4.444e-01', 'CARDINAL'), ('1.585e+17 seconds', 'TIME'), ('2018-08-15', 'DATE'), ('kpc', 'ORG'), ('2018-08-15 20:03:28,467', 'DATE'), ('Parameters', 'NORP'), ('2018-08-15 20:03:28,467', 'DATE'), ('Parameters', 'NORP'), ('2018-08-15 20:03:28,467', 'DATE'), ('Parameters', 'NORP'), ('0', 'CARDINAL'), ('2018-08-15 20:03:28,467', 'DATE'), ('Parameters', 'NORP'), ('50000', 'CARDINAL'), ('50000', 'CARDINAL'), ('50000', 'CARDINAL'), ('2018-08-15', 'DATE'), ('Parameters', 'NORP'), ('1', 'CARDINAL'), ('2018-08-15', 'DATE'), ('Parameters', 'NORP'), ('1.2500002398009817', 'CARDINAL'), ('2018-08-15', 'DATE'), ('Parameters', 'NORP'), ('2018-08-15', 'DATE'), ('Parameters', 'NORP'), ('2018-08-15', 'DATE'), ('Parameters', 'NORP'), ('0.68', 'CARDINAL'), ('3', 'CARDINAL'), ('HaloCatalog\\n   ', 'PERSON'), ('4', 'CARDINAL')]\n",
      "[('5', 'CARDINAL'), ('hc.create', 'PERSON'), ('2018-08-15', 'DATE'), ('1 hc.create', 'TIME'), ('save_halos', 'GPE'), ('335', 'CARDINAL'), ('336', 'CARDINAL'), ('337', 'CARDINAL'), ('338', 'CARDINAL'), ('339     ', 'QUANTITY'), ('njobs=-1', 'GPE'), ('parallel_tools/parallel_analysis_interface.py', 'ORG'), ('barrierize(*args', 'GPE'), ('kwargs', 'GPE'), ('299     def', 'QUANTITY'), ('kwargs', 'GPE'), ('300', 'CARDINAL'), ('301', 'CARDINAL'), ('func(*args', 'DATE'), ('kwargs', 'GPE'), ('302', 'CARDINAL'), ('303', 'CARDINAL'), ('run(self', 'GPE'), ('save_halos', 'ORG'), ('404', 'CARDINAL'), ('self.halos_ds', 'DATE'), ('405             #', 'QUANTITY'), ('406', 'CARDINAL'), ('407', 'CARDINAL'), ('self.halos_ds', 'DATE'), ('408', 'CARDINAL'), (\"0}'.format(\\\\\", 'CARDINAL'), ('42', 'CARDINAL'), ('43     ', 'QUANTITY'), ('44', 'CARDINAL'), ('self.function(ds', 'PERSON'), ('45', 'CARDINAL'), ('finder_kwargs', 'GPE'), ('59', 'QUANTITY'), ('60', 'CARDINAL'), ('61', 'CARDINAL'), ('62', 'CARDINAL'), ('parse_old_halo_list(ds', 'PERSON'), ('halo_list', 'ORG'), ('63', 'CARDINAL'), ('RuntimeError', 'ORG'), ('1607', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('annotate_contour', 'TIME'), ('ParticlePlot', 'WORK_OF_ART'), ('AttributeError', 'ORG')]\n",
      "[('ProjectionPlot(ds', 'GPE')]\n",
      "[]\n",
      "[('<@U4ESETP43', 'ORG'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('cic', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('annotate_quiver', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('nearly a minute', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ooooh', 'ORG')]\n",
      "[('TypeError', 'ORG')]\n",
      "[('factor=32', 'GPE')]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[('older than 2018', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[('1D', 'CARDINAL'), ('ENZO', 'ORG')]\n",
      "[]\n",
      "[('``Traceback', 'WORK_OF_ART'), ('68', 'CARDINAL'), (\"mpl_kwargs={'dpi':dpi\", 'CARDINAL'), ('92', 'CARDINAL'), ('1127', 'DATE'), ('1121', 'DATE'), ('75', 'CARDINAL'), ('620', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ParticlePlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('X=0', 'PRODUCT'), ('X&gt;0', 'WORK_OF_ART'), ('several zeros', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2', 'CARDINAL'), ('4', 'CARDINAL'), ('2019-10-31', 'DATE'), ('2', 'CARDINAL'), ('2019-10-31', 'DATE'), ('3 / 4', 'DATE'), ('2019-10-31', 'DATE'), ('2019-10-31', 'DATE'), ('1 / 4\\n1\\n3\\n5', 'DATE')]\n",
      "[('two', 'CARDINAL'), ('4', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[('several years', 'DATE')]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[('two', 'CARDINAL'), ('4', 'CARDINAL'), ('4', 'CARDINAL'), ('njobs&gt;1', 'ORG')]\n",
      "[('<@U04RL26HX', 'ORG')]\n",
      "[('Kiran', 'PERSON'), ('UG', 'ORG')]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('2', 'CARDINAL'), ('field_positions', 'ORG'), ('SPH', 'ORG'), ('interpolate_sph_grid_gather', 'ORG'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[('http://adsabs.harvard.edu/full/1989ApJS...70..419H', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('all_data', 'PERSON')]\n",
      "[]\n",
      "[('<http://yt-project.org/docs/dev/reference/api/yt.utilities.math_utils.html#yt.utilities.math_utils.get_rotation_matrix', 'PERSON')]\n",
      "[('OffAxisProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('all day', 'DATE'), ('today', 'DATE'), ('tomorrow', 'DATE')]\n",
      "[('tomorrow', 'DATE')]\n",
      "[('Subfind', 'GPE'), ('Gadget FOF', 'PERSON')]\n",
      "[('SlicePlot', 'PRODUCT'), ('Cartesian', 'NORP'), ('cylindrical_theta', 'ORG'), ('ProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.disk(my_center_of_mass', 'ORG'), ('5', 'CARDINAL'), ('#', 'CARDINAL'), ('Create Profile', 'ORG'), ('dict(cylindrical_radius=(0.,5', 'GPE'), ('5./2', 'WORK_OF_ART'), ('0.8e11', 'GPE'), ('1.5e11', 'DATE'), ('#', 'CARDINAL'), ('plot.save(\"yt_answer.png', 'GPE')]\n",
      "[]\n",
      "[('SPH', 'ORG'), ('``\\n    image = yt.on_axis_projection', 'WORK_OF_ART'), ('normal_vector=[1.0', 'CARDINAL'), ('0.0', 'CARDINAL'), ('0.0', 'CARDINAL'), ('256', 'CARDINAL'), ('0.0', 'CARDINAL'), ('1.0', 'CARDINAL'), ('SPH', 'ORG'), ('Received', 'PERSON')]\n",
      "[('@U91855PA9', 'PERSON'), ('SPH', 'ORG')]\n",
      "[('PartType', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Ben', 'PERSON')]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[('3526ad0f76cccff9825f4ee25b5a8758fa45c48b', 'CARDINAL'), ('17 00:00:00 2001', 'DATE'), ('Corentin Cadiou &lt;contact@cphyc.me&gt', 'ORG'), ('Sun', 'PERSON'), ('5 May 2019', 'DATE'), ('19:02:08', 'TIME'), ('data_structures.py | 1', 'PRODUCT'), ('2', 'CARDINAL'), ('3', 'CARDINAL'), ('insertions(+', 'PRODUCT'), ('1', 'CARDINAL'), ('12979474e 100644', 'DATE'), ('#', 'CARDINAL'), ('mass_sph', 'WORK_OF_ART'), ('9cc8278ed', 'CARDINAL'), ('a3235580a 100644', 'DATE'), ('3', 'CARDINAL'), ('7', 'DATE'), ('5', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('2.21.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Multiplot', 'ORG'), ('Slice', 'NORP')]\n",
      "[]\n",
      "[('``def _', 'WORK_OF_ART'), ('nhi(field', 'ORG'), (\"yt.load(path+'snap_052.0.hdf5\", 'PERSON'), (\"'O I','C II','Si II','Fe II'],sampling_type='particle'\", 'WORK_OF_ART'), ('ProjectionPlot(ds', 'GPE'), ('ProjectionPlot(ds', 'GPE'), ('reg', 'GPE')]\n",
      "[]\n",
      "[('``P009 yt : [INFO     ', 'WORK_OF_ART'), ('2020-02-19', 'DATE'), ('1200', 'CARDINAL'), ('1200', 'CARDINAL'), ('trident_projection.py', 'CARDINAL'), ('69', 'CARDINAL'), ('yt-yt-4.0', 'NORP'), ('136', 'CARDINAL'), ('266', 'CARDINAL'), ('2020-02-19', 'DATE'), ('2020-02-19', 'DATE'), ('14', 'CARDINAL'), ('1', 'CARDINAL'), ('14', 'CARDINAL')]\n",
      "[('3rd', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2020-02-19', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt.enable_plugins', 'PERSON'), ('2018-06-27', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2D', 'QUANTITY'), ('99', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('Nelle Varoquaux', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[('PR', 'ORG')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[('YTQuantity', 'ORG'), ('YTQuantity', 'ORG')]\n",
      "[('``Tmax', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('cut_regions', 'ORG'), ('nathan', 'ORG')]\n",
      "[]\n",
      "[('IsolatedGalaxy', 'ORG')]\n",
      "[('Stephanie', 'PERSON'), ('1093', 'MONEY')]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PhasePlot', 'WORK_OF_ART')]\n",
      "[('<@U37DTBL6N', 'ORG')]\n",
      "[]\n",
      "[('cell_mass', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[]\n",
      "[('yt.enable_parallelism', 'PERSON'), ('MPI', 'ORG')]\n",
      "[('<@U37DTBL6N', 'ORG')]\n",
      "[]\n",
      "[('@UCYBQ5KPA', 'PERSON')]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[('@UCYBQ5KPA', 'PERSON'), ('https://gist.github.com/ngoldbaum/a2d10e77715d6f96065e3214045d338a', 'ORG')]\n",
      "[('@UCYBQ5KPA', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('CI', 'ORG')]\n",
      "[('CI', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('argh', 'PERSON'), ('two', 'CARDINAL')]\n",
      "[('tomorrow', 'DATE')]\n",
      "[('<@UDJU40MFY', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('3.7.4', 'CARDINAL')]\n",
      "[('3.8', 'CARDINAL'), ('3.6', 'CARDINAL'), ('3.7', 'CARDINAL')]\n",
      "[('3.7', 'CARDINAL'), ('3.6', 'CARDINAL')]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('sudo', 'PERSON'), ('dtruss -p &lt;pid', 'ORG'), ('pip install&gt', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('IsolatedGalaxy', 'ORG'), ('AttributeError', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('``AttributeError                            Traceback', 'WORK_OF_ART'), ('&lt;module&gt', 'ORG'), ('16', 'CARDINAL'), ('17', 'CARDINAL'), ('18', 'CARDINAL'), ('19', 'CARDINAL'), ('color_map', 'GPE'), ('color_log', 'GPE'), ('color_map', 'GPE'), ('color_log', 'GPE'), ('1915', 'DATE'), ('sample_type', 'ORG'), ('ten million', 'CARDINAL'), ('export_ply(self', 'GPE'), ('color_map', 'GPE'), ('color_log', 'GPE'), ('sample_type', 'ORG'), ('no_ghost', 'GPE'), ('1762', 'DATE'), ('sample_type', 'ORG'), ('color_map', 'GPE'), ('color_log', 'GPE'), ('AttributeError', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('``AttributeError                            Traceback', 'WORK_OF_ART'), ('&lt;module&gt', 'ORG'), ('16', 'QUANTITY'), ('17', 'CARDINAL'), ('18', 'CARDINAL'), ('19', 'CARDINAL'), ('color_map', 'GPE'), ('color_log', 'GPE'), ('no_ghost', 'GPE'), ('export_ply(self', 'GPE'), ('color_map', 'GPE'), ('color_log', 'GPE'), ('sample_type', 'ORG'), ('no_ghost', 'GPE'), ('YTSurface', 'ORG')]\n",
      "[(\"Nathan's PR\", 'ORG')]\n",
      "[('IPython', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3', 'CARDINAL'), ('fourth', 'ORDINAL'), ('4', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt.load', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('particle_identifier', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('HaloCatalog\\n\\n', 'LOC'), ('halos_ds', 'CARDINAL'), ('halos_ds', 'CARDINAL'), ('#', 'CARDINAL'), ('0th', 'ORDINAL'), ('#', 'CARDINAL'), ('#', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('data_ds', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('np', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('the night', 'TIME')]\n",
      "[('evening', 'TIME')]\n",
      "[]\n",
      "[('rockstar mentions', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('halo_list', 'ORG'), ('data_object', 'ORG')]\n",
      "[('EAGLE', 'NORP'), ('GADGET', 'ORG')]\n",
      "[('HaloCatalog', 'ORG')]\n",
      "[('EAGLE', 'NORP')]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('save_halos', 'ORG'), ('halo_list', 'ORG')]\n",
      "[('halo_list', 'ORG')]\n",
      "[('data_object', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('FYI', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('HOP', 'ORG'), ('FOF', 'ORG'), ('yt_astro_analysis', 'DATE')]\n",
      "[]\n",
      "[('@UCYBQ5KPA', 'PERSON')]\n",
      "[('500c', 'ORG')]\n",
      "[]\n",
      "[('HOP', 'ORG'), ('RAMSES', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[('@UREBTGDKM', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ytree', 'PERSON'), ('yt-4.0', 'NORP'), ('conda Python3', 'ORG'), ('https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb', 'PERSON'), (\"'yt.extern.six'\", 'DATE'), ('ytree', 'PERSON'), ('Python2', 'GPE')]\n",
      "[('ytree', 'PERSON'), ('six', 'CARDINAL'), ('six', 'CARDINAL'), ('ytree', 'PERSON'), ('setup.py', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Gen Chiaki', 'PERSON'), ('Georgia', 'GPE'), ('76', 'CARDINAL'), ('2**(k-1', 'CARDINAL'), ('python-3.6', 'WORK_OF_ART'), ('OS', 'ORG')]\n",
      "[('RAM', 'ORG')]\n",
      "[('almost 100%', 'PERCENT'), ('64', 'CARDINAL')]\n",
      "[]\n",
      "[('yt 3.4.0', 'PERSON')]\n",
      "[('RAM', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('conda', 'GPE')]\n",
      "[]\n",
      "[('conda channel', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('AMRVAC', 'ORG')]\n",
      "[('3d', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('2d', 'CARDINAL')]\n",
      "[('3d', 'CARDINAL')]\n",
      "[('2d', 'CARDINAL'), ('mid-December', 'DATE')]\n",
      "[('<@U042HLT7U', 'ORG'), ('@U37DTBL6N', 'PERSON')]\n",
      "[('3d', 'CARDINAL'), ('PE', 'ORG'), ('np.interp', 'ORG')]\n",
      "[('doc', 'ORG')]\n",
      "[('ds.point', 'ORG'), ('3', 'CARDINAL')]\n",
      "[('AMR', 'ORG'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[('AMR', 'ORG'), ('one', 'CARDINAL'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('smoothed_covering_grid', 'ORG')]\n",
      "[]\n",
      "[('the next few months', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UFAGQL7U0', 'PERSON')]\n",
      "[('annotate_marker', 'PERSON'), ('p.annotate_marker(halo_center', 'PERSON')]\n",
      "[('python3', 'GPE'), ('2.7', 'CARDINAL')]\n",
      "[]\n",
      "[('FYI', 'ORG'), ('python3', 'PRODUCT'), ('2.7', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('plot.set_background_color', 'WORK_OF_ART')]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('MC', 'PERSON')]\n",
      "[('kinda', 'ORG'), ('this morning', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[(\"plot_args={'colors':'k\", 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('second', 'ORDINAL'), ('these days', 'DATE')]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('export_obj', 'ORG'), ('color_map', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3d', 'CARDINAL'), ('close to 50%', 'PERCENT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('https://en.wikipedia.org/wiki/GlTF', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('Mihir', 'PERSON'), ('3', 'CARDINAL'), ('the next few days', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('1E12', 'DATE'), ('2', 'CARDINAL'), ('two', 'CARDINAL'), ('catalog_ds', 'CARDINAL'), ('particle_position or particle_velocity', 'LAW'), ('2', 'CARDINAL'), ('two', 'CARDINAL')]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[('annotate_cell_edges', 'ORG')]\n",
      "[]\n",
      "[('ProjectionPlot(ds', 'GPE'), ('0.523349', 'CARDINAL'), ('width=(40', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('2020-04-11', 'DATE')]\n",
      "[]\n",
      "[('FLASH', 'ORG')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('Mac', 'PERSON'), ('4.0.dev0', 'CARDINAL'), ('Cython', 'GPE'), ('doc/source/reference/api/', 'ORG'), ('doc', 'PERSON'), ('-I/Users/claytonstrawn/anaconda2/envs/myenv/', 'ORG'), ('-c yt/geometry/particle_oct_container.cpp', 'ORG'), ('libc++', 'PERSON'), ('libc++', 'ORG'), ('yt/geometry/particle_oct_container.cpp:632', 'ORG'), ('/Users/claytonstrawn/anaconda2/envs/myenv/lib/python2.7', 'ORG'), ('arrayobject.h:4', 'CARDINAL'), ('NumPy', 'NORP'), ('#', 'CARDINAL'), ('NumPy', 'PRODUCT'), ('ios', 'ORG'), ('#', 'CARDINAL'), ('ios', 'ORG'), ('2', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[('Cython', 'PERSON'), ('Cython', 'GPE')]\n",
      "[('ios', 'ORG'), ('C++', 'LANGUAGE'), ('C++', 'LANGUAGE'), ('Cython', 'GPE')]\n",
      "[('libc++', 'PERSON'), ('libc++', 'ORG'), ('`CFLAGS', 'WORK_OF_ART')]\n",
      "[('shrug', 'PERSON')]\n",
      "[('<@U7KU54SG5> <', 'ORG')]\n",
      "[]\n",
      "[('3D', 'ORG')]\n",
      "[('plot3d', 'GPE')]\n",
      "[]\n",
      "[('yup', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[('AMR', 'ORG')]\n",
      "[('yt.load_uniform_grid', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('doc around', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('3D', 'ORG'), ('load_uniform_grid', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('cylindrical_velocity_theta', 'WORK_OF_ART'), ('n_bins=256', 'GPE'), ('weight_field=', 'PERSON')]\n",
      "[]\n",
      "[('```\\n', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('cylindrical_velocity_theta', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('cylindrical_velocity_theta', 'WORK_OF_ART')]\n",
      "[('velocity_cylindrical_theta', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('<https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html|https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html', 'ORG')]\n",
      "[]\n",
      "[('0', 'CARDINAL')]\n",
      "[('my_array', 'ORG'), ('radius', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3d', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[('a nice weekend', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('len(item', 'PRODUCT'), ('self.ds.dimensionality', 'ORG'), ('self.ds.dimensionality', 'ORG'), ('3', 'CARDINAL')]\n",
      "[('2D', 'CARDINAL')]\n",
      "[]\n",
      "[('ds.domain_right_edge', 'GPE'), ('dims=(128', 'DATE'), ('128', 'CARDINAL'), ('1', 'DATE'), ('obj[\"rho\"].squeeze().to_ndarray', 'CARDINAL'), ('2D.', 'DATE')]\n",
      "[('3D', 'FAC')]\n",
      "[]\n",
      "[]\n",
      "[('np', 'ORG'), ('matplotlib.patches import FancyArrowPatch', 'ORG'), ('YTArray', 'ORG'), ('Streamlines', 'ORG'), ('#', 'CARDINAL'), ('#', 'CARDINAL'), ('Constants', 'ORG'), ('9.1e-31', 'CARDINAL'), ('# electron mass', 'MONEY'), ('2000', 'CARDINAL'), ('#', 'CARDINAL'), ('2000', 'DATE'), ('x0=', 'PRODUCT'), ('np.zeros(6', 'ORG'), ('#', 'CARDINAL'), ('6 dim', 'MONEY'), ('zeros', 'CARDINAL'), ('3', 'CARDINAL'), ('3', 'CARDINAL'), ('Cartesian', 'NORP'), ('2', 'CARDINAL'), ('YMIN', 'ORG'), ('ZMAX', 'ORG'), ('5', 'CARDINAL'), ('10', 'CARDINAL'), ('XMAX', 'ORG'), ('YMAX', 'ORG'), ('ZMAX', 'ORG'), ('np.meshgrid(x', 'ORG'), ('Theta=np.arctan2(y', 'WORK_OF_ART'), ('# je treba', 'MONEY'), ('je skutecne', 'PERSON'), ('poradi y', 'PRODUCT'), ('Byva', 'PERSON'), ('B_z[w', 'PERSON'), ('0.25*R[w]*np.cos(z[w', 'LANGUAGE'), ('#', 'CARDINAL'), ('np.sin(Theta', 'ORG'), ('Choose', 'FAC'), ('0.025', 'CARDINAL'), ('Dictionary', 'ORG'), ('# 3d array', 'MONEY'), ('#', 'CARDINAL'), ('yt.load_uniform_grid(data', 'PERSON'), ('B_R.shape', 'PERSON'), ('length_unit=\"Mpc', 'ORG'), ('bbox=', 'PERSON'), ('bbox', 'GPE'), ('nprocs=100', 'CARDINAL'), ('#', 'CARDINAL'), ('Define', 'MONEY'), ('N', 'ORG'), ('Scale', 'ORG'), ('Streamlines(ds', 'GPE'), ('pos', 'PERSON'), ('streamlines.integrate_through_volume', 'PERSON')]\n",
      "[('3', 'CARDINAL')]\n",
      "[('nprocs=100', 'CARDINAL'), ('load_uniform_grid', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2D', 'QUANTITY')]\n",
      "[('2D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('2D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.find_max', 'PERSON'), ('find_max', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('arrays', 'DATE'), ('z', 'PERSON'), ('50', 'CARDINAL'), ('6713', 'DATE'), ('6713', 'DATE'), ('0.5', 'CARDINAL'), ('0.5', 'CARDINAL'), ('50', 'CARDINAL')]\n",
      "[('<@UL61M6ZV5', 'ORG')]\n",
      "[('<@U015R1BHQGL', 'ORG')]\n",
      "[('np.where', 'LOC')]\n",
      "[]\n",
      "[('`x_axis=ad[\"x', 'WORK_OF_ART'), (\"store.result=(ds.current_<http://time.in|time.in>_units('s'),x_axis[asd[0]],y_axis[asd[0\", 'NORP')]\n",
      "[]\n",
      "[('Jack', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[(\"ad.quantities.min_location('field_axis_ratio\", 'PERSON'), ('ds.find_min', 'PERSON'), ('4', 'CARDINAL'), ('2', 'CARDINAL'), ('ds.find_min', 'PERSON')]\n",
      "[('PartType0_smoothed_Temperature', 'WORK_OF_ART'), ('K *', 'PRODUCT'), ('Kelvin', 'PERSON'), ('10', 'CARDINAL'), ('K*cm', 'PERSON'), ('Gizmo', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('One', 'CARDINAL'), ('yt.load', 'GPE')]\n",
      "[('SPH', 'ORG')]\n",
      "[('err', 'ORG')]\n",
      "[('3.x', 'DATE')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[('https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb', 'PERSON')]\n",
      "[('<@UTGE6J7C5', 'ORG')]\n",
      "[('AMReX', 'ORG'), ('AMReX', 'ORG')]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('2', 'CARDINAL'), ('MPI', 'ORG'), ('3', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('``sc = yt.create_scene(ds', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('512', 'CARDINAL'), ('512', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('a few minutes', 'TIME')]\n",
      "[('ts.piter', 'PRODUCT')]\n",
      "[]\n",
      "[('UB', 'ORG')]\n",
      "[]\n",
      "[('e.g. <https://github.com/cython/cython/issues/2316', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('3x', 'CARDINAL')]\n",
      "[('MPI', 'ORG')]\n",
      "[('MPI', 'ORG')]\n",
      "[]\n",
      "[('GizmoDataset', 'ORG')]\n",
      "[('OMP_NUM_THREADS', 'DATE'), ('MPI', 'ORG')]\n",
      "[('@U91855PA9', 'PERSON'), ('two', 'CARDINAL')]\n",
      "[('two', 'CARDINAL'), ('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[(\"region[[('PartType0\", 'ORG'), ('two', 'CARDINAL'), ('two', 'CARDINAL')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[('``In', 'WORK_OF_ART'), ('5', 'CARDINAL'), ('~/python3.8/lib/python3.8', 'ORG'), ('1', 'CARDINAL'), ('249', 'CARDINAL'), ('250', 'CARDINAL'), ('251', 'CARDINAL'), ('252', 'CARDINAL'), ('253', 'CARDINAL'), ('1191', 'CARDINAL'), ('~/python3.8/lib/python3.8', 'ORG'), ('788', 'CARDINAL'), ('789', 'CARDINAL'), ('790', 'CARDINAL'), ('791', 'CARDINAL'), ('792', 'CARDINAL'), ('330', 'CARDINAL'), ('332', 'CARDINAL'), ('333', 'CARDINAL'), ('334', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL'), ('two', 'CARDINAL')]\n",
      "[]\n",
      "[('ds.index', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('reg', 'ORG')]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON'), ('2', 'CARDINAL'), ('2020-02-03', 'DATE'), ('16:34:04,784 Allocating', 'WORK_OF_ART'), ('100%|', 'CARDINAL'), ('19/19', 'CARDINAL'), ('100%|', 'CARDINAL'), ('15.2 s', 'CARDINAL'), ('209', 'CARDINAL'), ('15.4', 'CARDINAL'), ('15.5 s', 'CARDINAL'), ('3', 'CARDINAL'), ('2020-02-03 16:34:37,845', 'DATE'), ('Max Value', 'PERSON'), ('4.15344e-21', 'CARDINAL'), ('31995.6347656250000000 31473.6640625000000000 28969.8867187500000000', 'QUANTITY'), ('378', 'CARDINAL'), ('84.1', 'CARDINAL'), ('462', 'CARDINAL'), ('459', 'CARDINAL'), ('28969.88671875', 'CARDINAL'), ('4', 'CARDINAL'), ('2020-02-03 16:34:51,248', 'DATE'), ('Max Value', 'PERSON'), ('4.15344e-21', 'CARDINAL'), ('31995.6347656250000000 31473.6640625000000000 28969.8867187500000000', 'CARDINAL'), ('5', 'CARDINAL'), ('6', 'CARDINAL'), ('347', 'CARDINAL'), ('67.9', 'CARDINAL'), ('415', 'CARDINAL'), ('411', 'CARDINAL'), ('Out[6', 'PERSON'), ('0.00047384', 'CARDINAL'), ('0.00034478', 'GPE'), ('0.00031614', 'DATE'), ('0.00034297', 'MONEY'), ('0.00032042', 'CARDINAL'), ('7', 'CARDINAL'), ('340', 'CARDINAL'), ('39.9', 'CARDINAL'), ('380', 'CARDINAL'), ('378', 'CARDINAL'), ('kpc\\n\\n', 'PERSON'), ('3.08568e+21', 'CARDINAL'), ('100000', 'CARDINAL'), ('kpc\\n\\n', 'PERSON')]\n",
      "[]\n",
      "[('``         17023 function', 'WORK_OF_ART'), ('16785', 'DATE'), ('0.380 seconds', 'TIME'), ('1', 'CARDINAL'), ('0.083', 'CARDINAL'), ('0.083', 'CARDINAL'), ('0.204', 'CARDINAL'), ('0.204', 'CARDINAL'), ('18', 'CARDINAL'), ('0.174', 'CARDINAL'), ('0.010', 'CARDINAL'), ('85', 'CARDINAL'), ('0.056', 'CARDINAL'), ('0.001', 'CARDINAL'), ('0.077', 'CARDINAL'), ('51', 'CARDINAL'), ('0.038', 'CARDINAL'), ('0.001', 'CARDINAL'), ('0.038', 'CARDINAL'), ('204', 'CARDINAL'), ('0.000', 'CARDINAL'), ('0.026', 'CARDINAL'), ('34', 'CARDINAL'), ('0.000', 'CARDINAL'), ('0.070', 'CARDINAL'), ('0.002', 'CARDINAL'), ('72', 'CARDINAL'), ('0.000', 'CARDINAL'), ('0.011', 'CARDINAL'), ('files.py:150(make_fid', 'CARDINAL'), ('72', 'CARDINAL'), ('0.000', 'CARDINAL'), ('102', 'CARDINAL'), ('0.010', 'CARDINAL'), ('0.000', 'CARDINAL'), ('0.010', 'CARDINAL'), ('119', 'CARDINAL'), ('0.007', 'CARDINAL'), ('0.000', 'CARDINAL'), ('0.008', 'CARDINAL'), ('357', 'CARDINAL'), ('0.006', 'CARDINAL'), ('0.000', 'CARDINAL'), ('0.006', 'CARDINAL'), ('0.000 dataset.py:283(shape', 'CARDINAL'), ('72', 'CARDINAL'), ('34', 'CARDINAL'), ('0.000', 'CARDINAL'), ('18', 'CARDINAL'), ('0.000', 'CARDINAL'), ('0.121', 'CARDINAL'), ('0.007', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('obj.galaxies[0].pos', 'CARDINAL'), (\"yt.load('/ufrc\", 'GPE')]\n",
      "[('6910456', 'CARDINAL'), ('27.731 seconds', 'TIME'), ('2882', 'CARDINAL'), ('6.290', 'CARDINAL'), ('6.925', 'CARDINAL'), ('2820', 'DATE'), ('1.940', 'CARDINAL'), ('0.001', 'CARDINAL'), ('1.940', 'CARDINAL'), ('194', 'CARDINAL'), ('1.805', 'CARDINAL'), ('0.009', 'CARDINAL'), ('6.070', 'CARDINAL'), ('2', 'CARDINAL'), ('1.271', 'CARDINAL'), ('0.636', 'CARDINAL'), ('8.841', 'CARDINAL'), ('4.421', 'CARDINAL'), ('2163', 'DATE'), ('1.115', 'CARDINAL'), ('0.001', 'CARDINAL'), ('1.125', 'CARDINAL'), ('0.001', 'CARDINAL'), ('0.938', 'CARDINAL'), ('0.000', 'CARDINAL'), ('2.447', 'CARDINAL'), ('0.019', 'CARDINAL'), ('1307', 'CARDINAL'), ('2158', 'CARDINAL'), ('0.545', 'CARDINAL'), ('0.000', 'CARDINAL'), ('6856', 'DATE'), ('0.503', 'CARDINAL'), ('0.000', 'CARDINAL'), ('0.503', 'CARDINAL'), ('7752', 'DATE'), ('0.474', 'CARDINAL'), ('0.000', 'CARDINAL'), ('0.822', 'CARDINAL'), ('0.000', 'CARDINAL'), ('25515/8511', 'DATE'), ('0.383', 'CARDINAL'), ('0.000', 'CARDINAL'), ('1.066', 'CARDINAL'), ('896', 'CARDINAL'), ('0.000', 'CARDINAL'), ('1.786', 'CARDINAL')]\n",
      "[('yup', 'ORG')]\n",
      "[('h5py', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Rust', 'ORG')]\n",
      "[]\n",
      "[('``py-spy record -n -o profile.svg -- python test.py', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('IMO', 'ORG')]\n",
      "[('bench.py', 'CARDINAL'), ('64', 'CARDINAL'), ('32', 'CARDINAL')]\n",
      "[]\n",
      "[('sudo', 'PERSON'), ('mac', 'ORG'), ('mac', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('HPC', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('second', 'ORDINAL'), ('ds.index', 'PRODUCT')]\n",
      "[]\n",
      "[('third', 'ORDINAL'), ('fourth', 'ORDINAL'), ('two', 'CARDINAL')]\n",
      "[]\n",
      "[('100%', 'PERCENT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('matt', 'PERSON')]\n",
      "[('get_data', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('h5py', 'ORG'), ('obj.galaxies[0].pos', 'CARDINAL'), (\"yt.load('/ufrc\", 'GPE'), ('h5py', 'ORG'), (\"File(f'/ufrc\", 'GPE'), (\"narayanan/kimockb/FIRE2/h113_HR_sn1dy300ro100ss/snapshot_172.{i}.hdf5'\", 'ORG')]\n",
      "[('h5py', 'ORG')]\n",
      "[('two', 'CARDINAL'), ('seconds', 'TIME'), ('18.276262521743774', 'CARDINAL')]\n",
      "[('``In', 'WORK_OF_ART'), ('10', 'CARDINAL'), ('313', 'CARDINAL'), ('72.3', 'CARDINAL'), ('386', 'CARDINAL'), ('383', 'CARDINAL')]\n",
      "[]\n",
      "[('matt', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('h5py', 'ORG')]\n",
      "[('Matt', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('matt', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('<@UTGE6J7C5', 'ORG')]\n",
      "[('13 MB', 'QUANTITY')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2014', 'DATE'), ('ProjectionPlot(ds', 'GPE'), ('0', 'CARDINAL')]\n",
      "[('g/cm^2 to', 'ORG'), ('Msun/pc^2', 'ORG')]\n",
      "[]\n",
      "[(\"```prj.set_unit('g/cm**3'\", 'WORK_OF_ART'), ('``', 'WORK_OF_ART'), ('` or something', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMReX', 'ORG'), ('Amr/Advection_F/Exec/SingleVortex', 'ORG')]\n",
      "[('<@U043BNA00', 'ORG')]\n",
      "[]\n",
      "[('tomorrow', 'DATE')]\n",
      "[]\n",
      "[('AMReX', 'ORG'), ('VisIt', 'PRODUCT'), ('VisIt', 'PERSON')]\n",
      "[('first', 'ORDINAL')]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[('SlicePlot', 'ORG'), ('center=', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('f2py', 'NORP')]\n",
      "[('numpy.apply_along_axis', 'PERSON'), ('f2py', 'NORP')]\n",
      "[('f2py', 'NORP')]\n",
      "[('yt.testing.fake_amr_ds', 'PERSON')]\n",
      "[('f2py', 'NORP')]\n",
      "[('f2py', 'NORP')]\n",
      "[('4', 'CARDINAL'), ('only 2', 'CARDINAL'), ('Illustris', 'PERSON'), ('SPH', 'ORG'), ('one', 'CARDINAL'), ('SPH', 'ORG'), ('EAGLE', 'ORG'), ('EAGLE', 'NORP'), ('58', 'CARDINAL')]\n",
      "[('<@UELT6G0F6', 'ORG')]\n",
      "[('Britton', 'PERSON'), ('90%', 'PERCENT'), ('the remaining', 'PERCENT'), ('10%', 'PERCENT'), ('#', 'CARDINAL'), ('#', 'CARDINAL'), ('``\\n\\nThat `node.ancestors', 'WORK_OF_ART'), ('``\\ndef mmp_uid(field', 'WORK_OF_ART'), ('###', 'MONEY'), ('#', 'CARDINAL'), ('Vmax', 'PERSON'), (\"ancestors[np.argmax([n['Vmax\", 'PRODUCT'), ('``\\n\\nIs there a relatively easy way that', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('Shea', 'PERSON'), ('``\\n\\nYou', 'WORK_OF_ART'), ('node.root.uid', 'ORG'), ('first', 'ORDINAL'), ('US', 'GPE'), ('next week', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('<@U1TMQQB38', 'ORG')]\n",
      "[('SZ', 'ORG'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[(\"``data_ds = yt.load('snapshot_000'\", 'WORK_OF_ART'), (\"finder_method='fof',output_dir='./groups_000\", 'ORG')]\n",
      "[('TreeFarm', 'ORG'), ('my_tree', 'CARDINAL')]\n",
      "[('4', 'CARDINAL'), ('5 my_tree', 'CARDINAL'), ('6 my_tree.trace_descendents(\\'Group\\',filename=\"All_halos/', 'TIME'), ('367', 'CARDINAL'), ('368', 'CARDINAL'), ('369', 'CARDINAL'), ('370', 'CARDINAL'), ('371', 'CARDINAL'), ('load_ds(self', 'CARDINAL'), ('kwargs', 'GPE'), ('186', 'CARDINAL'), ('187', 'CARDINAL'), ('188', 'CARDINAL'), ('kwargs', 'GPE'), ('189', 'CARDINAL'), ('190             ', 'QUANTITY'), ('kwargs', 'GPE'), ('13', 'QUANTITY'), ('10', 'CARDINAL'), ('40', 'CARDINAL'), ('14', 'CARDINAL'), ('15', 'CARDINAL'), ('kwargs', 'GPE'), ('16', 'QUANTITY'), ('17', 'CARDINAL'), ('load(*args', 'GPE'), ('kwargs', 'GPE'), ('84', 'QUANTITY'), ('85', 'QUANTITY'), ('1', 'CARDINAL'), ('86', 'CARDINAL'), ('candidates[0](*args', 'PRODUCT'), ('kwargs', 'GPE'), ('87', 'QUANTITY'), ('0', 'CARDINAL'), ('88', 'CARDINAL'), ('ytcfg.get(\"yt', 'PERSON'), ('index_ptype', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('2D', 'QUANTITY'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('<@UKMHHD7RA>', 'ORG')]\n",
      "[('2D', 'CARDINAL'), ('3D', 'CARDINAL')]\n",
      "[('PIL', 'ORG')]\n",
      "[('halo_catalog', 'ORG'), ('1785', 'DATE'), ('1486', 'DATE'), ('10%', 'PERCENT')]\n",
      "[]\n",
      "[]\n",
      "[('Enzo', 'ORG'), ('h5py', 'ORG')]\n",
      "[('anaconda', 'GPE'), ('h5py', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('~/anaconda3/lib/python3.7/', 'ORG'), ('fid_data', 'GPE'), ('159', 'CARDINAL'), ('160', 'CARDINAL'), ('161', 'CARDINAL'), ('162', 'CARDINAL'), ('h5py', 'ORG'), ('163', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('h5py', 'ORG')]\n",
      "[]\n",
      "[('h5py', 'ORG')]\n",
      "[('18446744073709551615', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('tomorrow', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('18446744073709551615', 'DATE'), ('2', 'CARDINAL')]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('Enzo', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UBJU11GJU', 'ORG')]\n",
      "[('load_uniform_grid', 'PERSON'), ('StreamDataset', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('#', 'CARDINAL'), ('dens.view(np.ndarray', 'GPE'), ('my_density_alias', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('yt4.x', 'GPE'), ('voronoi', 'GPE'), ('arepo data', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('john', 'PERSON')]\n",
      "[]\n",
      "[('AMR', 'ORG'), ('voronoi', 'GPE')]\n",
      "[]\n",
      "[('yt.funcs.mylog.setLevel(50', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('<@U0152DMHADP', 'ORG')]\n",
      "[]\n",
      "[('data_source', 'ORG'), ('https://yt-project.org/docs/dev/visualizing/plots.html#off-axis-projection-plots', 'ORG')]\n",
      "[]\n",
      "[('amr', 'ORG'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.6', 'CARDINAL')]\n",
      "[]\n",
      "[('tomorrow', 'DATE')]\n",
      "[('3.6', 'CARDINAL')]\n",
      "[]\n",
      "[('3.6.0', 'DATE')]\n",
      "[('3.6', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yup', 'ORG')]\n",
      "[]\n",
      "[('this afternoon', 'TIME')]\n",
      "[]\n",
      "[('https://i.imgur.com/I1jkgvc.png', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U012PG4BLKX>', 'ORG')]\n",
      "[('3.6', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('February', 'DATE'), ('4 months old', 'DATE')]\n",
      "[('one', 'CARDINAL'), ('4.0', 'CARDINAL'), ('the days', 'DATE')]\n",
      "[]\n",
      "[('<@UBQ983866', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[(\"mpl_kwargs={'bbox_inches':'tight\", 'WORK_OF_ART')]\n",
      "[('PhasePlot', 'ORG')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('colormap', 'GPE'), ('e.g', 'GPE'), ('SlicePlot', 'ORG'), ('kg', 'PERSON')]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('mac', 'ORG')]\n",
      "[('xcode/homebrew/macports', 'ORG')]\n",
      "[]\n",
      "[('@U91855PA9>', 'PERSON'), ('OSX', 'ORG'), ('MacOS', 'PRODUCT'), ('first', 'ORDINAL')]\n",
      "[]\n",
      "[('apple', 'ORG'), ('macs', 'ORG'), ('several years', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('C++', 'LANGUAGE'), ('C++', 'NORP')]\n",
      "[('c++', 'CARDINAL'), ('macs', 'PERSON')]\n",
      "[('<https://github.com/cython/cython/issues/2694', 'ORG')]\n",
      "[]\n",
      "[('conda', 'ORG')]\n",
      "[('conda', 'GPE'), ('conda', 'ORG')]\n",
      "[]\n",
      "[('all_data', 'PERSON'), ('ds.disk', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('force_orverride', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[('Nathan', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[('second', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Nathan', 'ORG')]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('nathan', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UGBUVTQS3', 'ORG')]\n",
      "[('yt.load', 'ORG'), ('AMREX', 'ORG'), ('2019-02-19', 'DATE'), ('Parameters', 'NORP'), ('1.25', 'CARDINAL'), ('2019-02-19', 'DATE'), ('50 200', 'CARDINAL'), ('2019-02-19', 'DATE'), ('Parameters', 'NORP'), ('0', 'CARDINAL'), ('2019-02-19', 'DATE'), ('Parameters', 'NORP'), ('0.5 2', 'CARDINAL'), ('0', 'CARDINAL')]\n",
      "[('keyword', 'ORG'), ('yt.load', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U7KU54SG5>', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[('2D', 'CARDINAL')]\n",
      "[('OffAxisSlicePlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('err <https://mail.python.org/archives/list/yt-users@python.org/', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('PR', 'ORG'), ('six', 'CARDINAL'), ('string_types', 'ORG')]\n",
      "[('Yajie', 'PERSON'), ('ProjectionPlot', 'ORG')]\n",
      "[('ProjectionPlot', 'ORG')]\n",
      "[('ProjectionPlot', 'ORG')]\n",
      "[('frb', 'ORG'), ('FixedResolutionBuffer', 'CARDINAL'), ('ProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2013', 'DATE')]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[('6.1.2', 'ORG'), ('conda', 'ORG')]\n",
      "[]\n",
      "[('conda-forge', 'ORG'), ('GMP', 'WORK_OF_ART')]\n",
      "[('<@UTBKY6VRB', 'ORG')]\n",
      "[]\n",
      "[('conda install -c conda-forge', 'ORG'), ('gmp==6.1.2', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('4000', 'CARDINAL'), ('4000 frb', 'QUANTITY')]\n",
      "[('set_buff_size(4000', 'PRODUCT'), ('4000', 'DATE')]\n",
      "[('4000', 'DATE'), ('4000', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('mpl_kwargs', 'WORK_OF_ART')]\n",
      "[('123', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('doc', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<https://github.com/yt-project/yt/blob/master/yt/fields/field_info_container.py#L300', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('force_override', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('FieldInfoContainer[field', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('add_field', 'GPE')]\n",
      "[('266', 'CARDINAL')]\n",
      "[('kwargs', 'GPE'), ('kwargs', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('grumbles', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.field_info.alias', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('particle_mass', 'ORG')]\n",
      "[('10', 'CARDINAL'), ('particle_mass', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('``hc.create', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('``\\ndef', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('halo_analysis', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('particle_index', 'ORG')]\n",
      "[]\n",
      "[('ytree', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('<@U043BNA00', 'ORG')]\n",
      "[('SlicePlot', 'PRODUCT'), ('zeros', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('zero', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('0.1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('zero', 'CARDINAL'), ('zero', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('min', 'ORG'), ('max', 'PERSON')]\n",
      "[('min', 'ORG'), ('max', 'PERSON')]\n",
      "[('np.nextafter', 'PERSON')]\n",
      "[('np.nextafter(np.nanmin(image', 'ORG'), ('np.nanmax(image', 'GPE')]\n",
      "[]\n",
      "[('10', 'CARDINAL')]\n",
      "[('0.1', 'CARDINAL')]\n",
      "[('min', 'ORG'), ('max', 'PERSON')]\n",
      "[('zero', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('min', 'ORG'), ('max', 'PERSON'), ('greater than zero', 'CARDINAL')]\n",
      "[('min', 'ORG'), ('max is &lt;= 0', 'ORG')]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[('symlog', 'PERSON')]\n",
      "[]\n",
      "[('min', 'ORG'), ('0.0', 'CARDINAL'), ('max', 'PERSON'), ('10', 'CARDINAL')]\n",
      "[]\n",
      "[('PhasePlots', 'ORG'), ('0', 'CARDINAL')]\n",
      "[]\n",
      "[('max', 'PERSON'), ('one', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('about a year ago', 'DATE')]\n",
      "[]\n",
      "[('last august', 'DATE')]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('June', 'DATE')]\n",
      "[]\n",
      "[('NCSA', 'FAC'), ('this afternoon', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('32', 'CARDINAL')]\n",
      "[]\n",
      "[('64 bits', 'QUANTITY')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('GIZMO', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[('morton =', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('python3', 'PRODUCT'), ('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[('@UDJSRNAMQ', 'PERSON')]\n",
      "[]\n",
      "[('3d', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('669', 'CARDINAL')]\n",
      "[('ptest', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2032', 'MONEY')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('the athena vtk format', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U8FUK8KCL', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2033', 'MONEY'), ('two', 'CARDINAL')]\n",
      "[('first', 'ORDINAL'), ('today', 'DATE'), ('one', 'CARDINAL'), ('3.5', 'CARDINAL')]\n",
      "[('ds.sphere(center', 'CARDINAL'), ('10', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[('IndexError                                Traceback', 'PERSON'), ('1', 'CARDINAL'), ('10', 'CARDINAL'), ('2', 'CARDINAL'), ('3', 'CARDINAL'), ('kwargs', 'GPE'), ('65', 'CARDINAL'), ('66', 'CARDINAL'), ('67', 'CARDINAL'), ('kwargs', 'GPE'), ('68', 'CARDINAL'), ('69', 'CARDINAL'), ('kwargs', 'GPE'), ('462', 'CARDINAL'), ('particle_specific_angular_momentum_%s” % axis] *\\n    463                            data[“all”', 'WORK_OF_ART'), ('464', 'CARDINAL'), ('465', 'CARDINAL'), ('466', 'CARDINAL'), ('462', 'CARDINAL'), ('particle_specific_angular_momentum_%s” % axis] *\\n    463                            data[“all”', 'WORK_OF_ART'), ('464', 'CARDINAL'), ('465', 'CARDINAL'), ('466', 'CARDINAL'), ('280                 ', 'QUANTITY'), ('281', 'CARDINAL'), ('282', 'CARDINAL'), ('283', 'CARDINAL'), ('284', 'CARDINAL'), ('824', 'CARDINAL'), ('825', 'CARDINAL'), ('826', 'CARDINAL'), ('827', 'CARDINAL'), ('828', 'CARDINAL'), ('1036', 'DATE'), ('super(YTArray', 'PERSON'), ('self).__getitem__(item', 'PERSON'), ('YTQuantity(ret', 'GPE'), ('boolean', 'NORP'), ('0', 'CARDINAL'), ('3', 'CARDINAL'), ('boolean', 'NORP'), ('331', 'CARDINAL')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[('the last six months', 'DATE'), ('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('100%|', 'CARDINAL'), ('10/10', 'CARDINAL'), ('00:00&lt;00:00', 'CARDINAL'), ('3', 'CARDINAL'), ('854', 'CARDINAL'), ('find_max', 'GPE'), ('622', 'CARDINAL'), ('self).__call__(field', 'PERSON'), ('sample_fields', 'PERSON'), ('578', 'CARDINAL'), ('self).__call__(field', 'PERSON'), ('sample_fields', 'PERSON'), ('65', 'CARDINAL'), ('65', 'CARDINAL'), ('528', 'CARDINAL'), ('223', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('2.7.1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2.7.1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('INTERESTING', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('IsolatedGalaxy', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ValueError', 'ORG')]\n",
      "[]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Intriguing', 'ORG')]\n",
      "[('these days', 'DATE')]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('find_max(self', 'DATE'), ('852', 'CARDINAL'), ('853', 'CARDINAL'), ('854', 'CARDINAL'), ('855', 'CARDINAL'), ('856', 'CARDINAL'), ('620', 'CARDINAL'), ('621', 'CARDINAL'), ('622', 'CARDINAL'), ('self).__call__(field', 'PERSON'), ('sample_fields', 'GPE'), ('623', 'CARDINAL'), ('len(rv', 'PERSON'), ('1', 'CARDINAL'), ('624', 'CARDINAL'), ('sample_fields', 'PRODUCT'), ('576', 'CARDINAL'), ('577     ', 'QUANTITY'), ('578', 'CARDINAL'), ('self).__call__(field', 'PERSON'), ('sample_fields', 'GPE'), ('579', 'CARDINAL'), ('len(rv', 'PERSON'), ('1', 'CARDINAL'), ('580', 'CARDINAL'), ('kwargs', 'GPE'), ('63', 'CARDINAL'), ('64', 'CARDINAL'), ('65', 'CARDINAL'), ('66', 'CARDINAL'), ('67', 'CARDINAL'), ('63', 'CARDINAL'), ('64', 'CARDINAL'), ('65', 'CARDINAL'), ('66', 'CARDINAL'), ('67', 'CARDINAL'), ('input_units', 'GPE'), ('526', 'CARDINAL'), ('527', 'CARDINAL'), ('unyt_array', 'ORG'), ('528', 'CARDINAL'), ('coerce_iterable_units(input_array', 'PERSON'), ('529', 'CARDINAL'), ('Input', 'NORP'), ('221', 'CARDINAL'), ('222', 'CARDINAL'), ('223', 'CARDINAL'), ('IterableUnitCoercionError(input_object', 'PRODUCT'), ('224', 'CARDINAL'), ('225', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('tomorrow', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('IsolatedGalaxy', 'ORG'), ('Gizmo', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('the remaining hours of your day', 'TIME')]\n",
      "[]\n",
      "[('CGS', 'ORG'), ('1', 'CARDINAL'), ('CGS', 'ORG'), ('2', 'CARDINAL'), ('code_length', 'ORG'), ('CGS', 'ORG'), ('1', 'CARDINAL')]\n",
      "[('a year ago', 'DATE'), ('CGS', 'ORG'), ('Presumably', 'ORG'), ('first', 'ORDINAL'), ('a DerivedQuantity on *', 'WORK_OF_ART'), ('one', 'CARDINAL'), ('first', 'ORDINAL'), ('first', 'ORDINAL'), ('second', 'ORDINAL')]\n",
      "[('Length', 'GPE'), ('kpc', 'ORG')]\n",
      "[]\n",
      "[('max', 'PERSON'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Britton', 'PERSON')]\n",
      "[('np\\n\\n', 'PERSON'), ('centimeter', 'GPE'), ('erg', 'GPE'), ('s', 'GPE'), ('kg', 'PERSON'), ('K', 'GPE'), ('#', 'CARDINAL'), ('2.17896', 'CARDINAL'), ('10**-11.*erg', 'CARDINAL'), ('phi=1.*s', 'GPE'), ('9.109', 'CARDINAL'), ('30', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('0.190775', 'CARDINAL'), ('n_one*m_delta_n*(1', 'CARDINAL'), ('3.469', 'CARDINAL'), ('Emissivity', 'ORG')]\n",
      "[]\n",
      "[('z.append(float(words[15', 'GPE')]\n",
      "[]\n",
      "[('0.5219305', 'CARDINAL'), ('0.3977057', 'CARDINAL'), ('50', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Shai', 'PERSON'), ('One', 'CARDINAL'), ('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('DM', 'ORG')]\n",
      "[('<@UATLRVA65', 'ORG')]\n",
      "[('<@UAU5L1R6G', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U01046WNS02', 'ORG')]\n",
      "[('yt-4.0', 'NORP'), ('pip install', 'PERSON'), ('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('mac', 'ORG')]\n",
      "[('conda', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('pip install', 'PERSON'), ('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[('yt4.0', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('conda', 'ORG')]\n",
      "[('#', 'CARDINAL'), ('/Users/clm/miniconda3/envs/yt4:', 'ORG'), ('#\\n#', 'MONEY'), ('Version                   Build', 'PRODUCT'), ('bzip2', 'PRODUCT'), ('7.68.0', 'CARDINAL'), ('h8da9a1a_0', 'PRODUCT'), ('h46ab8bc_1002', 'PRODUCT'), ('2.25.0', 'CARDINAL'), ('ipython_genutils          ', 'LOC'), ('jpeg                      ', 'PERSON'), ('libblas                   ', 'PRODUCT'), ('16_openblas', 'CARDINAL'), ('16_openblas', 'CARDINAL'), ('7.68.0', 'CARDINAL'), ('9.0.1', 'CARDINAL'), ('1', 'CARDINAL'), ('2', 'CARDINAL'), ('1.15', 'CARDINAL'), ('16_openblas', 'CARDINAL'), ('4.7.3', 'CARDINAL'), ('9.0.1', 'CARDINAL'), ('6.1', 'CARDINAL'), ('1.1.1e', 'CARDINAL'), ('conda-forge\\n', 'GPE'), ('pip                       20.0.2', 'PRODUCT'), ('six', 'CARDINAL'), ('conda-forge', 'FAC')]\n",
      "[]\n",
      "[('mac', 'ORG'), ('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Python', 'ORG')]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('later in the week', 'DATE'), ('3.6', 'CARDINAL'), ('yt-4.0', 'NORP')]\n",
      "[('<@UAURGNEE6', 'ORG')]\n",
      "[('MPL', 'ORG'), ('one', 'CARDINAL'), ('MPL', 'ORG')]\n",
      "[]\n",
      "[('FRB', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('FRB', 'ORG'), ('Presumably', 'ORG'), ('FRB', 'ORG')]\n",
      "[]\n",
      "[('Gotcha', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UFMRUG2KA>', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG'), ('10', 'CARDINAL'), ('bounding_box=[[-x', 'ORG'), ('x],[-x', 'ORG'), ('x],[-x', 'ORG')]\n",
      "[('3.5', 'CARDINAL'), ('conda', 'ORG'), ('repodata.json', 'ORG')]\n",
      "[('3.5.1', 'CARDINAL'), ('<', 'NORP'), ('conda', 'ORG'), ('about 3.4.1', 'CARDINAL')]\n",
      "[('@U4ESETP43', 'ORG')]\n",
      "[('3.5', 'CARDINAL')]\n",
      "[]\n",
      "[('3.5', 'CARDINAL')]\n",
      "[]\n",
      "[('3.6', 'CARDINAL'), ('3.7', 'CARDINAL')]\n",
      "[('anaconda inc', 'ORG')]\n",
      "[]\n",
      "[('@U4ESETP43', 'ORG')]\n",
      "[('2.7', 'CARDINAL'), ('3.6', 'CARDINAL'), ('3.7', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.6', 'CARDINAL')]\n",
      "[('3.7', 'CARDINAL')]\n",
      "[('3.8', 'CARDINAL')]\n",
      "[('3.7', 'CARDINAL'), ('a year or so :', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('3.6', 'CARDINAL'), ('3.7', 'CARDINAL')]\n",
      "[('conda install', 'ORG'), ('repodata.json', 'ORG')]\n",
      "[('partyparrot', 'PERSON')]\n",
      "[('conda install -c conda-forge python=3.7', 'ORG')]\n",
      "[('conda', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.7', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('3.6', 'FAC')]\n",
      "[]\n",
      "[('3.6', 'CARDINAL'), ('3.7', 'CARDINAL'), ('3.5', 'CARDINAL')]\n",
      "[('conda-forge', 'ORG'), ('two', 'CARDINAL'), ('python3', 'PERSON')]\n",
      "[]\n",
      "[('CI', 'ORG')]\n",
      "[('3.8', 'CARDINAL')]\n",
      "[('Nathan', 'ORG')]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb', 'PERSON')]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[('EWAH', 'ORG')]\n",
      "[('a ton', 'QUANTITY'), ('RAM', 'ORG')]\n",
      "[]\n",
      "[('RAM', 'ORG')]\n",
      "[]\n",
      "[(\"index_ptype='PartType0\", 'PRODUCT')]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('rockstar yes', 'ORG')]\n",
      "[]\n",
      "[('2019-05-30', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('19', 'CARDINAL'), ('98', 'CARDINAL'), ('YTOutputNotIdentified(args', 'GPE'), ('kwargs', 'GPE')]\n",
      "[('http://ytini.com/downloads/withPythonSOP.hipnc', 'CARDINAL'), ('Python SOP', 'ORG')]\n",
      "[]\n",
      "[('IsolatedGalaxy.tgz', 'ORG'), (\"`yt.load('IsolatedGalaxy/galaxy0030/galaxy0030'\", 'WORK_OF_ART')]\n",
      "[('100', 'CARDINAL'), ('32062', 'CARDINAL'), ('100 32062', 'CARDINAL'), ('6262k', 'DATE'), ('1', 'CARDINAL'), ('24', 'CARDINAL'), ('0 10', 'CARDINAL'), ('1 240', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('strcat(oppwf', 'PERSON'), (\"2.7182818284590452354'\", 'MONEY')]\n",
      "[('a million', 'CARDINAL')]\n",
      "[('Slice', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('One', 'CARDINAL')]\n",
      "[]\n",
      "[('``mass_gas_profile = yt.create_profile(sphere', 'WORK_OF_ART'), ('cell_mass', 'ORG'), ('n_bins=60', 'GPE')]\n",
      "[('``ValueError: bins must be monotonically', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('3.6', 'CARDINAL')]\n",
      "[]\n",
      "[('```&gt;ds', 'ORG'), ('0.521550', 'CARDINAL'), ('0.539467', 'CARDINAL'), ('radius=(10', 'GPE')]\n",
      "[]\n",
      "[('``mass_gas_profile = yt.create_profile(sphere', 'WORK_OF_ART'), ('cell_mass', 'ORG'), ('n_bins=60', 'GPE')]\n",
      "[('6', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('``print(sphere)\\n&gt;YTSphere', 'WORK_OF_ART'), ('center=[9.87174778e+25', 'ORG'), ('1.14221664e+26 1.18145563e+26', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProjectionPlot(ds', 'GPE'), ('`&lt;yt.visualization.plot_window', 'ORG')]\n",
      "[('NxN', 'ORG')]\n",
      "[('@UF4561YH4', 'PERSON')]\n",
      "[('EAGLE', 'ORG')]\n",
      "[('3', 'CARDINAL'), ('Mpc', 'PERSON'), ('Wendland-C2', 'PRODUCT'), ('1.825742', 'CARDINAL')]\n",
      "[('yesterday', 'DATE')]\n",
      "[]\n",
      "[('half', 'CARDINAL')]\n",
      "[('EAGLE', 'ORG')]\n",
      "[('Eq', 'PERSON'), ('3', 'CARDINAL'), ('9', 'CARDINAL'), ('Eq', 'PERSON'), ('3', 'CARDINAL'), ('Eq', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('Trident', 'WORK_OF_ART'), ('Trident', 'PERSON')]\n",
      "[]\n",
      "[('<@U012V1CJ6JV', 'ORG')]\n",
      "[('DM', 'ORG')]\n",
      "[('SPH', 'ORG')]\n",
      "[('Nathan Golbaum', 'PERSON'), ('np', 'ORG'), ('bbox', 'GPE'), ('yt.load_uniform_grid(data', 'PERSON'), ('256]*3, length_unit', 'ORG'), ('bbox=', 'PERSON')]\n",
      "[]\n",
      "[('DM', 'ORG'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('SPH', 'ORG')]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('x_log=0', 'CARDINAL'), ('n_bins=500', 'PERSON'), ('xaxis', 'GPE')]\n",
      "[]\n",
      "[('x_log=0', 'PRODUCT'), ('xaxis', 'ORG')]\n",
      "[('<@U015R1BHQGL', 'ORG'), ('PR', 'ORG')]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[('AMRVAC', 'ORG'), ('3.6', 'CARDINAL')]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[('<@UFGGCP9R6', 'ORG')]\n",
      "[('two', 'CARDINAL'), ('two', 'CARDINAL'), ('two', 'QUANTITY'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('2', 'CARDINAL'), ('1', 'CARDINAL'), ('ART', 'ORG')]\n",
      "[]\n",
      "[('1', 'CARDINAL')]\n",
      "[]\n",
      "[('<@UC6L85LBB', 'ORG')]\n",
      "[('AMReX', 'ORG')]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('GDB', 'ORG')]\n",
      "[]\n",
      "[('10-100', 'CARDINAL')]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[('10', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('HPC', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[('GDB', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('travis CI', 'PERSON'), ('NCSA', 'LOC')]\n",
      "[('HPC', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('the day', 'DATE')]\n",
      "[('a nice day', 'DATE')]\n",
      "[('HPC', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Nx', 'ORG'), ('Ny', 'PERSON'), ('Nz', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('One', 'CARDINAL'), ('np', 'ORG'), ('Streamlines', 'ORG'), ('Mpc\\n', 'GPE'), ('mpl_toolkits.mplot3d', 'GPE'), ('2', 'CARDINAL'), ('64', 'CARDINAL'), ('cm/s', 'ORG'), ('cm/s', 'PRODUCT'), ('v_z', 'ORG'), ('cm/s', 'PRODUCT'), ('yt.load_uniform_grid(data', 'PERSON'), ('length_unit=\"Mpc', 'ORG'), ('bbox=', 'PERSON'), ('100', 'CARDINAL'), ('np.random.random((N,3))*scale', 'CARDINAL'), ('Streamlines(ds', 'GPE'), ('pos', 'PERSON'), ('velocity_y', 'PERSON'), ('#', 'CARDINAL'), ('3D', 'CARDINAL'), ('3D', 'LAW'), ('0.0', 'CARDINAL'), ('axis=1)&gt;1', 'GPE'), ('#', 'CARDINAL')]\n",
      "[('3D', 'CARDINAL')]\n",
      "[('@UF1UC363Z', 'ORG')]\n",
      "[]\n",
      "[('arepo', 'GPE'), ('gas’,‘’density', 'PRODUCT'), ('one', 'CARDINAL')]\n",
      "[('#', 'CARDINAL'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[('@U9CE5D9LZ', 'PRODUCT'), ('DustDensity', 'ORG')]\n",
      "[('HPC', 'ORG'), ('last week', 'DATE'), ('miniconda', 'PERSON'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[('FYI', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('3.4.0', 'CARDINAL'), ('2', 'CARDINAL'), ('Conda', 'GPE'), ('3.4.0', 'CARDINAL'), ('Conda', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('e.g. 1.2x R_vir', 'ORG'), ('1x', 'CARDINAL')]\n",
      "[]\n",
      "[('caesar', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('FOF', 'ORG')]\n",
      "[]\n",
      "[('Caesar', 'PERSON')]\n",
      "[('mika rafierferantosa', 'PERSON'), ('caesar', 'PERSON')]\n",
      "[('caesar', 'PERSON'), ('@U91855PA9', 'PERSON')]\n",
      "[('Caesar', 'PERSON'), ('AHF', 'PERSON'), ('FoF', 'ORG'), ('Caesar', 'PERSON')]\n",
      "[('SWIFT', 'ORG')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[('FakeCaesar', 'ORG')]\n",
      "[('1.2', 'CARDINAL')]\n",
      "[('caesar', 'GPE')]\n",
      "[('Mika', 'NORP'), ('Caesar', 'PERSON')]\n",
      "[]\n",
      "[('49', 'CARDINAL'), ('2018-07-24', 'DATE'), (\"YTRegion'&gt\", 'PERSON'), ('yt/scripts/iyt in &lt;module&gt', 'ORG'), ('1', 'CARDINAL'), ('axes_unit', 'GPE'), ('weight_field', 'GPE'), ('field_parameters', 'ORG'), ('data_source', 'ORG'), ('proj_style', 'ORG'), ('1454', 'DATE'), ('isinstance(ds', 'GPE'), ('YTSpatialPlotDataset', 'GPE'), ('137', 'CARDINAL'), ('138', 'CARDINAL'), ('0', 'CARDINAL'), ('139', 'CARDINAL'), ('YTInvalidFieldType(invalid_fields', 'CARDINAL'), ('140', 'CARDINAL'), ('141', 'CARDINAL'), ('SlicePlot', 'ORG'), ('ProjectionPlot', 'ORG'), ('OffAxisProjectionPlot', 'ORG'), ('ParticlePlot', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[('AMR', 'ORG'), ('3D', 'CARDINAL')]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[('2D', 'QUANTITY'), ('AMR', 'ORG')]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('~50%', 'PERCENT')]\n",
      "[('SPH', 'ORG'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[('intel', 'ORG'), ('Loaded Modules', 'PERSON'), ('1', 'CARDINAL'), ('2', 'CARDINAL'), ('3', 'CARDINAL'), ('git/2.14.1\\n\\n', 'PERSON'), ('``\\n\\n', 'WORK_OF_ART'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('23', 'CARDINAL'), ('#', 'CARDINAL'), ('24', 'CARDINAL'), ('25', 'CARDINAL'), ('yt.funcs', 'ORG'), ('26', 'CARDINAL'), ('27', 'CARDINAL'), ('45', 'CARDINAL'), ('46', 'CARDINAL'), ('yt.extern.tqdm', 'ORG'), ('47', 'CARDINAL'), ('yt.units.yt_array import YTArray', 'ORG'), ('YTQuantity', 'ORG'), ('48', 'CARDINAL'), ('49', 'CARDINAL'), ('1', 'CARDINAL'), ('yt.units', 'NORP'), ('unit_symbols', 'ORG'), ('2', 'CARDINAL'), ('3', 'CARDINAL'), ('YTQuantity', 'ORG'), ('5', 'CARDINAL'), ('12 #', 'CARDINAL'), ('13 #-----------------------------------------------------------------------------\\n', 'MONEY'), ('14', 'CARDINAL'), ('YTQuantity', 'ORG'), ('15', 'CARDINAL'), ('37', 'CARDINAL'), ('None,)*4', 'ORG'), ('39', 'CARDINAL'), ('yt.units.unit_object import Unit', 'ORG'), ('40', 'CARDINAL'), ('UnitRegistry', 'ORG'), ('41', 'CARDINAL'), ('32', 'CARDINAL'), ('prefixable_units', 'GPE'), ('latex_prefixes', 'GPE'), ('33', 'CARDINAL'), ('34', 'CARDINAL'), ('35', 'CARDINAL'), ('UnitRegistry', 'ORG'), ('36', 'CARDINAL'), ('UnitParseError\\n\\n~/yt/yt', 'ORG'), ('19', 'CARDINAL'), ('20', 'CARDINAL'), ('21', 'CARDINAL'), ('22', 'CARDINAL'), ('six', 'CARDINAL'), ('23', 'CARDINAL'), ('desika.narayanan/yt/yt/utilities/lib/', 'ORG')]\n",
      "[]\n",
      "[('intel', 'ORG')]\n",
      "[]\n",
      "[('intel', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('Loaded Modules', 'PERSON'), ('1', 'CARDINAL'), ('3', 'CARDINAL'), ('1', 'CARDINAL'), ('3.6.5', 'CARDINAL'), ('Inc.|', 'GPE'), ('29 2018', 'DATE'), ('1', 'CARDINAL'), ('2', 'CARDINAL')]\n",
      "[('PITA', 'ORG'), ('intel', 'ORG')]\n",
      "[('save_as_dataset', 'PERSON'), ('AMR', 'ORG')]\n",
      "[('AMR', 'ORG')]\n",
      "[('hahaha', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('halos_ds', 'CARDINAL')]\n",
      "[('finder_method=‘hop', 'PERSON'), ('hc.create', 'PERSON')]\n",
      "[('DM', 'ORG')]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[('HOP', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('HaloCatalog', 'PERSON')]\n",
      "[('halos_ds.all_data', 'PERSON')]\n",
      "[]\n",
      "[('particle_mass', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('One', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Gadget2', 'PERSON'), ('``yt.utilities.exceptions', 'WORK_OF_ART'), ('11999.999, 11999.999, 11999.999', 'DATE'), ('12000', 'CARDINAL'), ('12000', 'CARDINAL'), ('12000', 'CARDINAL'), ('just only one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('yesterday', 'DATE'), ('np', 'ORG'), ('yt.load(snapshot', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('0.75 0.75 0.75]', 'PERCENT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('zlim', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('NewHorizon', 'ORG')]\n",
      "[]\n",
      "[('fed', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[('ART/AMR', 'ORG'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[('Nathan', 'ORG')]\n",
      "[('@UTE2603SQ', 'PERSON')]\n",
      "[]\n",
      "[('@U37DTBL6N', 'PERSON'), ('cmap', 'PERSON')]\n",
      "[]\n",
      "[('<@U0HJX841Z', 'ORG')]\n",
      "[('a little bit ago', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('ds.sphere(center', 'PRODUCT')]\n",
      "[('10', 'CARDINAL'), ('Mpccm', 'PERSON')]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('between two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('kiloparsecs', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('2', 'CARDINAL')]\n",
      "[('Kriti', 'PERSON')]\n",
      "[('ProjectionPlot(ds', 'GPE'), (\"weight_field=('cell'\", 'ORG')]\n",
      "[('CIC', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2019', 'DATE')]\n",
      "[(\"Yohan Dubois'\", 'PERSON'), ('PhD', 'WORK_OF_ART')]\n",
      "[('Ramses', 'PRODUCT')]\n",
      "[(\"New Horizon's\", 'GPE'), ('New Horizon', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[('cphyc', 'ORG')]\n",
      "[('``def', 'WORK_OF_ART'), (\"particles\\n    '\", 'PERSON'), ('data.ds.cosmological_simulation==1', 'ORG'), (\"data['io','particle_birth_time\", 'ORG'), ('0', 'CARDINAL'), (\"data['io','particle_birth_time\", 'ORG'), (\"data['io','particle_birth_time\", 'ORG'), ('0', 'CARDINAL'), (\"data['io','particle_birth_time\", 'ORG'), (\"data['io','particle_birth_time\", 'ORG'), ('0', 'CARDINAL'), ('0', 'CARDINAL'), ('filter=', 'CARDINAL'), ('0', 'CARDINAL'), ('0', 'CARDINAL'), ('0', 'CARDINAL'), (\"data['io','particle_birth_time'] &lt\", 'ORG'), (\"data['io','particle_birth_time\", 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProjectionPlot', 'ORG')]\n",
      "[('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[('ProjectionPlot(ds', 'GPE'), ('140', 'CARDINAL'), ('2019-11-28', 'DATE'), ('2019-11-28', 'DATE'), ('0.500986', 'MONEY'), ('2019-11-28', 'DATE'), ('0.500986', 'MONEY'), ('2019-11-28', 'DATE'), ('0.500986', 'MONEY'), ('2019-11-28', 'DATE'), ('0.500986', 'MONEY'), ('2019-11-28', 'DATE'), ('800', 'CARDINAL'), ('2019-11-28', 'DATE'), ('1', 'CARDINAL'), ('plot1.pan_rel((-0.1', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('plot1', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('this afternoon', 'TIME'), ('one', 'CARDINAL'), ('CGS', 'ORG')]\n",
      "[('code_mass/code_length**2', 'ORG')]\n",
      "[('2 minute', 'TIME')]\n",
      "[('``\\n', 'WORK_OF_ART'), ('cmap=\"magma', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProjectionPlot', 'ORG'), ('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[('OffAxisProjectionPlot', 'ORG')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('Hunh', 'ORG'), ('ProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[(\"imshow`'ing\", 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('yup', 'ORG')]\n",
      "[]\n",
      "[('argh', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('last night', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Heisengbug', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('matplotlib_style_context', 'NORP')]\n",
      "[('yt.funcs.matplotlib_style_context', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('``In', 'WORK_OF_ART'), ('7', 'CARDINAL'), ('2020-06-15', 'DATE'), ('mpa/temp/mrmgehlm/yt/yt/geometry/particle_geometry_handler.py', 'ORG'), ('131', 'CARDINAL'), ('132', 'CARDINAL'), ('133', 'CARDINAL'), ('&lt;module&gt', 'ORG'), ('1', 'CARDINAL'), ('all_data(self', 'GPE'), ('find_max', 'PERSON'), ('928', 'CARDINAL'), ('929', 'CARDINAL'), ('930', 'CARDINAL'), ('931', 'CARDINAL'), ('find_max', 'PERSON'), ('932', 'CARDINAL'), ('503', 'CARDINAL'), ('504', 'CARDINAL'), ('505', 'CARDINAL'), ('506', 'CARDINAL'), ('507', 'CARDINAL'), ('25', 'CARDINAL'), ('26', 'CARDINAL'), ('27', 'CARDINAL'), ('28', 'CARDINAL'), ('29     ', 'QUANTITY'), ('138', 'QUANTITY'), ('139', 'CARDINAL'), ('140', 'CARDINAL'), ('141', 'CARDINAL'), ('142', 'CARDINAL'), ('dont_cache', 'NORP'), ('os.access(wdir', 'GPE'), ('177', 'CARDINAL'), ('%s', 'ORG'), ('mask_threshold, count_threshold', 'ORG'), ('178', 'CARDINAL'), ('0', 'CARDINAL'), ('179', 'CARDINAL'), ('mask &gt;= 2) &amp', 'ORG'), ('self.regions.particle_counts &gt', 'ORG'), ('roughly %', 'PERCENT'), ('181', 'CARDINAL'), ('100', 'CARDINAL'), ('ParticleBitmap', 'ORG')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('last week', 'DATE'), ('yt-4.0', 'NORP')]\n",
      "[('only a matter of days', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('Galactic', 'PERSON')]\n",
      "[]\n",
      "[('CCA', 'ORG')]\n",
      "[]\n",
      "[('40', 'CARDINAL')]\n",
      "[]\n",
      "[('ds.point([x', 'GPE')]\n",
      "[]\n",
      "[('YT', 'GPE')]\n",
      "[]\n",
      "[('<@U37DTBL6N', 'ORG')]\n",
      "[]\n",
      "[('@U44SW7J11', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('data_projection', 'ORG'), ('data_transform', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('data_projection', 'ORG')]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('Stampede2', 'WORK_OF_ART')]\n",
      "[('more than 1 node', 'CARDINAL'), ('first', 'ORDINAL'), ('TG', 'NORP'), ('#', 'CARDINAL'), ('np\\nimport yt', 'ORG'), ('yt.enable_parallelism', 'PERSON'), ('1250', 'CARDINAL'), ('np.zeros(NUM', 'ORG'), ('np.zeros(NUM', 'ORG'), ('np.zeros(NUM', 'ORG'), ('np.zeros(NUM', 'ORG'), ('print(val', 'CARDINAL'), ('0 to 1249', 'DATE'), ('96', 'CARDINAL'), ('val+4', 'PERSON'), ('#', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('Mihir', 'PERSON')]\n",
      "[('Britton', 'PERSON')]\n",
      "[]\n",
      "[(\"PhasePlot(sphere,'density','temperature','cell_mass\", 'PERSON'), ('``\\n\\n', 'WORK_OF_ART'), ('2D', 'CARDINAL')]\n",
      "[]\n",
      "[('arrays', 'DATE')]\n",
      "[('CosmologyOutputRedshift', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('niba=', 'ORG')]\n",
      "[('800', 'CARDINAL'), ('800', 'CARDINAL')]\n",
      "[('OffAxisProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[('yt.off_axis_projection', 'CARDINAL'), ('resolution=100', 'CARDINAL'), ('ImageArray', 'ORG'), ('ImageArray', 'ORG')]\n",
      "[('Corey', 'PERSON'), ('OffAxisProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@UBGR3S1FF', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('http://use.yt/upload/aed7f64d', 'PERSON'), ('sub_halos', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[('pip install -e', 'PERSON')]\n",
      "[]\n",
      "[('ds.field_list', 'ORG'), (\"slc.annotate_quiver('nlc_x','nlc_y\", 'PRODUCT'), ('Buffer', 'ORG'), ('1', 'CARDINAL'), ('2', 'CARDINAL')]\n",
      "[]\n",
      "[('velocity_y', 'PERSON')]\n",
      "[('<@UFAGQL7U0', 'PERSON')]\n",
      "[('AMR', 'ORG'), ('exodus II', 'WORK_OF_ART')]\n",
      "[('connect1', 'PRODUCT')]\n",
      "[('exodus II', 'ORG'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('connect1', 'PRODUCT')]\n",
      "[('`yt upload my_file.tar.gz', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('TreeNodes', 'ORG')]\n",
      "[]\n",
      "[('europe', 'LOC')]\n",
      "[]\n",
      "[('Gizmo', 'PRODUCT'), (\"Desktop/School/Research/snapshot_160.0.hdf5'\", 'ORG'), ('2', 'CARDINAL'), ('0', 'CARDINAL'), ('0', 'CARDINAL')]\n",
      "[]\n",
      "[('Georgia Tech', 'ORG'), ('First', 'ORDINAL')]\n",
      "[('h5', 'PRODUCT'), ('two', 'CARDINAL'), ('two', 'CARDINAL'), ('numy', 'ORG')]\n",
      "[]\n",
      "[('conda environment', 'ORG')]\n",
      "[('Atharva', 'ORG'), ('``git clone <', 'WORK_OF_ART'), ('pip install -e', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('add_source', 'ORG'), ('MeshSource', 'ORG')]\n",
      "[]\n",
      "[('<https://matplotlib.org/3.1.1/tutorials/colors/colormap-manipulation.html', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U34G2M0KU', 'ORG')]\n",
      "[('@UARDXRMDM', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('7', 'CARDINAL'), ('8 years ago', 'DATE')]\n",
      "[('chris hayward', 'PERSON')]\n",
      "[('2015', 'DATE')]\n",
      "[('yea', 'ORG'), ('2015', 'ORDINAL')]\n",
      "[('btw', 'ORG')]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('20', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('this afternoon', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[('``    sc = yt.create_scene(ds', 'WORK_OF_ART'), ('70', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('``    sc = yt.create_scene(ds', 'WORK_OF_ART'), ('70', 'CARDINAL')]\n",
      "[]\n",
      "[('hehe… yea', 'PERSON'), ('12', 'CARDINAL')]\n",
      "[]\n",
      "[('``  File', 'ORG'), ('56', 'CARDINAL'), ('70', 'CARDINAL'), ('213', 'CARDINAL'), ('56', 'CARDINAL'), ('70', 'CARDINAL'), ('213', 'CARDINAL'), ('504', 'CARDINAL'), ('357', 'CARDINAL'), ('504', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('56', 'CARDINAL')]\n",
      "[('ds.index', 'ORG')]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('MemoryError', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('20-30x', 'DATE')]\n",
      "[]\n",
      "[('3.5', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U7EV2LFAT', 'PERSON')]\n",
      "[('zero', 'CARDINAL')]\n",
      "[('e.g\\n', 'ORG'), ('``import matplotlib.pyplot as plt\\n', 'WORK_OF_ART'), ('DivergingNorm', 'WORK_OF_ART'), ('z', 'PERSON')]\n",
      "[]\n",
      "[('Max(|minval|', 'GPE'), ('|maxval|', 'PERSON')]\n",
      "[('one', 'CARDINAL'), ('zero', 'CARDINAL'), ('zero', 'CARDINAL')]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PATH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('pip', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[('OS', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('pip install', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('EnvironmentError', 'ORG'), ('Errno 13', 'EVENT')]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SHELL', 'ORG')]\n",
      "[]\n",
      "[('Kriti', 'GPE'), ('2.7', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('HOME/.local/bin:$PATH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('<http://swcarpentry.github.io/shell-novice/', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('conda install -c conda-forge', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('more than one Python', 'PERCENT')]\n",
      "[]\n",
      "[('DM', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('EnzoGrid_???', 'WORK_OF_ART')]\n",
      "[('the years', 'DATE')]\n",
      "[]\n",
      "[('3.5', 'CARDINAL')]\n",
      "[]\n",
      "[('3.5.1', 'QUANTITY')]\n",
      "[('http://yt-project.org/doc/installing.html#installing-yt-from-source', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('WTF', 'ORG')]\n",
      "[('3.5', 'CARDINAL'), ('conda -', 'ORG')]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[('oct by oct', 'DATE')]\n",
      "[]\n",
      "[('oct', 'DATE')]\n",
      "[('MeshIdentifier', 'ORG')]\n",
      "[]\n",
      "[('process_octree', 'WORK_OF_ART')]\n",
      "[('process_octree', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('particle_deposition_functions', 'WORK_OF_ART')]\n",
      "[('OctreeSubset', 'ORG')]\n",
      "[('process_octree', 'ORG')]\n",
      "[('OctreeSubset', 'ORG')]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('4096', 'CARDINAL'), ('3', 'CARDINAL')]\n",
      "[('one', 'CARDINAL'), ('~275 gB.', 'ORG')]\n",
      "[('16', 'CARDINAL'), ('MPI', 'ORG'), ('1', 'CARDINAL'), ('128', 'CARDINAL')]\n",
      "[('2', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('YTQuadTreeProj', 'PERSON')]\n",
      "[('10', 'CARDINAL'), ('68', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Cori', 'PRODUCT'), ('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt/funcs.py', 'ORG')]\n",
      "[('OS', 'ORG')]\n",
      "[('one', 'CARDINAL')]\n",
      "[('70', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('RAM', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('128', 'CARDINAL'), ('3', 'CARDINAL')]\n",
      "[('128', 'CARDINAL'), ('8', 'CARDINAL'), ('17', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('gc.collect', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('RAM', 'ORG')]\n",
      "[]\n",
      "[('<@U043BNA00', 'ORG')]\n",
      "[('theb iggest', 'PERSON')]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('SLURM', 'ORG')]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@property', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('between the start and the end of that function', 'DATE'), ('4', 'CARDINAL'), ('82', 'QUANTITY'), ('max', 'PERSON')]\n",
      "[('grid metadata', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('FWIW', 'ORG')]\n",
      "[('all_data', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('apply_along_axis', 'PRODUCT'), ('ytQuantity', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('vy = sp.mean(\"velocity_y', 'PERSON'), ('vy', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('YTEP 3', 'FAC')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('One', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[]\n",
      "[('YTArray', 'ORG'), ('0.75994256', 'CARDINAL'), ('0.75994258', 'CARDINAL'), ('0.75994255', 'MONEY'), ('0.35083768', 'GPE'), ('0.27859338', 'CARDINAL'), ('0.03780791', 'CARDINAL')]\n",
      "[('YTArray', 'ORG'), ('0.99999891', 'CARDINAL'), ('0.99999891', 'CARDINAL'), ('0.99999891', 'CARDINAL'), ('0.46166219', 'CARDINAL'), ('0.36664719', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('H_p0', 'PRODUCT')]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('IsolatedGalaxy', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('0.75', 'CARDINAL'), ('0.99', 'CARDINAL')]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[('One', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('the H II', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Trident', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.smoothed_grid', 'ORG')]\n",
      "[('JPG', 'ORG'), ('PNG', 'ORG')]\n",
      "[('ps', 'ORG'), ('Agg', 'PERSON'), ('JPG', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('later today', 'DATE')]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('10', 'CARDINAL')]\n",
      "[('10', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('10', 'CARDINAL'), ('4', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProfilePlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[('``ds.add_deposited_particle_field((\"star', 'WORK_OF_ART'), ('n_bins= 256', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG'), ('cell_mass', 'ORG')]\n",
      "[('star_mass', 'ORG')]\n",
      "[('star_mass', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('star_mass', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[('CIC', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('star_mass', 'ORG')]\n",
      "[('particle_radius', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('bin.', 'PERSON'), ('particle_radius', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('bin', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('star_mass', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('cylindrical_velocity_theta', 'ORG'), ('two', 'CARDINAL')]\n",
      "[('last 5 hours', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('```sphere.set_field_parameter(\"normal\"', 'WORK_OF_ART'), ('unit_l', 'GPE'), ('unit_l', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('unit_l', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('particle_ones', 'WORK_OF_ART')]\n",
      "[('particle_ones', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Two', 'CARDINAL')]\n",
      "[('load_uniform_grid', 'PERSON'), ('magnetic_field_x/y/z', 'ORG'), ('create_vector_fields', 'ORG')]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('load_uniform_grid', 'GPE')]\n",
      "[]\n",
      "[('second', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('load_uniform_grid', 'GPE')]\n",
      "[('OII', 'ORG'), ('OIII', 'ORG'), ('O++', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('y', 'PERSON'), ('y', 'PRODUCT'), ('z', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('2D arrays', 'DATE')]\n",
      "[('three', 'CARDINAL'), ('1D arrays', 'DATE')]\n",
      "[('3', 'CARDINAL'), ('1D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('1D arrays', 'DATE'), ('3D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('load_uniform_grid', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('magnetic_field_x', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('three', 'CARDINAL')]\n",
      "[('ds.add_gradient_field', 'GPE')]\n",
      "[('TM', 'ORG')]\n",
      "[]\n",
      "[('np', 'ORG')]\n",
      "[('<@UB95PD8BA', 'ORG')]\n",
      "[('@U4ESETP43', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('vertex', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('calculate_flux', 'GPE')]\n",
      "[('question--', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('6', 'CARDINAL')]\n",
      "[('generating vertex', 'ORG')]\n",
      "[]\n",
      "[('Aaaaaah', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('radial_velocity', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('radial_velocity', 'ORG')]\n",
      "[('``\\ndef my_field(field', 'WORK_OF_ART'), ('FieldDetector', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('radial_velocity', 'ORG')]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('0.3', 'CARDINAL'), ('0.4', 'CARDINAL'), ('0.5', 'CARDINAL')]\n",
      "[('function=_gas_density_in', 'PERSON'), ('units=\"Msun/kpc**3', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[(\"get_field_parameter('center\", 'PERSON')]\n",
      "[('the next few days', 'DATE')]\n",
      "[('one', 'CARDINAL')]\n",
      "[('Enzo', 'ORG'), ('next week', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[('90 degree', 'QUANTITY')]\n",
      "[('https://github.com/trident-project/trident/pull/105', 'PERSON')]\n",
      "[('@U8FUK8KCL', 'ORG')]\n",
      "[('this week', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('write_png', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.r[-0.2,0.0,0.5', 'ORG'), ('ValueError', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('One', 'CARDINAL')]\n",
      "[('@U37DTBL6N', 'PERSON'), ('one', 'CARDINAL'), ('set_zlim', 'PRODUCT')]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('NERSC', 'PERSON'), ('yt-4.0', 'NORP'), ('NERSC', 'PERSON'), ('MPI', 'ORG'), ('pip install -e', 'PERSON'), ('2.7', 'CARDINAL'), ('January 1st, 2020', 'DATE'), ('Python as Python 2.7', 'ORG'), ('pip', 'ORG'), ('Python 2.7', 'ORG'), ('10:47:58', 'TIME'), ('2019', 'DATE'), ('MPI', 'ORG'), ('1', 'CARDINAL')]\n",
      "[('MPI', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('cykdtree', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('False', 'WORK_OF_ART'), ('RTDFLAG', 'PERSON')]\n",
      "[]\n",
      "[('Cameron', 'PERSON'), ('https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb', 'PERSON'), ('setup.py', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('yt’s setup.py', 'PERSON')]\n",
      "[]\n",
      "[('2.7', 'CARDINAL'), ('January 1st, 2020', 'DATE'), ('Python as Python 2.7', 'ORG'), ('pip', 'ORG'), ('Python 2.7', 'ORG'), ('path:/global/u2/c/cstrawn/yt/setup.py', 'ORG'), ('file:///global/u2/c/cstrawn/yt\\n    Running', 'ORG'), ('Using', 'WORK_OF_ART'), ('dependency_links.txt', 'CARDINAL'), ('Compiling', 'GPE'), ('/global/u2/c/cstrawn/cykdtree/cykdtree/kdtree.pxd', 'ORG'), ('Compiling', 'GPE'), ('/global/u2/c/cstrawn/cykdtree/cykdtree/kdtree.pxd', 'ORG'), ('yt/utilities/lib', 'ORG'), ('2/2', 'CARDINAL'), ('11:40:48', 'TIME'), ('2019', 'DATE'), ('MPI', 'ORG'), ('1', 'CARDINAL'), ('/global/u2/c/cstrawn/', 'ORG'), ('179', 'CARDINAL'), ('315', 'CARDINAL'), ('131', 'CARDINAL'), ('294', 'CARDINAL'), ('resolve_one', 'ORG'), ('226', 'CARDINAL'), ('self.use_user_site', 'ORG'), ('self.finder', 'ORG'), ('382', 'CARDINAL'), ('158', 'CARDINAL'), ('530', 'CARDINAL'), ('prepare_metadata', 'GPE'), ('609', 'CARDINAL'), (\"run_egg_info\\n    command_desc='python setup.py\", 'PERSON'), ('761', 'CARDINAL'), ('call_subprocess\\n    %', 'ORG'), ('command_desc', 'ORG'), ('proc.returncode', 'ORG'), ('cwd', 'GPE')]\n",
      "[('/global/u2/c/cstrawn/', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Python', 'ORG')]\n",
      "[]\n",
      "[('cykdtree', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('cykdtree', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('second', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('<@U010U6B8WF5', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('yesterday', 'DATE')]\n",
      "[('3.5', 'CARDINAL'), ('@U37DTBL6N', 'PERSON')]\n",
      "[]\n",
      "[('@U37DTBL6N', 'PERSON'), (\"`' 8\", 'DATE'), ('d\\\\n', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UBJU11GJU>', 'ORG'), ('https://i.imgur.com/wQZOzDW.png', 'PERSON')]\n",
      "[('<@UBJU11GJU', 'ORG')]\n",
      "[]\n",
      "[('<@UBJU11GJU', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('VAR_DESC_RE', 'GPE')]\n",
      "[('<@UBJU11GJU> <https://github.com/yt-project/yt/pull/2202/files#r263888458', 'ORG')]\n",
      "[('1', 'CARDINAL'), ('sci/astro/home/cjstrawn/yt/yt/data_objects/static_output.py', 'ORG'), ('855', 'CARDINAL'), ('find_max', 'GPE'), ('sci/astro/home/cjstrawn/yt/yt/data_objects/derived_quantities.py', 'ORG'), ('638', 'CARDINAL'), ('self).__call__(field', 'PERSON'), ('sample_fields', 'PERSON'), ('sci/astro/home/cjstrawn/yt/yt/data_objects/derived_quantities.py', 'ORG'), ('594', 'CARDINAL'), ('self).__call__(field', 'PERSON'), ('sample_fields', 'PERSON'), ('sci/astro/home/cjstrawn/yt/yt/data_objects/derived_quantities.py', 'ORG'), ('75', 'CARDINAL'), ('sto.result', 'GPE'), ('sci/astro/home/cjstrawn/yt/yt/data_objects/derived_quantities.py', 'ORG'), ('603', 'CARDINAL'), ('process_chunk', 'GPE'), ('data[field].size &gt', 'ORG'), ('0', 'CARDINAL'), ('sci/astro/home/cjstrawn/yt/yt/data_objects/data_containers.py', 'ORG'), ('257', 'CARDINAL'), ('sci/astro/home/cjstrawn/yt/yt/data_objects/data_containers.py', 'ORG'), ('1411', 'DATE'), ('get_data', 'GPE'), ('sci/astro/home/cjstrawn/yt/yt/geometry/geometry_handler.py', 'ORG'), ('227', 'CARDINAL'), ('sci/astro/home/cjstrawn/yt/yt/utilities/io_handler.py', 'ORG'), ('218', 'CARDINAL'), ('field_r', 'PERSON'), ('self._read_particle_fields(chunks', 'ORG'), ('sci/astro/home/cjstrawn/yt/yt/frontends/tipsy/io.py', 'ORG'), ('237', 'CARDINAL')]\n",
      "[]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@UECQ4DEA1', 'ORG')]\n",
      "[('<@UMKGP9EJE', 'ORG')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('FieldDetector', 'ORG'), ('0', 'DATE'), ('0', 'CARDINAL')]\n",
      "[('FieldDetector', 'ORG')]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Gadget/OWLS', 'ORG'), ('two', 'CARDINAL')]\n",
      "[('SPH', 'ORG'), ('Illustris', 'GPE'), ('llustrisTNG', 'PERSON'), ('75', 'CARDINAL'), ('EAGLE', 'ORG'), ('AREPO', 'ORG'), ('YT', 'ORG'), ('TNG100-3', 'DATE'), ('2', 'CARDINAL'), ('np\\nimport h5py', 'ORG'), ('FlatLambdaCDM', 'PRODUCT'), ('TNG100-3', 'DATE'), ('#', 'CARDINAL'), ('33 #', 'CARDINAL'), ('#', 'CARDINAL'), ('h5py', 'ORG'), (\"header['OmegaBaryon'],Tcmb0=2.725\", 'GPE'), ('Ob(rs', 'ORG'), ('#', 'CARDINAL'), ('critical_density', 'ORG'), ('M_sun/cosmo.h/((u.kpc/cosmo.h)**3', 'ORG'), ('#', 'CARDINAL'), ('#', 'CARDINAL'), ('h5py', 'ORG'), ('m.append(mass', 'CARDINAL'), ('elif', 'PERSON'), (\"np.hstack(posz).astype('float64\", 'PERSON'), (\"np.hstack(dens).astype('float64\", 'PERSON'), ('particle_mass', 'ORG'), ('smoothing_length', 'ORG'), ('16', 'CARDINAL'), ('600 #', 'MONEY'), ('ds.domain_right_edge', 'GPE'), ('dims=[N', 'GPE'), ('N', 'GPE'), ('yt.load_particles', 'PERSON'), ('yt.load', 'GPE'), ('SPH', 'ORG'), ('YT', 'ORG')]\n",
      "[('SPH', 'ORG'), ('1e+10 Msun/h', 'ORG'), ('kpc', 'GPE'), ('Mpc', 'PERSON')]\n",
      "[('3H²/8piG *Ob\\n\\n', 'FAC'), ('kpc', 'ORG')]\n",
      "[('YT', 'ORG')]\n",
      "[('2', 'CARDINAL'), ('3', 'CARDINAL'), ('20', 'CARDINAL'), ('1+z)³', 'CARDINAL'), ('23', 'CARDINAL'), ('2', 'CARDINAL'), ('55.6', 'CARDINAL'), ('3', 'CARDINAL'), ('20', 'CARDINAL')]\n",
      "[]\n",
      "[('kpc', 'ORG')]\n",
      "[('3H²/8piG *', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('the past few days', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('One', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[('PR', 'ORG')]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[('H_p[0', 'PERSON'), ('1]_[_mass', 'GPE'), ('number_density', 'ORG'), ('He_p[0', 'PRODUCT'), ('1', 'CARDINAL'), ('2]__[mass', 'CARDINAL'), ('number_density', 'ORG')]\n",
      "[('today', 'DATE')]\n",
      "[]\n",
      "[('0it', 'ORDINAL'), ('00:00', 'TIME'), ('120', 'CARDINAL')]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[('@UD9L1D44T', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PR', 'ORG'), ('num_neighbors', 'ORG'), ('num_neighbors', 'ORG'), ('KDTree', 'PRODUCT'), ('```Interpolating (gather) SPH field', 'WORK_OF_ART'), ('100%|', 'CARDINAL'), ('215990000/216000000', 'CARDINAL'), ('34:58<00:00', 'CARDINAL'), ('130008.65it/s', 'ORG'), ('0.6', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two weeks', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('sc.show', 'GPE')]\n",
      "[]\n",
      "[('VolumeSource', 'ORG'), ('``sc = yt.create_scene(ds', 'WORK_OF_ART'), ('2020-04-23', 'DATE'), ('center=[1.543e+24', 'NORP'), ('0', 'CARDINAL'), ('0.81649658', 'CARDINAL'), ('1.5 1.5', 'CARDINAL'), ('None\\n\\tresolution:(512', 'PERSON'), ('512', 'CARDINAL'), ('lt;Lens', 'CARDINAL'), ('center=[1.543e+24', 'NORP'), ('0', 'CARDINAL')]\n",
      "[('``sc.show', 'WORK_OF_ART'), ('2020-04-23', 'DATE'), ('2020-04-23 23:40:05,924', 'DATE'), ('2020-04-23', 'DATE'), ('2020-04-23', 'DATE'), ('343', 'CARDINAL'), ('344', 'CARDINAL'), ('345', 'CARDINAL'), ('346', 'CARDINAL'), ('347', 'CARDINAL'), ('repr_png_(self', 'GPE'), ('926', 'CARDINAL'), ('927', 'CARDINAL'), ('928', 'CARDINAL'), ('929', 'CARDINAL'), ('930', 'CARDINAL'), ('png\\n\\nE:\\\\SOFTWARES\\\\Anaconda\\\\lib\\\\site-packages\\\\yt\\\\data_objects\\\\image_array.py', 'PERSON'), ('write_png(self', 'DATE'), ('sigma_clip', 'FAC'), ('clip_ratio', 'ORG'), ('287', 'CARDINAL'), ('288', 'CARDINAL'), ('289', 'CARDINAL'), ('290', 'CARDINAL'), ('291', 'CARDINAL'), ('rescale(self', 'GPE'), ('amax', 'PERSON'), ('240', 'CARDINAL'), ('3', 'CARDINAL'), ('3', 'CARDINAL'), ('241', 'CARDINAL'), ('242', 'CARDINAL'), ('0.0', 'CARDINAL'), ('1.0', 'CARDINAL'), ('243', 'CARDINAL'), ('244', 'CARDINAL'), ('clip(*args', 'GPE'), ('kwargs', 'GPE'), ('clip(a', 'GPE'), ('a_min', 'GPE'), ('a_max', 'GPE'), ('kwargs', 'GPE'), ('2084', 'DATE'), ('a_min', 'GPE'), ('a_max', 'GPE'), ('kwargs', 'GPE'), ('wrapfunc(obj', 'GPE'), ('59', 'CARDINAL'), ('60', 'CARDINAL'), ('61', 'CARDINAL'), ('62', 'CARDINAL'), ('TypeError', 'ORG'), ('63', 'CARDINAL'), ('min', 'PERSON'), ('max', 'PERSON'), ('kwargs', 'GPE'), ('130', 'CARDINAL'), ('131', 'CARDINAL'), ('132', 'CARDINAL'), ('max', 'PERSON'), ('kwargs', 'GPE'), ('keepdims', 'QUANTITY'), ('kwargs', 'GPE'), ('83     ', 'QUANTITY'), ('84', 'QUANTITY'), ('85', 'CARDINAL'), ('ufunc(*args', 'GPE'), ('kwargs', 'GPE'), ('86', 'CARDINAL'), ('87', 'CARDINAL'), ('Numpy', 'PERSON'), ('1.17.0, 2019-02-24', 'DATE'), ('RuntimeError', 'ORG'), ('Support for the %s ufunc with %i inputs has not been\"', 'WORK_OF_ART'), ('1401                     ', 'TIME'), ('YTArray', 'ORG'), ('np.array(out_arr', 'CARDINAL'), (\"the &lt;ufunc 'clip'&gt\", 'ORG'), ('3', 'CARDINAL'), ('YTArray', 'ORG'), ('data0029', 'PERSON'), ('1.96740872e+26', 'CARDINAL'), ('right_edge=[3.93481744e+26 3.93481744e+26 3.93481744e+26', 'PERSON'), ('0.81649658', 'CARDINAL'), ('1.5 1.5', 'CARDINAL'), ('None\\n\\tresolution:(512', 'PERSON'), ('512', 'CARDINAL'), ('lt;Lens', 'CARDINAL')]\n",
      "[]\n",
      "[('ParticlePlot', 'ORG')]\n",
      "[]\n",
      "[('EAGLE', 'ORG'), ('Illustris', 'GPE'), ('TNG', 'ORG'), ('two', 'CARDINAL'), ('EAGLE', 'NORP'), ('58', 'CARDINAL'), ('TNG', 'ORG'), ('~270', 'NORP'), ('EAGLE', 'ORG'), ('Illustris', 'GPE'), ('Mpc', 'PERSON')]\n",
      "[('first', 'ORDINAL'), ('2nd', 'ORDINAL'), ('first', 'ORDINAL'), (\"a[0]['tree\", 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[('0.675810317993164', 'CARDINAL'), ('0.6937116980552673', 'CARDINAL')]\n",
      "[('TreeNode[343', 'CARDINAL'), ('343', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('343', 'CARDINAL')]\n",
      "[(\"a[0]['tree\", 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ParticleProjectionPlot', 'WORK_OF_ART'), ('Splatting', 'WORK_OF_ART'), ('800', 'CARDINAL'), ('800 mesh', 'MONEY')]\n",
      "[('@U91855PA9', 'PERSON'), ('buff_size', 'PRODUCT')]\n",
      "[]\n",
      "[('1600', 'DATE')]\n",
      "[]\n",
      "[('FRB', 'ORG')]\n",
      "[]\n",
      "[('FRB', 'ORG')]\n",
      "[]\n",
      "[('plot.set_buff_size', 'WORK_OF_ART')]\n",
      "[(\"plot.save(mpl_kwargs={'dpi':400\", 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('validate_plot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('source.tfh.set_bounds((0.16e-3,1.2e-3', 'CARDINAL'), ('sc.save', 'PRODUCT')]\n",
      "[('<@UC6L85LBB', 'ORG')]\n",
      "[('source.tfh.set_bounds((0.16e-3,1.2e-3', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('http://yt-project.org/docs/dev/examining/loading_data.html#generic-array-data', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('2D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('sam skillman', 'PERSON'), ('CT scan cross', 'ORG')]\n",
      "[('the day', 'DATE'), ('astro', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('DICOM', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('particle_type', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('particle_type', 'ORG')]\n",
      "[('DM', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('particle_type', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('angular momentum vector', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('particle_ftype', 'ORG')]\n",
      "[('particle_type', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('particle_type', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('yt-4.0', 'NORP'), ('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('yt-4.0', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[]\n",
      "[('Python', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U37DTBL6N', 'ORG')]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('more than one', 'CARDINAL')]\n",
      "[('AMR', 'ORG')]\n",
      "[('4', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('YTRegion', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('vertex', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('YTQuantity', 'ORG')]\n",
      "[('one', 'CARDINAL')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('sig', 'ORG')]\n",
      "[]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PlotWindow', 'FAC'), ('7.5', 'CARDINAL'), ('7.5', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('one', 'CARDINAL')]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[]\n",
      "[('WindowPlotMPL', 'PERSON'), ('ImagePlotMPL', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('ProfilePlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('fields= selected_field', 'PRODUCT'), ('4', 'CARDINAL'), ('1', 'CARDINAL'), ('40', 'CARDINAL'), ('help(prj', 'MONEY'), ('#', 'MONEY')]\n",
      "[('OffAxisProjectionPlot', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('<https://i.imgur.com/UUnHZur.png', 'PERSON')]\n",
      "[('1', 'CARDINAL'), ('2', 'CARDINAL'), ('&lt;yt.visualization.plot_window', 'ORG'), ('3', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('MCVE', 'ORG')]\n",
      "[]\n",
      "[('```\\n    prj = yt', 'WORK_OF_ART'), ('fields= selected_field', 'PRODUCT'), ('4', 'CARDINAL'), (\"prj.set_xlabel('kpc\", 'PERSON')]\n",
      "[]\n",
      "[('TypeError', 'ORG'), ('set_ylabel', 'PERSON'), (\"`      prj.set_ylabel('kpc'\", 'ORG'), ('size=15', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('12', 'CARDINAL')]\n",
      "[('set_font', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('``\\n    for axis in', 'WORK_OF_ART'), ('ProjectionPlot', 'ORG'), ('extent=[0', 'ORG'), ('500', 'CARDINAL'), ('0', 'DATE'), ('500', 'CARDINAL')]\n",
      "[('one', 'CARDINAL'), ('ProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('this tonight', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('cbax.yaxis.label', 'WORK_OF_ART')]\n",
      "[('athena', 'ORG'), ('2d', 'CARDINAL'), ('LinePlot', 'ORG'), ('1st', 'ORDINAL')]\n",
      "[('<@U01046WNS02', 'ORG'), ('ProfilePlot', 'ORG'), ('ProfilePlot', 'ORG'), ('ProfilePlot', 'ORG')]\n",
      "[('64', 'CARDINAL'), ('0.1', 'CARDINAL')]\n",
      "[('#', 'CARDINAL'), ('#', 'CARDINAL'), ('#', 'CARDINAL'), ('#', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('np', 'ORG'), ('mpl_toolkits.mplot3d', 'GPE'), ('mpl_toolkits.mplot3d', 'LOC'), ('proj3d', 'PRODUCT'), ('YTArray', 'ORG'), ('Streamlines', 'ORG'), ('#', 'CARDINAL'), ('#', 'CARDINAL'), ('Choose', 'PRODUCT'), ('0.007089085922957821', 'CARDINAL'), ('0.038439192046320805', 'CARDINAL'), ('#', 'CARDINAL'), ('ImportError', 'ORG'), ('pickle.load(fp', 'ORG'), ('# 3d array', 'MONEY'), ('bbox = np.array([[-0.15', 'WORK_OF_ART'), ('0.15', 'CARDINAL'), ('0', 'CARDINAL'), ('0.1', 'CARDINAL'), ('#', 'CARDINAL'), ('Bx_d.shape', 'GPE'), ('length_unit=\"Mpc', 'ORG'), ('bbox=', 'PERSON'), ('bbox', 'GPE'), ('nprocs=100', 'CARDINAL'), ('#', 'CARDINAL'), ('Define', 'MONEY'), ('c1', 'ORG'), ('print(type(c1', 'PRODUCT'), ('#', 'CARDINAL'), ('Create', 'PRODUCT'), ('3D', 'CARDINAL'), ('Streamlines(ds', 'GPE'), ('pos', 'PERSON'), ('3D', 'ORDINAL'), ('s=40', 'PERSON'), (\"print('tisk\", 'PERSON'), ('0.0', 'CARDINAL'), ('#', 'CARDINAL'), ('plt.show', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Enzo', 'LOC'), ('Enzo', 'ORG')]\n",
      "[('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('mylog.debug(str(data_file', 'PRODUCT'), ('2019-05-17', 'DATE'), ('0%|', 'CARDINAL'), ('2019-05-17', 'DATE'), ('14:29:23,684 &lt;yt.frontends.tipsy.data_structures', 'ORG'), ('2019-05-17', 'DATE'), ('2019-05-17', 'DATE'), ('6.474e-03', 'CARDINAL'), ('2019-05-17', 'DATE'), ('2.801e-02', 'CARDINAL'), ('2019-05-17', 'DATE'), ('14:29:23,761 &lt;yt.frontends.tipsy.data_structures', 'ORG'), ('2019-05-17', 'DATE'), ('3.689e-01', 'CARDINAL'), ('2019-05-17', 'DATE'), ('4.364e-01', 'CARDINAL'), ('2019-05-17', 'DATE'), ('9.265e-01', 'CARDINAL'), ('2019-05-17', 'DATE'), ('14:29:23,784 &lt;yt.frontends.tipsy.data_structures', 'ORG')]\n",
      "[('9', 'CARDINAL'), ('10', 'CARDINAL'), ('11', 'CARDINAL'), ('556', 'CARDINAL'), ('@property', 'PRODUCT'), ('557     ', 'QUANTITY'), ('558', 'CARDINAL'), ('559', 'CARDINAL'), ('514', 'CARDINAL'), ('515', 'CARDINAL'), ('516', 'CARDINAL'), ('517', 'CARDINAL'), ('518             #', 'QUANTITY'), ('41', 'CARDINAL'), ('42', 'CARDINAL'), ('43', 'CARDINAL'), ('44', 'CARDINAL'), ('45     def', 'QUANTITY'), ('90', 'CARDINAL'), ('self.kdtree', 'ORG'), ('91', 'CARDINAL'), ('92', 'CARDINAL'), ('93', 'CARDINAL'), ('148', 'CARDINAL'), ('OSError', 'ORG'), ('149', 'CARDINAL'), ('150', 'CARDINAL'), ('151', 'CARDINAL'), ('152', 'CARDINAL'), ('164', 'CARDINAL'), ('enumerate(self.data_files', 'GPE'), ('165', 'FAC'), ('166', 'CARDINAL'), ('pos', 'ORG'), ('self.io._yield_coordinates(data_file', 'GPE'), ('167                 ', 'QUANTITY'), ('168', 'CARDINAL'), ('sph_ptype', 'ORG'), ('needed_ptype', 'ORG'), ('311', 'CARDINAL'), ('312', 'CARDINAL'), ('313', 'CARDINAL'), ('314', 'CARDINAL'), ('315', 'FAC'), ('30 def', 'QUANTITY'), ('keepdims', 'CARDINAL'), ('31', 'CARDINAL'), ('32', 'CARDINAL'), ('33', 'CARDINAL'), ('keepdims', 'CARDINAL'), ('zero', 'CARDINAL')]\n",
      "[('IO', 'GPE')]\n",
      "[('Monday 10AM', 'DATE')]\n",
      "[('Monday', 'DATE'), ('all day', 'DATE'), ('all week', 'DATE'), ('Tuesday', 'DATE')]\n",
      "[('Nathan', 'ORG')]\n",
      "[]\n",
      "[('<@U7HS3717V', 'ORG')]\n",
      "[('SlicePlot', 'ORG'), ('90 degrees', 'QUANTITY')]\n",
      "[('<@U043BNA00', 'ORG'), ('yesterday', 'DATE')]\n",
      "[('Nathan', 'ORG')]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt.off_axis_projection', 'CARDINAL'), ('1', 'CARDINAL'), ('500', 'CARDINAL'), ('500', 'CARDINAL'), ('resolution=[1024', 'PERSON'), ('1024', 'DATE'), ('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('` and `', 'WORK_OF_ART')]\n",
      "[('ParticlePlot', 'ORG')]\n",
      "[]\n",
      "[('SPH data', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ParticlePlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('kinda', 'ORG'), ('CUDA', 'ORG')]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('8', 'CARDINAL'), ('first', 'ORDINAL')]\n",
      "[('second', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('IsolatedGalaxy/galaxy0030/galaxy0030', 'ORG'), ('ProjectionPlot(ds', 'GPE')]\n",
      "[]\n",
      "[('YTQuantity', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('like 4 hours', 'TIME'), ('sci', 'ORG'), ('today', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('more than 1', 'CARDINAL'), ('3.7', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('``def Slices(Folder', 'WORK_OF_ART'), ('#', 'CARDINAL'), ('nbmax', 'FAC'), ('range(nbmin', 'PERSON'), ('time_unit', 'GPE')]\n",
      "[('set_width', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Ramses', 'PRODUCT')]\n",
      "[('<@U37DTBL6N', 'ORG')]\n",
      "[]\n",
      "[('Matt', 'PERSON')]\n",
      "[]\n",
      "[('length_unit', 'WORK_OF_ART'), ('1.0', 'CARDINAL'), ('time_unit', 'WORK_OF_ART'), ('1.0', 'CARDINAL'), ('mass_unit', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('@U37DTBL6N', 'PERSON'), ('Sedov', 'GPE'), ('Joki', 'GPE')]\n",
      "[]\n",
      "[('1cm', 'QUANTITY')]\n",
      "[('RAMSES', 'ORG'), ('first', 'ORDINAL')]\n",
      "[('Ramses/Yt', 'ORG'), ('CGS', 'ORG')]\n",
      "[('```&UNITS_PARAMS\\n', 'ORG'), ('1 Myr in seconds', 'QUANTITY'), ('1 kpc', 'QUANTITY')]\n",
      "[('Dalton', 'PERSON'), ('g/cm3', 'ORG')]\n",
      "[('number_density', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('Matt', 'PERSON'), ('output_XXXXX/info_XXXXX.txt', 'ORG')]\n",
      "[]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[]\n",
      "[('```unit_l', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Ramses', 'PERSON')]\n",
      "[]\n",
      "[('a couple of years ago', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('September 2017', 'DATE')]\n",
      "[]\n",
      "[('One', 'CARDINAL'), ('31556926000000.0 s', 'QUANTITY')]\n",
      "[(\"`<http://ds.current_time.to|ds.current_time.to>('kyr'\", 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('128', 'CARDINAL'), ('64', 'CARDINAL'), ('yt.load', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON'), ('Python', 'GPE'), ('Cython', 'GPE')]\n",
      "[]\n",
      "[('a minute', 'TIME')]\n",
      "[]\n",
      "[('crummy wifi', 'PERSON'), ('https://www.cse.iitb.ac.in/~sharat/icvgip.org/icvgip00/G-51.pdf', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('First', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[('h5py', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('Coordinates', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('all_data', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.index', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('nag', 'ORG')]\n",
      "[('363 MB', 'QUANTITY'), ('13', 'CARDINAL')]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('h5py', 'ORG'), (\"yt.load('gizmo_cosmology_plus/snap_N128L16_131.hdf5'\", 'ORG'), ('Coordinates', 'PERSON'), ('h5py', 'ORG'), (\"File('gizmo_cosmology_plus/snap_N128L16_131.hdf5'\", 'ORG')]\n",
      "[('1.0 seconds', 'TIME'), ('0.02', 'CARDINAL'), ('h5py', 'ORG'), ('1.0 seconds', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[('419 seconds', 'TIME'), ('count_particles_chunks', 'GPE')]\n",
      "[('yt ~16', 'PERSON'), ('seconds', 'TIME'), ('h5py', 'ORG'), ('1 second', 'TIME')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('32', 'CARDINAL')]\n",
      "[('MPI', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('0m40.207s', 'CARDINAL'), ('14m2.574s', 'CARDINAL'), ('```\\n', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('shrug', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('all_data', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<https://github.com/yt-project/website/pull/39', 'PERSON')]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[('PR', 'ORG'), ('tomorrow', 'DATE')]\n",
      "[]\n",
      "[('ray[‘t', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[('Enzo', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('np.array([halo_center[0]-0.001/5, halo_center[1]+0.001/5,', 'DATE'), ('np.array([halo_center[0]-0.001/5', 'CARDINAL'), ('halo_center[1]-0.001/5', 'GPE'), ('code_length', 'ORG')]\n",
      "[('ray[‘t', 'PERSON'), ('input-70-893c526a201d&gt', 'DATE'), ('1', 'CARDINAL'), ('250', 'CARDINAL'), ('251', 'CARDINAL'), ('252', 'CARDINAL'), ('253', 'CARDINAL'), ('254', 'CARDINAL'), ('248', 'CARDINAL'), ('249', 'QUANTITY'), ('250', 'CARDINAL'), ('251', 'CARDINAL'), ('252', 'CARDINAL'), ('269', 'CARDINAL'), ('270', 'CARDINAL'), ('271', 'CARDINAL'), ('272', 'CARDINAL'), ('273', 'CARDINAL'), ('tcoords(self', 'GPE'), ('378', 'CARDINAL'), ('379     def', 'QUANTITY'), ('380', 'CARDINAL'), ('381', 'CARDINAL'), ('382', 'CARDINAL'), ('269', 'CARDINAL'), ('270', 'CARDINAL'), ('271', 'CARDINAL'), ('272', 'CARDINAL'), ('273', 'CARDINAL'), ('391', 'CARDINAL'), ('obj.select_tcoords(self.dobj', 'CARDINAL'), ('392', 'CARDINAL'), ('0', 'CARDINAL'), ('393', 'CARDINAL'), ('394', 'CARDINAL'), ('395', 'CARDINAL'), ('31', 'DATE'), ('29', 'CARDINAL')]\n",
      "[('ray_start', 'ORG')]\n",
      "[('ray_start', 'ORG')]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('ray[‘t', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('629', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('yesterday', 'DATE')]\n",
      "[('one', 'CARDINAL'), ('select_tcoords', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('offhand', 'GPE'), ('first', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('RaySelector.get_dts', 'PERSON'), ('`yt/geometry/', 'ORG')]\n",
      "[]\n",
      "[('CGM', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('26', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('Yong', 'PERSON'), ('Baltimore', 'GPE'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[('1e-4', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('1e-6', 'CARDINAL'), ('1e-4', 'CARDINAL')]\n",
      "[('1e-6', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('cartesian', 'NORP')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('dts', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt.load_octree', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON'), ('first', 'ORDINAL'), ('second', 'ORDINAL')]\n",
      "[('first', 'ORDINAL'), ('second', 'ORDINAL')]\n",
      "[]\n",
      "[('zero', 'CARDINAL')]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('YTOctree', 'ORG'), (\"units='g/cm**3'\", 'ORG'), ('YTOctree', 'ORG'), ('YTArray', 'ORG')]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[('AREPO', 'ORG'), ('Gadget', 'PERSON'), ('<https://yt-project.org/docs/dev/reference/code_support.html', 'ORG')]\n",
      "[]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U37DTBL6N', 'ORG')]\n",
      "[('<@UGPSVM3GW', 'ORG')]\n",
      "[]\n",
      "[('<@UGNAZ0JN7', 'ORG')]\n",
      "[('@UGNMQTDGS', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('frb.bounds', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[('Nathan Golbaum', 'PERSON'), ('np', 'ORG'), ('bbox', 'GPE'), ('yt.load_uniform_grid(data', 'PERSON'), (\"256]*3, length_unit=<http://ds.length_unit.to|ds.length_unit.to>('kpc'\", 'ORG'), ('bbox=', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('`&lt;NameOfYourCode&gt;Dataset.__init', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt-4.0', 'NORP'), ('about 2', 'CARDINAL')]\n",
      "[('h5py', 'ORG')]\n",
      "[('bitmap', 'GPE')]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[('yt.load', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('13', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('second', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('bitmap', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('matt', 'PERSON')]\n",
      "[]\n",
      "[('yesterday', 'DATE')]\n",
      "[]\n",
      "[('@U9CE5D9LZ', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('first', 'ORDINAL')]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SPLASH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('2', 'CARDINAL'), ('3', 'CARDINAL'), ('tomorrow', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[(\"ds.all_data()['PartType0\", 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('13', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('gizmo_64', 'PERSON')]\n",
      "[('one', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('13', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('tomorrow', 'DATE')]\n",
      "[]\n",
      "[('tomorrow', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[(\"i.e `ParticleID's\", 'PERSON')]\n",
      "[]\n",
      "[('AMR', 'ORG'), ('AMRGrid.from_yt', 'ORG'), ('first', 'ORDINAL'), ('``(Pdb', 'WORK_OF_ART'), ('YTRegion', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('center[1]-box_len', 'PERSON'), ('center[1]+box_len', 'PERSON'), ('#', 'CARDINAL'), ('2020-01', 'DATE'), ('Parameters', 'NORP'), ('0.0060000200028298', 'CARDINAL'), ('2020-01', 'DATE'), ('Parameters', 'NORP'), ('32 32 32', 'DATE'), ('2020-01', 'DATE'), ('Parameters', 'NORP'), ('2020-01', 'DATE'), ('Parameters', 'NORP'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('2020-01', 'DATE'), ('Parameters', 'NORP'), ('100%|', 'CARDINAL'), ('2020-01', 'DATE'), ('0.00000000e+00 0.00000000e+00', 'DATE'), ('1.37880524e+16', 'NORP'), ('2020-01-16', 'DATE'), ('2020-01', 'DATE'), ('Parameters', 'NORP'), ('0.0060000200028298', 'CARDINAL'), ('code_time', 'ORG'), ('2020-01', 'DATE'), ('Parameters', 'NORP'), ('2 2 2', 'CARDINAL'), ('2020-01', 'DATE'), ('Parameters', 'NORP'), ('0', 'CARDINAL'), ('2020-01', 'DATE'), ('Parameters', 'NORP'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('2020-01', 'DATE'), ('2020-01', 'DATE'), ('0.000e+00', 'CARDINAL'), ('2020-01', 'DATE'), ('1.000e+00', 'CARDINAL'), ('22', 'CARDINAL'), ('249', 'CARDINAL'), ('1335', 'CARDINAL'), ('determine_fields', 'PERSON'), ('739', 'CARDINAL'), ('temp_enzo.h5', 'GPE')]\n",
      "[]\n",
      "[('save_as_dataset', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('plot.set_buff_size', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('yup', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('``velocity_divergence_absolute```', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PhD', 'WORK_OF_ART')]\n",
      "[('AMR', 'ORG')]\n",
      "[('<https://hg.sr.ht/~ngoldbaum/galaxy_analysis/browse/default/galanyl/galaxy_analyzer.py#L750|https://hg.sr.ht/~ngoldbaum/galaxy_analysis/browse/default/galanyl/galaxy_analyzer.py#L750', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('3D', 'CARDINAL'), ('a very long', 'DATE'), ('1:1:50', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ds.domain_width', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('27', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ValidateSpatial', 'ORG'), ('zero', 'CARDINAL')]\n",
      "[]\n",
      "[('ValidateSpatial', 'ORG')]\n",
      "[('@U37DTBL6N', 'PERSON')]\n",
      "[('10 km/s', 'QUANTITY'), ('ISM', 'ORG'), ('100 km/s', 'QUANTITY'), ('MW', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('sec', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('appendix', 'GPE')]\n",
      "[('only one', 'CARDINAL'), ('@U37DTBL6N', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[('PR', 'ORG')]\n",
      "[('PR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('corentin', 'ORG'), ('PR', 'ORG'), ('oct', 'DATE'), ('AMR', 'ORG'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('corentin', 'ORG'), ('PR', 'ORG'), ('oct', 'DATE'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[('FRB', 'ORG'), ('PhasePlot', 'WORK_OF_ART'), ('ProjectionPlot', 'ORG'), ('SlicePlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('YT', 'ORG')]\n",
      "[]\n",
      "[('pip install', 'PERSON')]\n",
      "[]\n",
      "[('pip', 'ORG')]\n",
      "[('gotcha', 'ORG')]\n",
      "[]\n",
      "[('pip', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('pip install', 'PERSON')]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('file__=\\'\"\\'\"\\'C:\\\\\\\\Users\\\\\\\\Alberto\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\pip', 'PERSON'), ('26', 'CARDINAL'), ('489', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('more than one', 'CARDINAL'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[('3.8', 'CARDINAL'), ('3.7', 'CARDINAL')]\n",
      "[('pip', 'ORG'), ('3.8', 'CARDINAL')]\n",
      "[('3.7', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('14.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('3.7', 'CARDINAL')]\n",
      "[]\n",
      "[('pip', 'ORG')]\n",
      "[]\n",
      "[('Gotcha', 'ORG')]\n",
      "[('a nice weekend', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('4', 'CARDINAL'), ('2019-03-13', 'DATE'), ('2.700e+04', 'CARDINAL'), ('2019-03-13', 'DATE'), ('metadata', 'GPE'), ('2019-03-13', 'DATE'), ('bounding_box=[[-0.99254775', 'CARDINAL'), ('0.99507153', 'CARDINAL'), ('Coordinates', 'WORK_OF_ART'), ('InternalEnergy', 'PERSON'), ('Coordinates', 'PERSON'), ('InternalEnergy', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('3.5.1', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('3.5.1', 'ORG')]\n",
      "[('https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb', 'PERSON')]\n",
      "[('4', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.5.1', 'CARDINAL')]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[('3.x', 'DATE'), ('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[('4.0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[('<@UC6L85LBB', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('1D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('conda', 'ORG'), ('conda', 'ORG'), ('conda', 'ORG'), ('` and/or pip `', 'PRODUCT'), ('pip', 'ORG')]\n",
      "[('bin', 'ORG'), ('bin', 'PERSON'), ('bin', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb', 'PERSON')]\n",
      "[('Slice Plot', 'ORG')]\n",
      "[('ProjectionPlot(ds', 'GPE'), ('sp3.center', 'CARDINAL'), ('width=(80', 'ORG')]\n",
      "[('``>gas3slice = yt.SlicePlot(ds', 'WORK_OF_ART'), ('sp3.center', 'CARDINAL'), ('width=(80', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@UMLNT6WDS', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('PR', 'ORG'), ('UnitRegistry', 'ORG')]\n",
      "[]\n",
      "[('next', 'ORG')]\n",
      "[('mike tremmel', 'PERSON'), ('nchilada', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[]\n",
      "[('the past few weeks', 'DATE'), ('between 2', 'CARDINAL')]\n",
      "[('kpc\\n\\n', 'PERSON'), ('0.019', 'CARDINAL'), ('#', 'CARDINAL'), ('0', 'CARDINAL'), ('0.8', 'CARDINAL'), ('9', 'CARDINAL'), ('0.5', 'CARDINAL'), ('0.8', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('2020-05-15', 'DATE'), ('Parameters', 'NORP'), ('30.0', 'CARDINAL'), ('2020-05-15', 'DATE'), ('Parameters', 'NORP'), ('1001', 'DATE'), ('39', 'CARDINAL'), ('51', 'CARDINAL'), ('2020-05-15', 'DATE'), ('Parameters', 'NORP'), ('2020-05-15', 'DATE'), ('Parameters', 'NORP'), ('20', 'CARDINAL'), ('0.75', 'CARDINAL'), ('# grids        #', 'MONEY'), ('0        ', 'QUANTITY'), ('136', 'CARDINAL'), ('1990989', 'DATE'), ('126', 'CARDINAL'), ('136', 'CARDINAL'), ('3.00000000e+01', 'NORP'), ('3.00000000e+01', 'NORP'), ('6.475e-27', 'CARDINAL'), ('6.475e-21', 'CARDINAL'), ('1.336e-15', 'CARDINAL'), ('1.998e-02 cm', 'QUANTITY')]\n",
      "[('2', 'CARDINAL')]\n",
      "[]\n",
      "[('2020-05-15', 'DATE'), ('Parameters', 'NORP'), ('2020-05-15', 'DATE'), ('Parameters', 'NORP'), ('20', 'CARDINAL'), ('0.75', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('1.3892176', 'MONEY'), ('1.23081842 1.05004701 0.86141738 0.67979444', 'MONEY'), ('0.52100306 0.38255975 0.2683003', 'MONEY')]\n",
      "[('0 ... 1', 'DATE')]\n",
      "[]\n",
      "[('zero', 'CARDINAL')]\n",
      "[('solve_ivp', 'ORG'), ('Runge-Kutta', 'ORG'), ('mpl_toolkits.mplot3d', 'GPE'), ('mpl_toolkits.mplot3d', 'GPE'), ('proj3d', 'PRODUCT'), ('matplotlib.patches import FancyArrowPatch', 'ORG'), ('matplotlib.patches', 'ORG'), ('YTArray', 'ORG'), ('Streamlines', 'ORG'), ('#', 'CARDINAL'), ('matplotlib.pylab', 'CARDINAL'), ('Configuration', 'ORG'), ('ConfigParser', 'ORG'), ('Constants', 'ORG'), ('9.1e-31', 'CARDINAL'), ('# electron mass', 'MONEY'), ('2000', 'CARDINAL'), ('2000', 'CARDINAL'), ('NUM', 'ORG'), ('150', 'CARDINAL'), ('np.zeros(6', 'ORG'), ('#', 'CARDINAL'), ('6 dim', 'MONEY'), ('zeros', 'CARDINAL'), ('3', 'CARDINAL'), ('3', 'CARDINAL'), ('0', 'CARDINAL'), ('0', 'CARDINAL'), ('np.pi', 'CARDINAL'), ('0.25', 'CARDINAL'), ('0.75 *', 'PERCENT'), ('0.5', 'CARDINAL'), ('0', 'CARDINAL'), ('np', 'ORG'), ('Grid', 'ORG'), ('0.000001', 'CARDINAL'), ('1', 'CARDINAL'), ('0', 'CARDINAL'), ('2*np.pi', 'CARDINAL'), ('ZMAX', 'NORP'), ('2', 'CARDINAL'), ('RMAX', 'ORG'), ('NUM', 'ORG'), ('np.linspace(THETA_MIN', 'ORG'), ('NUM', 'ORG'), ('ZMAX', 'ORG'), ('NUM', 'ORG'), ('get_b(grid=', 'PERSON'), ('THETA', 'ORG'), ('print(B_R.shape', 'PERSON'), ('0.025', 'CARDINAL'), ('0', 'CARDINAL'), ('Dictionary', 'ORG'), ('data[\"B1', 'PERSON'), ('# 3d', 'MONEY'), ('0.15', 'CARDINAL'), ('0', 'CARDINAL'), ('0.1', 'CARDINAL'), ('#', 'CARDINAL'), ('yt.load_uniform_grid(data', 'PERSON'), ('B_R.shape', 'PERSON'), ('length_unit=\"Mpc', 'ORG'), ('bbox=', 'PERSON'), ('bbox', 'GPE'), ('nprocs=100', 'CARDINAL'), ('#', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('One', 'CARDINAL'), ('0', 'CARDINAL'), ('x0[0', 'PERSON'), ('168', 'CARDINAL')]\n",
      "[]\n",
      "[('tuesday', 'DATE'), ('10am', 'TIME')]\n",
      "[('10am', 'TIME'), ('tomorrow', 'DATE'), ('the quarter', 'DATE'), ('Later Tuesday (afternoon', 'TIME')]\n",
      "[('Tomorrow', 'DATE')]\n",
      "[('1:15pm', 'CARDINAL')]\n",
      "[]\n",
      "[('ytree', 'DATE')]\n",
      "[('CFD', 'ORG'), ('CFD', 'ORG')]\n",
      "[]\n",
      "[('yt.load_uniform_grid', 'ORG')]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('<http://yt-project.org/doc/examining/loading_data.html#semi-structured-grid-data', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('YT', 'ORG'), ('YT', 'ORG')]\n",
      "[('<@UDN6ACN6N', 'ORG'), ('2D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('screenshots', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('yt.load', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('2D', 'CARDINAL')]\n",
      "[]\n",
      "[('<@U042HLT7U>', 'ORG')]\n",
      "[('100%', 'PERCENT')]\n",
      "[('AMReX', 'ORG')]\n",
      "[]\n",
      "[('first', 'ORDINAL'), ('one', 'CARDINAL'), ('second', 'ORDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('ProfilePlot', 'ORG')]\n",
      "[('2-D', 'CARDINAL'), ('2-D', 'CARDINAL'), ('``NeedsGridType', 'WORK_OF_ART'), ('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('conf.py', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[('doc/source/conf.py', 'ORG')]\n",
      "[]\n",
      "[('NCSA', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('QuadTree', 'PRODUCT')]\n",
      "[('AMR', 'ORG')]\n",
      "[('QuadTree', 'PRODUCT'), ('2D', 'QUANTITY'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('QuadTree', 'PRODUCT')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('a ton', 'QUANTITY'), ('RAM', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[('AMR', 'ORG')]\n",
      "[('less than a million', 'MONEY')]\n",
      "[('400000', 'CARDINAL'), ('one', 'CARDINAL')]\n",
      "[]\n",
      "[('fifth', 'ORDINAL')]\n",
      "[('vx', 'GPE'), ('vy', 'GPE'), ('vx', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('a ton', 'QUANTITY')]\n",
      "[('a ton', 'QUANTITY')]\n",
      "[('261406', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('Nathan', 'ORG')]\n",
      "[]\n",
      "[('zero', 'CARDINAL'), ('265 km/s', 'QUANTITY'), ('s1', 'PRODUCT'), ('unset', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('1', 'CARDINAL'), ('2', 'CARDINAL'), ('1', 'CARDINAL'), ('3', 'CARDINAL'), ('0', 'CARDINAL'), ('4', 'CARDINAL'), ('0', 'CARDINAL'), ('5', 'CARDINAL'), ('6', 'CARDINAL')]\n",
      "[('https://i.imgur.com/LJPOf7O.png', 'PERSON')]\n",
      "[('1', 'DATE'), ('2', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot', 'ORG')]\n",
      "[('FRB', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('frb', 'ORG')]\n",
      "[('12288', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('mpl_kwargs', 'ORG')]\n",
      "[]\n",
      "[('frb', 'ORG')]\n",
      "[('mike', 'PERSON'), ('new york', 'GPE'), ('today', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('NYC', 'ORG')]\n",
      "[]\n",
      "[('Matt', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('<@U04RL26HX', 'ORG'), ('emoji', 'PERSON')]\n",
      "[]\n",
      "[('the end of the day', 'DATE')]\n",
      "[('a few hours', 'TIME')]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('1d', 'CARDINAL'), ('2D', 'QUANTITY')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('gotcha', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('YTArray', 'ORG'), ('YTArray', 'ORG'), ('np.array', 'ORG')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[('array_name.d', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('.to_ndarray', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('@UBU23Q9HV', 'PERSON')]\n",
      "[]\n",
      "[('netcdf4', 'PRODUCT'), ('netcdf4', 'PRODUCT')]\n",
      "[]\n",
      "[('find_clumps', 'WORK_OF_ART')]\n",
      "[]\n",
      "[('one', 'CARDINAL')]\n",
      "[('AMR', 'ORG'), ('Enzo', 'PERSON'), ('AMR', 'ORG')]\n",
      "[('load_uniform_grid', 'PERSON'), ('AMR', 'ORG')]\n",
      "[]\n",
      "[('britton', 'ORG')]\n",
      "[('nathan', 'ORG')]\n",
      "[('save_as_dataset', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('AMR', 'ORG')]\n",
      "[('<@U37DTBL6N', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('PdfPages', 'ORG'), ('ProjectionPlot', 'ORG')]\n",
      "[('first', 'ORDINAL'), ('PdfPages', 'ORG')]\n",
      "[('kinda', 'ORG'), ('inheritance', 'GPE')]\n",
      "[]\n",
      "[(\"plot.plots['density'].figure\", 'CARDINAL')]\n",
      "[('@U91855PA9', 'PERSON')]\n",
      "[('pdf.savefig', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('yt.funcs.matplotlib_style_context', 'WORK_OF_ART')]\n",
      "[('matplotlib_style_context', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('PdfPages', 'ORG'), ('PlotCollection', 'ORG')]\n",
      "[('an hour', 'TIME'), ('ProjectionPlot', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[('ProjectionPlot', 'ORG')]\n",
      "[('yt-4.0', 'NORP'), ('@U91855PA9', 'PERSON')]\n",
      "[('https://nbviewer.jupyter.org/url/trident-project.org/notebooks/trident_demesh_install.ipynb', 'PERSON')]\n",
      "[('YTEP-0032', 'ORG'), ('2017', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[('1 hour', 'TIME'), ('4 minutes', 'TIME')]\n",
      "[('@U91855PA9', 'PERSON'), ('4.0', 'CARDINAL')]\n",
      "[('yt-4.0', 'NORP')]\n",
      "[('YTEP', 'ORG')]\n",
      "[('O(Ncpus', 'PRODUCT')]\n",
      "[('Editing rockstar.py', 'PERSON')]\n",
      "[]\n",
      "[('two', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('<@UF14FNZTQ', 'ORG')]\n",
      "[('64', 'CARDINAL'), ('3', 'CARDINAL'), ('one', 'CARDINAL'), ('AMR', 'ORG'), ('ds.covering_grid(level=1', 'PRODUCT')]\n",
      "[('<@UC6L85LBB', 'ORG')]\n",
      "[('0', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[('two', 'CARDINAL'), ('RAM', 'ORG')]\n",
      "[('<@UC6L85LBB', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.6', 'CARDINAL'), ('yt.units.physical_constants', 'GPE'), ('``import yt\\nh_planck = yt.units.physical_constants.planck_constant_cgs\\n', 'WORK_OF_ART')]\n",
      "[('2', 'CARDINAL')]\n",
      "[]\n",
      "[('2', 'CARDINAL'), ('1', 'CARDINAL'), ('2', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('last week', 'DATE')]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('one', 'CARDINAL'), ('a few years ago', 'DATE')]\n",
      "[('Nathan', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('this about 3.5 years ago', 'DATE')]\n",
      "[('matt', 'PERSON')]\n",
      "[]\n",
      "[('aaaaah', 'ORG'), ('Nathan', 'GPE')]\n",
      "[('this a few weeks back', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('6 months ago', 'DATE')]\n",
      "[]\n",
      "[]\n",
      "[('2015', 'DATE'), ('Universe', 'ORG')]\n",
      "[('One', 'CARDINAL'), ('Cameron', 'PERSON')]\n",
      "[]\n",
      "[('many weeks', 'DATE'), ('0.6', 'CARDINAL'), ('2', 'CARDINAL'), ('0.26', 'CARDINAL'), ('3', 'CARDINAL'), ('Illustris', 'GPE'), ('TNG', 'ORG')]\n",
      "[('Robin', 'PERSON')]\n",
      "[('Eq', 'PERSON'), ('3.98', 'CARDINAL'), ('32nd', 'ORDINAL')]\n",
      "[('Eq', 'PERSON'), ('9', 'CARDINAL'), ('SPLASH', 'ORG')]\n",
      "[('a few percent', 'PERCENT')]\n",
      "[('SPH', 'ORG')]\n",
      "[('SPH', 'ORG'), ('zero', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('CIC', 'ORG')]\n",
      "[('128j', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('DM', 'ORG')]\n",
      "[('BTW', 'ORG'), ('KeyError', 'ORG'), (\"grid['deposit\", 'PERSON'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('Gotcha', 'ORG')]\n",
      "[('SPH', 'ORG'), ('SPH', 'ORG')]\n",
      "[('``KeyError:', 'WORK_OF_ART'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[(\"grid['deposit\", 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('one', 'CARDINAL'), ('DM', 'ORG'), ('SPH', 'ORG')]\n",
      "[(\"grid['PartType1\", 'ORG')]\n",
      "[('first', 'ORDINAL'), ('Reload DM', 'ORG'), ('#', 'CARDINAL'), ('SPH', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[('two', 'CARDINAL'), ('two', 'CARDINAL')]\n",
      "[('Fixed Resolution Buffers', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('SlicePlot', 'ORG'), ('ProjectionPlot', 'ORG')]\n",
      "[('@ULLD7B8GK', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('August', 'DATE')]\n",
      "[]\n",
      "[('UK', 'GPE')]\n",
      "[('One', 'CARDINAL')]\n",
      "[('<@UFGGCP9R6', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('@UA068E10D', 'PERSON'), ('non-zero', 'QUANTITY')]\n",
      "[]\n",
      "[]\n",
      "[('0', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('photon', 'PERSON')]\n",
      "[('http://hea-www.cfa.harvard.edu/~jzuhone/pyxsim/', 'PERSON')]\n",
      "[]\n",
      "[]\n",
      "[('yt4.x', 'GPE'), ('4.x', 'GPE'), ('two', 'CARDINAL')]\n",
      "[('np\\n\\n', 'PERSON'), ('``12.784961670719257 dimensionless', 'WORK_OF_ART')]\n",
      "[('matter_mass', 'ORG'), ('yt3', 'GPE'), ('grid', 'GPE')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('<@U042HLT7U', 'ORG')]\n",
      "[('1.0', 'CARDINAL')]\n",
      "[]\n",
      "[('3D', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[('1', 'CARDINAL'), ('1', 'CARDINAL')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('AMReX', 'ORG'), ('Cantera', 'ORG'), ('TPX', 'ORG'), ('Cantera', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[('3.6.0', 'DATE'), ('4', 'CARDINAL'), ('#', 'CARDINAL'), ('left_edge=ds.domain_left_edge', 'WORK_OF_ART')]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# trying the pipeline feature\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for doc in nlp.pipe(user_text, disable=[\"tagger\", \"parser\"]):\n",
    "    # Do something with the doc here\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi ! I’m pretty sure it’s possible to define units equivalences from the user-end, but I can’t find back how it’s done in the cookbook... little help ?\n"
     ]
    }
   ],
   "source": [
    "print(user_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi INTJ UH ROOT xx True False\n",
      "! PUNCT . punct ! False False\n",
      "-PRON- PRON PRP nsubj X True True\n",
      "be VERB VBP ROOT ’x False True\n",
      "pretty ADV RB advmod xxxx True False\n",
      "sure ADJ JJ acomp xxxx True False\n",
      "-PRON- PRON PRP nsubj xx True True\n",
      "’ VERB VBZ ROOT ’x False True\n",
      "possible ADJ JJ acomp xxxx True False\n",
      "to PART TO aux xx True True\n",
      "define VERB VB xcomp xxxx True False\n",
      "unit NOUN NNS compound xxxx True False\n",
      "equivalence NOUN NNS dobj xxxx True False\n",
      "from ADP IN prep xxxx True True\n",
      "the DET DT det xxx True True\n",
      "user NOUN NN compound xxxx True False\n",
      "- PUNCT HYPH punct - False False\n",
      "end NOUN NN pobj xxx True False\n",
      ", PUNCT , punct , False False\n",
      "but CCONJ CC cc xxx True True\n",
      "-PRON- PRON PRP nsubj X True True\n",
      "can VERB MD aux xx True True\n",
      "not PART RB neg x’x False True\n",
      "find VERB VB ROOT xxxx True False\n",
      "back ADV RB advmod xxxx True True\n",
      "how ADV WRB advmod xxx True True\n",
      "-PRON- PRON PRP nsubj xx True True\n",
      "’ VERB VBZ aux ’x False True\n",
      "do VERB VBN ccomp xxxx True True\n",
      "in ADP IN prep xx True True\n",
      "the DET DT det xxx True True\n",
      "cookbook NOUN NN pobj xxxx True False\n",
      "... PUNCT : punct ... False False\n",
      "little ADJ JJ amod xxxx True False\n",
      "help NOUN NN ROOT xxxx True False\n",
      "? PUNCT . punct ? False False\n"
     ]
    }
   ],
   "source": [
    "# testing out tokenization\n",
    "\n",
    "doc = nlp(user_text[0])\n",
    "# print(doc)\n",
    "for token in doc:\n",
    "    print(token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: the Rockstar Halo Finder 33 57 Label: FAC\n",
      "Text: the Rockstar User's Guide 137 162 Label: ORG\n"
     ]
    }
   ],
   "source": [
    "# trying out named entities\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(user_text[10])\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(\"Text:\", ent.text, ent.start_char, ent.end_char, \"Label:\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi True 20.08216 True\n",
      "everyone True 20.838543 True\n",
      ", True 18.883696 True\n",
      "\n",
      " True 17.503597 True\n",
      "I True 21.45263 True\n",
      "'m True 21.350115 True\n",
      "using True 22.74428 True\n",
      "yt True 17.032454 True\n",
      "to True 19.731726 True\n",
      "run True 22.315348 True\n",
      "the True 17.793186 True\n",
      "Rockstar True 19.082636 True\n",
      "Halo True 18.895746 True\n",
      "Finder True 17.538334 True\n",
      ". True 16.740887 True\n",
      "Is True 20.67176 True\n",
      "it True 22.07135 True\n",
      "somehow True 21.025724 True\n",
      "possible True 20.382547 True\n",
      "to True 21.93313 True\n",
      "output True 20.802433 True\n",
      "the True 20.223085 True\n",
      "particle True 17.281292 True\n",
      "data True 17.56046 True\n",
      "of True 20.707445 True\n",
      "each True 21.678823 True\n",
      "halo True 16.526228 True\n",
      "? True 20.766848 True\n",
      "According True 21.904463 True\n",
      "to True 19.064468 True\n",
      "the True 19.106318 True\n",
      "Rockstar True 19.462444 True\n",
      "User True 20.695772 True\n",
      "'s True 17.814976 True\n",
      "Guide True 20.398699 True\n",
      "this True 20.47331 True\n",
      "is True 20.69268 True\n",
      "doable True 20.785278 True\n",
      "but True 22.564295 True\n",
      "I True 21.317434 True\n",
      "could True 24.556904 True\n",
      "n't True 25.348547 True\n",
      "figure True 23.888636 True\n",
      "out True 20.540094 True\n",
      "how True 20.073406 True\n",
      "in True 18.64618 True\n",
      "yt True 17.52832 True\n",
      ". True 17.508883 True\n",
      "Thanks True 19.0958 True\n",
      "in True 17.01043 True\n",
      "advance True 18.481947 True\n",
      "for True 20.263725 True\n",
      "your True 20.436335 True\n",
      "support True 18.838299 True\n",
      "! True 19.945827 True\n"
     ]
    }
   ],
   "source": [
    "nlp_md = spacy.load(\"en_core_web_md\")\n",
    "tokens = nlp(user_text[10])\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Hi 1.0\n",
      "Hi everyone 0.17960533\n",
      "Hi , -0.20161864\n",
      "Hi \n",
      " 0.20259415\n",
      "Hi I 0.23036276\n",
      "Hi 'm 0.09194209\n",
      "Hi using 0.06512105\n",
      "Hi yt 0.20456247\n",
      "Hi to -0.058259893\n",
      "Hi run 0.086694784\n",
      "Hi the -0.010082898\n",
      "Hi Rockstar -0.14298458\n",
      "Hi Halo 0.0027026562\n",
      "Hi Finder 0.04480707\n",
      "Hi . -0.19044921\n",
      "Hi Is 0.2693308\n",
      "Hi it 0.06745813\n",
      "Hi somehow 0.25642216\n",
      "Hi possible 0.025402823\n",
      "Hi to -0.08799042\n",
      "Hi output -0.007910854\n",
      "Hi the -0.037475307\n",
      "Hi particle -0.08878847\n",
      "Hi data 0.015099464\n",
      "Hi of 0.13451265\n",
      "Hi each 0.006411853\n",
      "Hi halo -0.013925342\n",
      "Hi ? -0.20968693\n",
      "Hi According 0.07759787\n",
      "Hi to 0.043419085\n",
      "Hi the -0.05278793\n",
      "Hi Rockstar -0.12526739\n",
      "Hi User 0.027806975\n",
      "Hi 's -0.11499486\n",
      "Hi Guide 0.13340518\n",
      "Hi this -0.06019851\n",
      "Hi is 0.22837877\n",
      "Hi doable 0.12119339\n",
      "Hi but 0.24568978\n",
      "Hi I 0.15807469\n",
      "Hi could 0.31173414\n",
      "Hi n't 0.2279573\n",
      "Hi figure 0.08111673\n",
      "Hi out -0.04382779\n",
      "Hi how 0.07235979\n",
      "Hi in 0.19859283\n",
      "Hi yt 0.08762924\n",
      "Hi . -0.1341996\n",
      "Hi Thanks 0.26144847\n",
      "Hi in 0.18180202\n",
      "Hi advance 0.06684664\n",
      "Hi for 0.08780641\n",
      "Hi your -0.030821275\n",
      "Hi support -0.20409453\n",
      "Hi ! -0.25907907\n",
      "everyone Hi 0.17960533\n",
      "everyone everyone 1.0\n",
      "everyone , 0.06479692\n",
      "everyone \n",
      " -0.11939057\n",
      "everyone I 0.19447476\n",
      "everyone 'm 0.064144306\n",
      "everyone using 0.036360007\n",
      "everyone yt 0.057404887\n",
      "everyone to 0.09382612\n",
      "everyone run 0.1634917\n",
      "everyone the 0.2239627\n",
      "everyone Rockstar 0.041785188\n",
      "everyone Halo 0.039163712\n",
      "everyone Finder 0.12257428\n",
      "everyone . -0.09285981\n",
      "everyone Is -0.09353458\n",
      "everyone it 0.32168856\n",
      "everyone somehow -0.06272408\n",
      "everyone possible 0.03270703\n",
      "everyone to 0.002348697\n",
      "everyone output 0.25352177\n",
      "everyone the 0.20397924\n",
      "everyone particle 0.3265038\n",
      "everyone data 0.085335046\n",
      "everyone of 0.07311386\n",
      "everyone each 0.14303492\n",
      "everyone halo 0.3239822\n",
      "everyone ? 0.14608173\n",
      "everyone According 0.011680606\n",
      "everyone to 0.07447013\n",
      "everyone the 0.19948292\n",
      "everyone Rockstar 0.014099446\n",
      "everyone User 0.25891036\n",
      "everyone 's 0.10066008\n",
      "everyone Guide 0.21810208\n",
      "everyone this 0.21346182\n",
      "everyone is 0.0010299301\n",
      "everyone doable 0.15151332\n",
      "everyone but -0.09335777\n",
      "everyone I 0.18859659\n",
      "everyone could 0.1972463\n",
      "everyone n't 0.24737084\n",
      "everyone figure 0.34122095\n",
      "everyone out 0.15227097\n",
      "everyone how -0.039744757\n",
      "everyone in 0.10796753\n",
      "everyone yt 0.077059984\n",
      "everyone . -0.085296266\n",
      "everyone Thanks -0.08648325\n",
      "everyone in 0.13392821\n",
      "everyone advance 0.4335847\n",
      "everyone for 0.057326086\n",
      "everyone your 0.014232022\n",
      "everyone support 0.3083148\n",
      "everyone ! -0.074575186\n",
      ", Hi -0.20161864\n",
      ", everyone 0.06479692\n",
      ", , 1.0\n",
      ", \n",
      " 0.30502662\n",
      ", I -0.23680867\n",
      ", 'm -0.018517755\n",
      ", using 0.083722524\n",
      ", yt -0.06856421\n",
      ", to 0.0020481243\n",
      ", run 0.040636983\n",
      ", the -0.14961617\n",
      ", Rockstar -0.07866471\n",
      ", Halo -0.054049343\n",
      ", Finder -0.015286101\n",
      ", . 0.3402563\n",
      ", Is -0.09484016\n",
      ", it -0.13336563\n",
      ", somehow -0.0950506\n",
      ", possible -0.018160397\n",
      ", to 0.033961166\n",
      ", output 0.113222964\n",
      ", the -0.06947174\n",
      ", particle 0.11529388\n",
      ", data -0.046089042\n",
      ", of 0.028167095\n",
      ", each -0.057537\n",
      ", halo -0.014573961\n",
      ", ? 0.4724007\n",
      ", According 0.03919615\n",
      ", to -0.07740964\n",
      ", the -0.13619632\n",
      ", Rockstar -0.059447847\n",
      ", User -0.057446495\n",
      ", 's 0.026969321\n",
      ", Guide 0.043101933\n",
      ", this -0.07401814\n",
      ", is -0.056215506\n",
      ", doable 0.029413832\n",
      ", but 0.025278673\n",
      ", I -0.14130746\n",
      ", could -0.01897995\n",
      ", n't -0.06283637\n",
      ", figure 0.15372881\n",
      ", out 0.07923468\n",
      ", how 0.08333305\n",
      ", in -0.09484523\n",
      ", yt -0.101757556\n",
      ", . 0.38273433\n",
      ", Thanks -0.075700134\n",
      ", in 0.03116122\n",
      ", advance 0.044000342\n",
      ", for 0.18593101\n",
      ", your -0.20122357\n",
      ", support 0.10340355\n",
      ", ! 0.4811996\n",
      "\n",
      " Hi 0.20259415\n",
      "\n",
      " everyone -0.11939057\n",
      "\n",
      " , 0.30502662\n",
      "\n",
      " \n",
      " 1.0\n",
      "\n",
      " I 0.062486418\n",
      "\n",
      " 'm -0.013834252\n",
      "\n",
      " using -0.14683361\n",
      "\n",
      " yt 0.025459073\n",
      "\n",
      " to -0.11342091\n",
      "\n",
      " run -0.0811053\n",
      "\n",
      " the -0.051680554\n",
      "\n",
      " Rockstar -0.22727998\n",
      "\n",
      " Halo -0.17842487\n",
      "\n",
      " Finder 0.06963097\n",
      "\n",
      " . 0.47507364\n",
      "\n",
      " Is 0.033185042\n",
      "\n",
      " it -0.035047855\n",
      "\n",
      " somehow -0.13940838\n",
      "\n",
      " possible -0.18837376\n",
      "\n",
      " to -0.0468279\n",
      "\n",
      " output -0.10554969\n",
      "\n",
      " the -0.0776435\n",
      "\n",
      " particle 0.017756859\n",
      "\n",
      " data -0.08929576\n",
      "\n",
      " of -0.05795885\n",
      "\n",
      " each 0.03370298\n",
      "\n",
      " halo -0.06816956\n",
      "\n",
      " ? 0.33923057\n",
      "\n",
      " According -0.12629364\n",
      "\n",
      " to -0.093866594\n",
      "\n",
      " the -0.011273731\n",
      "\n",
      " Rockstar -0.1391312\n",
      "\n",
      " User -0.045758598\n",
      "\n",
      " 's -0.26694527\n",
      "\n",
      " Guide -0.004773321\n",
      "\n",
      " this 0.0042792885\n",
      "\n",
      " is 0.02414896\n",
      "\n",
      " doable 0.02820831\n",
      "\n",
      " but 0.3323853\n",
      "\n",
      " I -0.01225748\n",
      "\n",
      " could 0.03915839\n",
      "\n",
      " n't -0.12895338\n",
      "\n",
      " figure 0.030054122\n",
      "\n",
      " out -0.04295294\n",
      "\n",
      " how 0.10373024\n",
      "\n",
      " in -0.067018956\n",
      "\n",
      " yt 0.013197038\n",
      "\n",
      " . 0.44662184\n",
      "\n",
      " Thanks 0.0853085\n",
      "\n",
      " in 0.023947151\n",
      "\n",
      " advance -0.069637604\n",
      "\n",
      " for 0.08991105\n",
      "\n",
      " your -0.13890532\n",
      "\n",
      " support -0.09543171\n",
      "\n",
      " ! 0.31698865\n",
      "I Hi 0.23036276\n",
      "I everyone 0.19447476\n",
      "I , -0.23680867\n",
      "I \n",
      " 0.062486418\n",
      "I I 1.0\n",
      "I 'm 0.0045180838\n",
      "I using 0.039400287\n",
      "I yt 0.2597359\n",
      "I to 0.024787452\n",
      "I run 0.06662126\n",
      "I the 0.028149603\n",
      "I Rockstar 0.0037242859\n",
      "I Halo -0.015319316\n",
      "I Finder 0.06870633\n",
      "I . -0.12890658\n",
      "I Is 0.08598054\n",
      "I it 0.4220554\n",
      "I somehow 0.0009206402\n",
      "I possible -0.08860382\n",
      "I to -0.11028195\n",
      "I output 0.03134295\n",
      "I the 0.11392936\n",
      "I particle 0.08754201\n",
      "I data 0.021290293\n",
      "I of 0.041479275\n",
      "I each 0.13482146\n",
      "I halo 0.06118081\n",
      "I ? -0.17093109\n",
      "I According -0.12819633\n",
      "I to -0.0030580128\n",
      "I the 0.02237848\n",
      "I Rockstar -0.07608498\n",
      "I User 0.15186216\n",
      "I 's 0.06450714\n",
      "I Guide 0.106838115\n",
      "I this 0.22198439\n",
      "I is -0.03612251\n",
      "I doable 0.1537336\n",
      "I but 0.13575214\n",
      "I I 1.0\n",
      "I could 0.014841805\n",
      "I n't 0.10454577\n",
      "I figure 0.02072231\n",
      "I out -0.09812246\n",
      "I how -0.018570364\n",
      "I in 0.13580233\n",
      "I yt 0.19352359\n",
      "I . -0.063700095\n",
      "I Thanks 0.08044374\n",
      "I in -0.048901986\n",
      "I advance 0.18924372\n",
      "I for -0.09198397\n",
      "I your 0.1405704\n",
      "I support 0.09128368\n",
      "I ! -0.066929094\n",
      "'m Hi 0.09194209\n",
      "'m everyone 0.064144306\n",
      "'m , -0.018517755\n",
      "'m \n",
      " -0.013834252\n",
      "'m I 0.0045180838\n",
      "'m 'm 1.0\n",
      "'m using 0.023302875\n",
      "'m yt 0.04347297\n",
      "'m to 0.10214702\n",
      "'m run 0.19282658\n",
      "'m the 3.0089877e-05\n",
      "'m Rockstar -0.1506687\n",
      "'m Halo -0.282137\n",
      "'m Finder -0.2026822\n",
      "'m . -0.095943764\n",
      "'m Is 0.33563945\n",
      "'m it 0.14552249\n",
      "'m somehow 0.11464592\n",
      "'m possible -0.0075994763\n",
      "'m to 0.01586986\n",
      "'m output 0.13482359\n",
      "'m the -0.020938205\n",
      "'m particle -0.029605273\n",
      "'m data -0.08533268\n",
      "'m of -0.1734136\n",
      "'m each 0.1681117\n",
      "'m halo -0.22172675\n",
      "'m ? -0.025267573\n",
      "'m According -0.0165855\n",
      "'m to -0.03759129\n",
      "'m the -0.010372122\n",
      "'m Rockstar -0.22432244\n",
      "'m User -0.15941754\n",
      "'m 's 0.08304575\n",
      "'m Guide -0.18615584\n",
      "'m this 0.04180482\n",
      "'m is 0.27715588\n",
      "'m doable 0.08493891\n",
      "'m but 0.12491707\n",
      "'m I 0.08097468\n",
      "'m could 0.49244896\n",
      "'m n't 0.12802567\n",
      "'m figure 0.27017048\n",
      "'m out -0.033314813\n",
      "'m how -0.119237214\n",
      "'m in -0.08384223\n",
      "'m yt -0.014558908\n",
      "'m . -0.055872824\n",
      "'m Thanks -0.02910461\n",
      "'m in 0.09071505\n",
      "'m advance 0.06427351\n",
      "'m for 0.048743\n",
      "'m your -0.03861253\n",
      "'m support 0.05293569\n",
      "'m ! -0.010697375\n",
      "using Hi 0.06512105\n",
      "using everyone 0.036360007\n",
      "using , 0.083722524\n",
      "using \n",
      " -0.14683361\n",
      "using I 0.039400287\n",
      "using 'm 0.023302875\n",
      "using using 1.0\n",
      "using yt -0.041940264\n",
      "using to 0.096999764\n",
      "using run 0.075970605\n",
      "using the 0.08161917\n",
      "using Rockstar 0.0071952417\n",
      "using Halo -0.065457895\n",
      "using Finder -0.17298622\n",
      "using . -0.027881393\n",
      "using Is 0.0453565\n",
      "using it 0.1762023\n",
      "using somehow 0.31351247\n",
      "using possible 0.2741956\n",
      "using to -0.03661327\n",
      "using output 0.19518432\n",
      "using the 0.09536544\n",
      "using particle 0.12391549\n",
      "using data -0.117895484\n",
      "using of 0.13807622\n",
      "using each -0.0155991465\n",
      "using halo -0.04774718\n",
      "using ? 0.019607946\n",
      "using According 0.645737\n",
      "using to 0.10226652\n",
      "using the 0.07374004\n",
      "using Rockstar 0.041675135\n",
      "using User -0.012871384\n",
      "using 's 0.18111208\n",
      "using Guide 0.015719067\n",
      "using this -0.012093432\n",
      "using is -0.0031973007\n",
      "using doable 0.19692673\n",
      "using but -0.013347847\n",
      "using I 0.050075628\n",
      "using could -0.052855514\n",
      "using n't 0.0137119545\n",
      "using figure 0.114905864\n",
      "using out -0.04352716\n",
      "using how -0.03554337\n",
      "using in 0.12536435\n",
      "using yt -0.054004114\n",
      "using . -0.06931585\n",
      "using Thanks 0.03269754\n",
      "using in 0.08277201\n",
      "using advance 0.30079395\n",
      "using for 0.17146626\n",
      "using your 0.16144356\n",
      "using support 0.0364688\n",
      "using ! 0.038336366\n",
      "yt Hi 0.20456247\n",
      "yt everyone 0.057404887\n",
      "yt , -0.06856421\n",
      "yt \n",
      " 0.025459073\n",
      "yt I 0.2597359\n",
      "yt 'm 0.04347297\n",
      "yt using -0.041940264\n",
      "yt yt 1.0\n",
      "yt to 0.1323532\n",
      "yt run -0.08118388\n",
      "yt the -0.018436246\n",
      "yt Rockstar 0.08237881\n",
      "yt Halo 0.011909549\n",
      "yt Finder 0.15972428\n",
      "yt . -0.0277829\n",
      "yt Is 0.3053759\n",
      "yt it 0.2975001\n",
      "yt somehow 0.022701593\n",
      "yt possible 0.03148418\n",
      "yt to 0.13358907\n",
      "yt output -0.10802229\n",
      "yt the 0.095337294\n",
      "yt particle 0.022884829\n",
      "yt data 0.11492802\n",
      "yt of 0.04761757\n",
      "yt each 0.07889548\n",
      "yt halo 0.06632083\n",
      "yt ? -0.15787925\n",
      "yt According -0.037718948\n",
      "yt to 0.2678223\n",
      "yt the -0.015854087\n",
      "yt Rockstar 0.04929925\n",
      "yt User 0.046476044\n",
      "yt 's 0.07281221\n",
      "yt Guide 0.024250224\n",
      "yt this -0.011204084\n",
      "yt is 0.13508606\n",
      "yt doable -0.05805588\n",
      "yt but 0.094009854\n",
      "yt I 0.2509748\n",
      "yt could -0.15401481\n",
      "yt n't 0.04038918\n",
      "yt figure -0.049000084\n",
      "yt out 0.014597924\n",
      "yt how 0.14050995\n",
      "yt in 0.21289557\n",
      "yt yt 1.0\n",
      "yt . -0.011428421\n",
      "yt Thanks 0.29598102\n",
      "yt in 0.14802985\n",
      "yt advance 0.07154192\n",
      "yt for 0.091223866\n",
      "yt your 0.20610662\n",
      "yt support -0.0071781618\n",
      "yt ! -0.19187434\n",
      "to Hi -0.058259893\n",
      "to everyone 0.09382612\n",
      "to , 0.0020481243\n",
      "to \n",
      " -0.11342091\n",
      "to I 0.024787452\n",
      "to 'm 0.10214702\n",
      "to using 0.096999764\n",
      "to yt 0.1323532\n",
      "to to 1.0\n",
      "to run -0.08822339\n",
      "to the -0.07170183\n",
      "to Rockstar -0.13862251\n",
      "to Halo -0.12737788\n",
      "to Finder -0.17786765\n",
      "to . -0.1708586\n",
      "to Is -0.04185065\n",
      "to it 0.07726714\n",
      "to somehow 0.09439148\n",
      "to possible -0.04187636\n",
      "to to 1.0\n",
      "to output -0.041214302\n",
      "to the 0.027234748\n",
      "to particle 0.06274484\n",
      "to data -0.04640098\n",
      "to of 0.13128139\n",
      "to each -0.051443957\n",
      "to halo -0.04011447\n",
      "to ? -0.08407705\n",
      "to According 0.15887827\n",
      "to to 1.0\n",
      "to the 0.036872555\n",
      "to Rockstar -0.21000984\n",
      "to User -0.086429246\n",
      "to 's -0.043949537\n",
      "to Guide -0.14244793\n",
      "to this -0.061340466\n",
      "to is 0.0894818\n",
      "to doable -0.07820919\n",
      "to but 0.040619835\n",
      "to I -0.037966672\n",
      "to could 0.24823198\n",
      "to n't 0.31313977\n",
      "to figure 0.13570216\n",
      "to out 0.023397285\n",
      "to"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swalkow2/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " how 0.010346979\n",
      "to in 0.019183608\n",
      "to yt 0.16104509\n",
      "to . -0.031584878\n",
      "to Thanks -0.024553124\n",
      "to in 0.068339944\n",
      "to advance -0.031013971\n",
      "to for 0.17393795\n",
      "to your 0.002571621\n",
      "to support 0.062396117\n",
      "to ! -0.055484314\n",
      "run Hi 0.086694784\n",
      "run everyone 0.1634917\n",
      "run , 0.040636983\n",
      "run \n",
      " -0.0811053\n",
      "run I 0.06662126\n",
      "run 'm 0.19282658\n",
      "run using 0.075970605\n",
      "run yt -0.08118388\n",
      "run to -0.08822339\n",
      "run run 1.0\n",
      "run the -0.022934174\n",
      "run Rockstar 0.21148457\n",
      "run Halo 0.0021444496\n",
      "run Finder 0.10963173\n",
      "run . 0.12645505\n",
      "run Is -0.05003866\n",
      "run it -0.07439761\n",
      "run somehow 0.05490173\n",
      "run possible -0.07286238\n",
      "run to -0.1651402\n",
      "run output 0.6964679\n",
      "run the -0.0318532\n",
      "run particle 0.1591395\n",
      "run data -0.0038735848\n",
      "run of 0.026041819\n",
      "run each -0.058466543\n",
      "run halo -0.011405501\n",
      "run ? 0.0861451\n",
      "run According 0.008251625\n",
      "run to 0.13605937\n",
      "run the 0.006804403\n",
      "run Rockstar 0.11733546\n",
      "run User 0.16013128\n",
      "run 's 0.12732849\n",
      "run Guide 0.3220655\n",
      "run this -0.006399283\n",
      "run is -0.009016437\n",
      "run doable 0.2059651\n",
      "run but -0.014491892\n",
      "run I 0.033109326\n",
      "run could 0.13086513\n",
      "run n't 0.07613054\n",
      "run figure 0.5939319\n",
      "run out 0.12865019\n",
      "run how -0.05930145\n",
      "run in -0.07932019\n",
      "run yt -0.06519329\n",
      "run . 0.18799222\n",
      "run Thanks -0.08284697\n",
      "run in -0.053743813\n",
      "run advance 0.028526168\n",
      "run for 0.033065133\n",
      "run your -0.030290782\n",
      "run support 0.1721825\n",
      "run ! 0.12950997\n",
      "the Hi -0.010082898\n",
      "the everyone 0.2239627\n",
      "the , -0.14961617\n",
      "the \n",
      " -0.051680554\n",
      "the I 0.028149603\n",
      "the 'm 3.0089877e-05\n",
      "the using 0.08161917\n",
      "the yt -0.018436246\n",
      "the to -0.07170183\n",
      "the run -0.022934174\n",
      "the the 1.0\n",
      "the Rockstar 0.25644705\n",
      "the Halo 0.16795938\n",
      "the Finder 0.12260781\n",
      "the . 0.050424427\n",
      "the Is -0.13053907\n",
      "the it 0.1118638\n",
      "the somehow -0.013010614\n",
      "the possible -0.034634512\n",
      "the to -0.039305814\n",
      "the output 0.059917685\n",
      "the the 1.0\n",
      "the particle 0.15345916\n",
      "the data -0.016274706\n",
      "the of 0.04522984\n",
      "the each 0.4863022\n",
      "the halo 0.10922449\n",
      "the ? -0.1051755\n",
      "the According 0.021663137\n",
      "the to 0.07650483\n",
      "the the 1.0\n",
      "the Rockstar 0.26276764\n",
      "the User 0.026483657\n",
      "the 's 0.15365063\n",
      "the Guide 0.023708427\n",
      "the this 0.43557504\n",
      "the is -0.113762304\n",
      "the doable 0.15521285\n",
      "the but 0.016530478\n",
      "the I -0.064828426\n",
      "the could -0.1332097\n",
      "the n't -0.040108774\n",
      "the figure 0.13926038\n",
      "the out 0.19933185\n",
      "the how -0.15515706\n",
      "the in 0.060739093\n",
      "the yt 0.03404641\n",
      "the . -0.084831856\n",
      "the Thanks -0.015187249\n",
      "the in 0.041372847\n",
      "the advance 0.2162737\n",
      "the for 0.012480866\n",
      "the your 0.12355241\n",
      "the support 0.22322023\n",
      "the ! -0.05786373\n",
      "Rockstar Hi -0.14298458\n",
      "Rockstar everyone 0.041785188\n",
      "Rockstar , -0.07866471\n",
      "Rockstar \n",
      " -0.22727998\n",
      "Rockstar I 0.0037242859\n",
      "Rockstar 'm -0.1506687\n",
      "Rockstar using 0.0071952417\n",
      "Rockstar yt 0.08237881\n",
      "Rockstar to -0.13862251\n",
      "Rockstar run 0.21148457\n",
      "Rockstar the 0.25644705\n",
      "Rockstar Rockstar 1.0\n",
      "Rockstar Halo 0.5246954\n",
      "Rockstar Finder 0.47402117\n",
      "Rockstar . 0.21749356\n",
      "Rockstar Is -0.05066611\n",
      "Rockstar it -0.1469146\n",
      "Rockstar somehow -0.034378137\n",
      "Rockstar possible 0.015921973\n",
      "Rockstar to -0.04271925\n",
      "Rockstar output 0.18053193\n",
      "Rockstar the 0.23078719\n",
      "Rockstar particle 0.26936972\n",
      "Rockstar data 0.13940422\n",
      "Rockstar of -0.061212983\n",
      "Rockstar each 0.03008525\n",
      "Rockstar halo 0.11717733\n",
      "Rockstar ? -0.04222009\n",
      "Rockstar According 0.07979578\n",
      "Rockstar to -0.030377502\n",
      "Rockstar the 0.17851937\n",
      "Rockstar Rockstar 1.0\n",
      "Rockstar User 0.42844254\n",
      "Rockstar 's 0.20227253\n",
      "Rockstar Guide 0.36845666\n",
      "Rockstar this 0.027006442\n",
      "Rockstar is -0.04870413\n",
      "Rockstar doable 0.03101355\n",
      "Rockstar but -0.13173614\n",
      "Rockstar I -0.050623063\n",
      "Rockstar could -0.031853277\n",
      "Rockstar n't -0.021795118\n",
      "Rockstar figure 0.08080608\n",
      "Rockstar out 0.041016474\n",
      "Rockstar how 0.007883923\n",
      "Rockstar in 0.05440898\n",
      "Rockstar yt 0.06922394\n",
      "Rockstar . -0.004425938\n",
      "Rockstar Thanks -0.13288647\n",
      "Rockstar in -0.18187015\n",
      "Rockstar advance 0.13529246\n",
      "Rockstar for -0.15041766\n",
      "Rockstar your 0.22025731\n",
      "Rockstar support 0.27753422\n",
      "Rockstar ! 0.07086655\n",
      "Halo Hi 0.0027026562\n",
      "Halo everyone 0.039163712\n",
      "Halo , -0.054049343\n",
      "Halo \n",
      " -0.17842487\n",
      "Halo I -0.015319316\n",
      "Halo 'm -0.282137\n",
      "Halo using -0.065457895\n",
      "Halo yt 0.011909549\n",
      "Halo to -0.12737788\n",
      "Halo run 0.0021444496\n",
      "Halo the 0.16795938\n",
      "Halo Rockstar 0.5246954\n",
      "Halo Halo 1.0\n",
      "Halo Finder 0.4017959\n",
      "Halo . 0.12200891\n",
      "Halo Is -0.01961173\n",
      "Halo it -0.13859455\n",
      "Halo somehow -0.028871464\n",
      "Halo possible 0.012107057\n",
      "Halo to -0.16482267\n",
      "Halo output -0.10386667\n",
      "Halo the 0.080422044\n",
      "Halo particle 0.14159225\n",
      "Halo data 0.10289323\n",
      "Halo of 0.16296367\n",
      "Halo each -0.14202389\n",
      "Halo halo 0.33261004\n",
      "Halo ? -0.051232897\n",
      "Halo According 0.017895345\n",
      "Halo to 0.17556685\n",
      "Halo the 0.042352192\n",
      "Halo Rockstar 0.5096655\n",
      "Halo User 0.542774\n",
      "Halo 's 0.21401754\n",
      "Halo Guide 0.37206903\n",
      "Halo this 0.039884783\n",
      "Halo is 0.06353398\n",
      "Halo doable 0.018762004\n",
      "Halo but -0.0065571344\n",
      "Halo I -0.09732211\n",
      "Halo could -0.0091885235\n",
      "Halo n't -0.01836821\n",
      "Halo figure -0.15575029\n",
      "Halo out -0.05605699\n",
      "Halo how 0.05908702\n",
      "Halo in 0.1612368\n",
      "Halo yt 0.0092675835\n",
      "Halo . -0.105963066\n",
      "Halo Thanks 0.012826049\n",
      "Halo in -0.06609577\n",
      "Halo advance -0.04201268\n",
      "Halo for 0.034106977\n",
      "Halo your -0.040498257\n",
      "Halo support 0.13409917\n",
      "Halo ! -0.18942472\n",
      "Finder Hi 0.04480707\n",
      "Finder everyone 0.12257428\n",
      "Finder , -0.015286101\n",
      "Finder \n",
      " 0.06963097\n",
      "Finder I 0.06870633\n",
      "Finder 'm -0.2026822\n",
      "Finder using -0.17298622\n",
      "Finder yt 0.15972428\n",
      "Finder to -0.17786765\n",
      "Finder run 0.10963173\n",
      "Finder the 0.12260781\n",
      "Finder Rockstar 0.47402117\n",
      "Finder Halo 0.4017959\n",
      "Finder Finder 1.0\n",
      "Finder . 0.21997482\n",
      "Finder Is -0.0017872582\n",
      "Finder it 0.004470498\n",
      "Finder somehow -0.17378728\n",
      "Finder possible -0.1525771\n",
      "Finder to -0.1681808\n",
      "Finder output 0.0421028\n",
      "Finder the 0.02755701\n",
      "Finder particle 0.20245034\n",
      "Finder data 0.16531196\n",
      "Finder of -0.11670855\n",
      "Finder each -0.15264717\n",
      "Finder halo 0.1844293\n",
      "Finder ? -0.12124964\n",
      "Finder According -0.09984278\n",
      "Finder to -0.06471595\n",
      "Finder the 0.0009723498\n",
      "Finder Rockstar 0.46337524\n",
      "Finder User 0.4625388\n",
      "Finder 's 0.16961434\n",
      "Finder Guide 0.29227856\n",
      "Finder this 0.068213925\n",
      "Finder is 0.059353195\n",
      "Finder doable -0.043880045\n",
      "Finder but -0.13810109\n",
      "Finder I -0.018801523\n",
      "Finder could -0.065800525\n",
      "Finder n't -0.08597771\n",
      "Finder figure 0.106061764\n",
      "Finder out -0.037703842\n",
      "Finder how -0.0815637\n",
      "Finder in -0.00360172\n",
      "Finder yt 0.15982367\n",
      "Finder . -0.07131491\n",
      "Finder Thanks 0.044440173\n",
      "Finder in -0.08068714\n",
      "Finder advance 0.08087063\n",
      "Finder for -0.19187692\n",
      "Finder your -0.048082516\n",
      "Finder support 0.34895846\n",
      "Finder ! -0.11136301\n",
      ". Hi -0.19044921\n",
      ". everyone -0.09285981\n",
      ". , 0.3402563\n",
      ". \n",
      " 0.47507364\n",
      ". I -0.12890658\n",
      ". 'm -0.095943764\n",
      ". using -0.027881393\n",
      ". yt -0.0277829\n",
      ". to -0.1708586\n",
      ". run 0.12645505\n",
      ". the 0.050424427\n",
      ". Rockstar 0.21749356\n",
      ". Halo 0.12200891\n",
      ". Finder 0.21997482\n",
      ". . 1.0\n",
      ". Is -0.084089905\n",
      ". it -0.18165639\n",
      ". somehow -0.14343618\n",
      ". possible -0.058412343\n",
      ". to -0.08575913\n",
      ". output 0.15605767\n",
      ". the 0.018458994\n",
      ". particle 0.1732419\n",
      ". data 0.021614086\n",
      ". of -0.13383129\n",
      ". each -0.076669745\n",
      ". halo 0.1153106\n",
      ". ? 0.45913053\n",
      ". According -0.11850517\n",
      ". to -0.018153507\n",
      ". the 0.06349347\n",
      ". Rockstar 0.23526704\n",
      ". User 0.3193336\n",
      ". 's 0.13238545\n",
      ". Guide 0.0669352\n",
      ". this 0.060826186\n",
      ". is 0.0005447097\n",
      ". doable 0.011936579\n",
      ". but 0.09880979\n",
      ". I -0.14595714\n",
      ". could -0.024150712\n",
      ". n't -0.15662464\n",
      ". figure 0.12537894\n",
      ". out 0.19181606\n",
      ". how 0.031469353\n",
      ". in -0.045441855\n",
      ". yt 0.052151427\n",
      ". . 1.0\n",
      ". Thanks -0.22257498\n",
      ". in -0.1555129\n",
      ". advance 0.045980137\n",
      ". for 0.017448498\n",
      ". your 0.007289701\n",
      ". support 0.115888655\n",
      ". ! 0.54608315\n",
      "Is Hi 0.2693308\n",
      "Is everyone -0.09353458\n",
      "Is , -0.09484016\n",
      "Is \n",
      " 0.033185042\n",
      "Is I 0.08598054\n",
      "Is 'm 0.33563945\n",
      "Is using 0.0453565\n",
      "Is yt 0.3053759\n",
      "Is to -0.04185065\n",
      "Is run -0.05003866\n",
      "Is the -0.13053907\n",
      "Is Rockstar -0.05066611\n",
      "Is Halo -0.01961173\n",
      "Is Finder -0.0017872582\n",
      "Is . -0.084089905\n",
      "Is Is 1.0\n",
      "Is it 0.14543648\n",
      "Is somehow 0.20661905\n",
      "Is possible -0.011520915\n",
      "Is to -0.099347256\n",
      "Is output 0.03811145\n",
      "Is the -0.17351986\n",
      "Is particle 0.01289293\n",
      "Is data 0.04048993\n",
      "Is of 0.025076007\n",
      "Is each -0.2064155\n",
      "Is halo -0.07308428\n",
      "Is ? -0.10650851\n",
      "Is According 0.27282178\n",
      "Is to 0.20643702\n",
      "Is the -0.16944236\n",
      "Is Rockstar -0.056891885\n",
      "Is User -0.06525545\n",
      "Is 's 0.03731652\n",
      "Is Guide -0.20114046\n",
      "Is this -0.26323748\n",
      "Is is 0.6230915\n",
      "Is doable -0.07364798\n",
      "Is but 0.189091\n",
      "Is I 0.16343494\n",
      "Is could 0.108498834\n",
      "Is n't 0.09943246\n",
      "Is figure 0.007106225\n",
      "Is out -0.016379839\n",
      "Is how 0.043337617\n",
      "Is in 0.15821132\n",
      "Is yt 0.12896928\n",
      "Is . -0.18026812\n",
      "Is Thanks 0.08921477\n",
      "Is in 0.20326507\n",
      "Is advance 0.044996463\n",
      "Is for 0.29261988\n",
      "Is your -0.050694443\n",
      "Is support -0.23585175\n",
      "Is ! -0.14175566\n",
      "it Hi 0.06745813\n",
      "it everyone 0.32168856\n",
      "it , -0.13336563\n",
      "it \n",
      " -0.035047855\n",
      "it I 0.4220554\n",
      "it 'm 0.14552249\n",
      "it using 0.1762023\n",
      "it yt 0.2975001\n",
      "it to 0.07726714\n",
      "it run -0.07439761\n",
      "it the 0.1118638\n",
      "it Rockstar -0.1469146\n",
      "it Halo -0.13859455\n",
      "it Finder 0.004470498\n",
      "it . -0.18165639\n",
      "it Is 0.14543648\n",
      "it it 1.0\n",
      "it somehow 0.17166369\n",
      "it possible 0.07103774\n",
      "it to -0.11596982\n",
      "it output 0.0280721\n",
      "it the 0.13122328\n",
      "it particle 0.09926889\n",
      "it data 0.09605843\n",
      "it of -0.10622105\n",
      "it each 0.1927987\n",
      "it halo 0.07410183\n",
      "it ? 0.02067557\n",
      "it According 0.024548931\n",
      "it to 0.0044000805\n",
      "it the 0.12133333\n",
      "it Rockstar -0.19271216\n",
      "it User -0.14573547\n",
      "it 's 0.18931647\n",
      "it Guide -0.08808726\n",
      "it this 0.26529962\n",
      "it is 0.051451404\n",
      "it doable 0.22195399\n",
      "it but -0.019734042\n",
      "it I 0.43858194\n",
      "it could -0.032118734\n",
      "it n't 0.24182206\n",
      "it figure 0.08864164\n",
      "it out 0.008641338\n",
      "it how -0.19737291\n",
      "it in -0.03427149\n",
      "it yt 0.25627556\n",
      "it . -0.10971745\n",
      "it Thanks 0.028499262\n",
      "it in -0.072115175\n",
      "it advance 0.12488499\n",
      "it for -0.12460923\n",
      "it your 0.12894712\n",
      "it support 0.019268168\n",
      "it ! -0.11508507\n",
      "somehow Hi 0.25642216\n",
      "somehow everyone -0.06272408\n",
      "somehow , -0.0950506\n",
      "somehow \n",
      " -0.13940838\n",
      "somehow I 0.0009206402\n",
      "somehow 'm 0.11464592\n",
      "somehow using 0.31351247\n",
      "somehow yt 0.022701593\n",
      "somehow to 0.09439148\n",
      "somehow run 0.05490173\n",
      "somehow the -0.013010614\n",
      "somehow Rockstar -0.034378137\n",
      "somehow Halo -0.028871464\n",
      "somehow Finder -0.17378728\n",
      "somehow . -0.14343618\n",
      "somehow Is 0.20661905\n",
      "somehow it 0.17166369\n",
      "somehow somehow 1.0\n",
      "somehow possible 0.2474136\n",
      "somehow to -0.097790286\n",
      "somehow output 0.027558053\n",
      "somehow the 0.003981133\n",
      "somehow particle -0.07638175\n",
      "somehow data 0.10624579\n",
      "somehow of 0.07400969\n",
      "somehow each -0.029035093\n",
      "somehow halo 0.08178019\n",
      "somehow ? -0.012753352\n",
      "somehow According 0.3000083\n",
      "somehow to 0.0154123325\n",
      "somehow the -0.07528896\n",
      "somehow Rockstar -0.09796393\n",
      "somehow User -0.1020392\n",
      "somehow 's 0.099809214\n",
      "somehow Guide -0.019196136\n",
      "somehow this 0.031445876\n",
      "somehow is 0.21620373\n",
      "somehow doable 0.14700569\n",
      "somehow but 0.04602209\n",
      "somehow I -0.13066109\n",
      "somehow could 0.14211333\n",
      "somehow n't 0.3515904\n",
      "somehow figure 0.018862886\n",
      "somehow out 0.19122943\n",
      "somehow how 0.21505533\n",
      "somehow in 0.2876073\n",
      "somehow yt -0.05675933\n",
      "somehow . -0.19022655\n",
      "somehow Thanks 0.0024256539\n",
      "somehow in 0.084836185\n",
      "somehow advance 0.089878656\n",
      "somehow for 0.030575382\n",
      "somehow your -0.038063455\n",
      "somehow support -0.057133935\n",
      "somehow ! -0.026333857\n",
      "possible Hi 0.025402823\n",
      "possible everyone 0.03270703\n",
      "possible , -0.018160397\n",
      "possible \n",
      " -0.18837376\n",
      "possible I -0.08860382\n",
      "possible 'm -0.0075994763\n",
      "possible using 0.2741956\n",
      "possible yt 0.03148418\n",
      "possible to -0.04187636\n",
      "possible run -0.07286238\n",
      "possible the -0.034634512\n",
      "possible Rockstar 0.015921973\n",
      "possible Halo 0.012107057\n",
      "possible Finder -0.1525771\n",
      "possible . -0.058412343\n",
      "possible Is -0.011520915\n",
      "possible it 0.07103774\n",
      "possible somehow 0.2474136\n",
      "possible possible 1.0\n",
      "possible to 0.040803675\n",
      "possible output 0.036198977\n",
      "possible the 0.0068078935\n",
      "possible particle 0.022542492\n",
      "possible data 0.00022761502\n",
      "possible of 0.094135635\n",
      "possible each 0.056882765\n",
      "possible halo 0.012309071\n",
      "possible ? -0.063771926\n",
      "possible According 0.234071\n",
      "possible to 0.13359553\n",
      "possible the -0.08242155\n",
      "possible Rockstar 0.0007855175\n",
      "possible User 0.0045279213\n",
      "possible 's 0.008607433\n",
      "possible Guide 0.034875546\n",
      "possible this 0.0064453944\n",
      "possible is -0.06975916\n",
      "possible doable 0.31968272\n",
      "possible but -0.09523223\n",
      "possible I 0.0803756\n",
      "possible could 0.03850741\n",
      "possible n't 0.06361824\n",
      "possible figure 0.0033471412\n",
      "possible out 0.13867223\n",
      "possible how 0.023736954\n",
      "possible in 0.22202983\n",
      "possible yt 0.010213123\n",
      "possible . -0.15436675\n",
      "possible Thanks -0.04071031\n",
      "possible in 0.17624831\n",
      "possible advance 0.04229015\n",
      "possible for 0.16528064\n",
      "possible your 0.09800035\n",
      "possible support -0.024780866\n",
      "possible ! -0.022037962\n",
      "to Hi -0.08799042\n",
      "to everyone 0.002348697\n",
      "to , 0.033961166\n",
      "to \n",
      " -0.0468279\n",
      "to I -0.11028195\n",
      "to 'm 0.01586986\n",
      "to using -0.03661327\n",
      "to yt 0.13358907\n",
      "to to 1.0\n",
      "to run -0.1651402\n",
      "to the -0.039305814\n",
      "to Rockstar -0.04271925\n",
      "to Halo -0.16482267\n",
      "to Finder -0.1681808\n",
      "to . -0.08575913\n",
      "to Is -0.099347256\n",
      "to it -0.11596982\n",
      "to somehow -0.097790286\n",
      "to possible 0.040803675\n",
      "to to 1.0\n",
      "to output -0.12784606\n",
      "to the 0.048908412\n",
      "to particle -0.10827045\n",
      "to data -0.1553437\n",
      "to of 0.09807049\n",
      "to each -0.05595333\n",
      "to halo -0.1213587\n",
      "to ? -0.043715347\n",
      "to According 0.105038844\n",
      "to to 1.0\n",
      "to the 0.066865735\n",
      "to Rockstar -0.07735791\n",
      "to User -0.14790468\n",
      "to 's -0.058544144\n",
      "to Guide -0.16534562\n",
      "to this -0.10664879\n",
      "to is 0.026388498\n",
      "to doable -0.1400799\n",
      "to but 0.0049328026\n",
      "to I -0.10245125\n",
      "to could 0.23174599\n",
      "to n't 0.12790039\n",
      "to figure 0.007557381\n",
      "to out -0.065104894\n",
      "to how 0.036792025\n",
      "to in -0.029869908\n",
      "to yt 0.031461198\n",
      "to . 0.023298187\n",
      "to Thanks -0.0710926\n",
      "to in 0.20484072\n",
      "to advance -0.07316365\n",
      "to for 0.20874995\n",
      "to your 0.14381692\n",
      "to support -0.112805866\n",
      "to ! 0.058788188\n",
      "output Hi -0.007910854\n",
      "output everyone 0.25352177\n",
      "output , 0.113222964\n",
      "output \n",
      " -0.10554969\n",
      "output I 0.03134295\n",
      "output 'm 0.13482359\n",
      "output using 0.19518432\n",
      "output yt -0.10802229\n",
      "output to -0.041214302\n",
      "output run 0.6964679\n",
      "output the 0.059917685\n",
      "output Rockstar 0.18053193\n",
      "output Halo -0.10386667\n",
      "output Finder 0.0421028\n",
      "output . 0.15605767\n",
      "output Is 0.03811145\n",
      "output it 0.0280721\n",
      "output somehow 0.027558053\n",
      "output possible 0.036198977\n",
      "output to -0.12784606\n",
      "output output 1.0\n",
      "output the 0.1303867\n",
      "output particle 0.31672007\n",
      "output data 0.040605392\n",
      "output of -0.08194725\n",
      "output each 0.016645778\n",
      "output halo 0.0572561\n",
      "output ? 0.13215216\n",
      "output According 0.121755235\n",
      "output to 0.06993556\n",
      "output the 0.07088504\n",
      "output Rockstar 0.122743376\n",
      "output User 0.088798255\n",
      "output 's 0.105359554\n",
      "output Guide 0.22165719\n",
      "output this -0.025200222\n",
      "output is 0.009678711\n",
      "output doable 0.17688337\n",
      "output but -0.1783708\n",
      "output I 0.032755867\n",
      "output could 0.084520735\n",
      "output n't 0.18219295\n",
      "output figure 0.66999924\n",
      "output out 0.118245475\n",
      "output how -0.055936303\n",
      "output in -0.22124948\n",
      "output yt -0.086070985\n",
      "output . 0.1270131\n",
      "output Thanks -0.18077327\n",
      "output in -0.17168172\n",
      "output advance 0.21518347\n",
      "output for 0.012280922\n",
      "output your 0.10969101\n",
      "output support 0.23892343\n",
      "output ! 0.14299308\n",
      "the Hi -0.037475307\n",
      "the everyone 0.20397924\n",
      "the , -0.06947174\n",
      "the \n",
      " -0.0776435\n",
      "the I 0.11392936\n",
      "the 'm -0.020938205\n",
      "the using 0.09536544\n",
      "the yt 0.095337294\n",
      "the to 0.027234748\n",
      "the run -0.0318532\n",
      "the the 1.0\n",
      "the Rockstar 0.23078719\n",
      "the Halo 0.080422044\n",
      "the Finder 0.02755701\n",
      "the . 0.018458994\n",
      "the Is -0.17351986\n",
      "the it 0.13122328\n",
      "the somehow 0.003981133\n",
      "the possible 0.0068078935\n",
      "the to 0.048908412\n",
      "the output 0.1303867\n",
      "the the 1.0\n",
      "the particle 0.21879245\n",
      "the data -0.07065709\n",
      "the of -0.041817017\n",
      "the each 0.5801839\n",
      "the halo 0.094016016\n",
      "the ? -0.18260278\n",
      "the According -0.07856332\n",
      "the to -0.04635867\n",
      "the the 1.0\n",
      "the Rockstar 0.19914648\n",
      "the User -0.026512329\n",
      "the 's 0.104192525\n",
      "the Guide 0.047563918\n",
      "the this 0.38907346\n",
      "the is -0.05613849\n",
      "the doable 0.20757215\n",
      "the but -0.015200244\n",
      "the I 0.015701955\n",
      "the could -0.13886581\n",
      "the n't 0.04735156\n",
      "the figure 0.12623407\n",
      "the out 0.15013298\n",
      "the how -0.034289464\n",
      "the in 0.058818918\n",
      "the yt 0.10962311\n",
      "the . -0.10659661\n",
      "the Thanks -0.19854034\n",
      "the in -0.07129996\n",
      "the advance 0.123630516\n",
      "the for -0.064804554\n",
      "the your 0.26874754\n",
      "the support 0.15924889\n",
      "the ! -0.0561242\n",
      "particle Hi -0.08878847\n",
      "particle everyone 0.3265038\n",
      "particle , 0.11529388\n",
      "particle \n",
      " 0.017756859\n",
      "particle I 0.08754201\n",
      "particle 'm -0.029605273\n",
      "particle using 0.12391549\n",
      "particle yt 0.022884829\n",
      "particle to 0.06274484\n",
      "particle run 0.1591395\n",
      "particle the 0.15345916\n",
      "particle Rockstar 0.26936972\n",
      "particle Halo 0.14159225\n",
      "particle Finder 0.20245034\n",
      "particle . 0.1732419\n",
      "particle Is 0.01289293\n",
      "particle it 0.09926889\n",
      "particle somehow -0.07638175\n",
      "particle possible 0.022542492\n",
      "particle to -0.10827045\n",
      "particle output 0.31672007\n",
      "particle the 0.21879245\n",
      "particle particle 1.0\n",
      "particle data 0.18301605\n",
      "particle of -0.04075996\n",
      "particle each 0.21713786\n",
      "particle halo 0.37972528\n",
      "particle ? 0.07096346\n",
      "particle According -0.006231167\n",
      "particle to 0.09466346\n",
      "particle the 0.13842753\n",
      "particle Rockstar 0.3101408\n",
      "particle User 0.27710193\n",
      "particle 's 0.13526975\n",
      "particle Guide 0.18314521\n",
      "particle this 0.20963502\n",
      "particle is 0.14135975\n",
      "particle doable 0.3778746\n",
      "particle but 0.060628258\n",
      "particle I 0.10760016\n",
      "particle could -0.058279265\n",
      "particle n't -0.07677289\n",
      "particle figure 0.35977444\n",
      "particle out -0.03255387\n",
      "particle how -0.09921537\n",
      "particle in 0.049312424\n",
      "particle yt 0.08280672\n",
      "particle . 0.08287049\n",
      "particle Thanks -0.09756437\n",
      "particle in -0.14005204\n",
      "particle advance 0.3468109\n",
      "particle for -0.0357185\n",
      "particle your 0.13017987\n",
      "particle support 0.4836165\n",
      "particle ! 0.03790695\n",
      "data Hi 0.015099464\n",
      "data everyone 0.085335046\n",
      "data , -0.046089042\n",
      "data \n",
      " -0.08929576\n",
      "data I 0.021290293\n",
      "data 'm -0.08533268\n",
      "data using -0.117895484\n",
      "data yt 0.11492802\n",
      "data to -0.04640098\n",
      "data run -0.0038735848\n",
      "data the -0.016274706\n",
      "data Rockstar 0.13940422\n",
      "data Halo 0.10289323\n",
      "data Finder 0.16531196\n",
      "data . 0.021614086\n",
      "data Is 0.04048993\n",
      "data it 0.09605843\n",
      "data somehow 0.10624579\n",
      "data possible 0.00022761502\n",
      "data to -0.1553437\n",
      "data output 0.040605392\n",
      "data the -0.07065709\n",
      "data particle 0.18301605\n",
      "data data 1.0\n",
      "data of 0.25950104\n",
      "data each 0.10095868\n",
      "data halo 0.38702294\n",
      "data ? 0.067374155\n",
      "data According -0.024149261\n",
      "data to 0.037385978\n",
      "data the -0.06731491\n",
      "data Rockstar 0.051007003\n",
      "data User 0.13462913\n",
      "data 's 0.10898526\n",
      "data Guide 0.17225534\n",
      "data this 0.005282925\n",
      "data is 0.15308906\n",
      "data doable -0.0823356\n",
      "data but 0.08977919\n",
      "data I 0.068021335\n",
      "data could -0.05153165\n",
      "data n't 0.0078020506\n",
      "data figure 0.014231263\n",
      "data out 0.05411644\n",
      "data how 0.09073041\n",
      "data in 0.015092228\n",
      "data yt 0.23831664\n",
      "data . 0.17365837\n",
      "data Thanks 0.24375492\n",
      "data in 0.018843018\n",
      "data advance 0.19663858\n",
      "data for -0.009910587\n",
      "data your 0.11100432\n",
      "data support 0.32190624\n",
      "data ! 0.13984138\n",
      "of Hi 0.13451265\n",
      "of everyone 0.07311386\n",
      "of , 0.028167095\n",
      "of \n",
      " -0.05795885\n",
      "of I 0.041479275\n",
      "of 'm -0.1734136\n",
      "of using 0.13807622\n",
      "of yt 0.04761757\n",
      "of to 0.13128139\n",
      "of run 0.026041819\n",
      "of the 0.04522984\n",
      "of Rockstar -0.061212983\n",
      "of Halo 0.16296367\n",
      "of Finder -0.11670855\n",
      "of . -0.13383129\n",
      "of Is 0.025076007\n",
      "of it -0.10622105\n",
      "of somehow 0.07400969\n",
      "of possible 0.094135635\n",
      "of to 0.09807049\n",
      "of output -0.08194725\n",
      "of the -0.041817017\n",
      "of particle -0.04075996\n",
      "of data 0.25950104\n",
      "of of 1.0\n",
      "of each 0.051554672\n",
      "of halo 0.19421938\n",
      "of ? 0.06550537\n",
      "of According 0.18716998\n",
      "of to 0.47461262\n",
      "of the -0.017156808\n",
      "of Rockstar -0.069769956\n",
      "of User 0.0595424\n",
      "of 's 0.042598836\n",
      "of Guide 0.1435809\n",
      "of this -0.043391727\n",
      "of is 0.0020557013\n",
      "of doable -0.10231545\n",
      "of but 0.2423468\n",
      "of I 0.08422053\n",
      "of could -0.03962203\n",
      "of n't -0.010889377\n",
      "of figure -0.10056594\n",
      "of out 0.016786195\n",
      "of how 0.09295072\n",
      "of in 0.29202363\n",
      "of yt 0.044282183\n",
      "of . 0.0067066923\n",
      "of Thanks 0.114243366\n",
      "of in 0.4775281\n",
      "of advance 0.011162984\n",
      "of for 0.4857952\n",
      "of your 0.042999435\n",
      "of support -0.06815974\n",
      "of ! 0.056811783\n",
      "each Hi 0.006411853\n",
      "each everyone 0.14303492\n",
      "each , -0.057537\n",
      "each \n",
      " 0.03370298\n",
      "each I 0.13482146\n",
      "each 'm 0.1681117\n",
      "each using -0.0155991465\n",
      "each yt 0.07889548\n",
      "each to -0.051443957\n",
      "each run -0.058466543\n",
      "each the 0.4863022\n",
      "each Rockstar 0.03008525\n",
      "each Halo -0.14202389\n",
      "each Finder -0.15264717\n",
      "each . -0.076669745\n",
      "each Is -0.2064155\n",
      "each it 0.1927987\n",
      "each somehow -0.029035093\n",
      "each possible 0.056882765\n",
      "each to -0.05595333\n",
      "each output 0.016645778\n",
      "each the 0.5801839\n",
      "each particle 0.21713786\n",
      "each data 0.10095868\n",
      "each of 0.051554672\n",
      "each each 1.0\n",
      "each halo 0.07895802\n",
      "each ? -0.08654324\n",
      "each According -0.15057713\n",
      "each to -0.081058964\n",
      "each the 0.58236307\n",
      "each Rockstar 0.018924303\n",
      "each User -0.18606101\n",
      "each 's -0.006576972\n",
      "each Guide -0.039469063\n",
      "each this 0.48755175\n",
      "each is -0.16573285\n",
      "each doable 0.19608854\n",
      "each but 0.21544038\n",
      "each I 0.08946361\n",
      "each could -0.07320853\n",
      "each n't 0.097630255\n",
      "each figure 0.06659406\n",
      "each out 0.0028671413\n",
      "each how -0.04407756\n",
      "each in -0.009775211\n",
      "each yt 0.1071525\n",
      "each . 0.047667842\n",
      "each Thanks 0.01821319\n",
      "each in 0.015759995\n",
      "each advance 0.18523087\n",
      "each for 0.013259723\n",
      "each your 0.14118399\n",
      "each support 0.20187037\n",
      "each ! 0.025847495\n",
      "halo Hi -0.013925342\n",
      "halo everyone 0.3239822\n",
      "halo , -0.014573961\n",
      "halo \n",
      " -0.06816956\n",
      "halo I 0.06118081\n",
      "halo 'm -0.22172675\n",
      "halo using -0.04774718\n",
      "halo yt 0.06632083\n",
      "halo to -0.04011447\n",
      "halo run -0.011405501\n",
      "halo the 0.10922449\n",
      "halo Rockstar 0.11717733\n",
      "halo Halo 0.33261004\n",
      "halo Finder 0.1844293\n",
      "halo . 0.1153106\n",
      "halo Is -0.07308428\n",
      "halo it 0.07410183\n",
      "halo somehow 0.08178019\n",
      "halo possible 0.012309071\n",
      "halo to -0.1213587\n",
      "halo output 0.0572561\n",
      "halo the 0.094016016\n",
      "halo particle 0.37972528\n",
      "halo data 0.38702294\n",
      "halo of 0.19421938\n",
      "halo each 0.07895802\n",
      "halo halo 1.0\n",
      "halo ? 0.106671974\n",
      "halo According -0.068045065\n",
      "halo to 0.092476934\n",
      "halo the 0.14243181\n",
      "halo Rockstar 0.13535783\n",
      "halo User 0.24244772\n",
      "halo 's 0.06705607\n",
      "halo Guide 0.1205808\n",
      "halo this 0.22864294\n",
      "halo is 0.1637306\n",
      "halo doable 0.1051275\n",
      "halo but 0.029044695\n",
      "halo I 0.058038555\n",
      "halo could -0.045268435\n",
      "halo n't -0.11455005\n",
      "halo figure 0.08918373\n",
      "halo out 0.18440455\n",
      "halo how -0.006342878\n",
      "halo in 0.06399074\n",
      "halo yt 0.09594962\n",
      "halo . 0.08299859\n",
      "halo Thanks 0.10038113\n",
      "halo in -0.076900445\n",
      "halo advance 0.24771968\n",
      "halo for 0.06577955\n",
      "halo your 0.005054288\n",
      "halo support 0.33053726\n",
      "halo ! 0.03781329\n",
      "? Hi -0.20968693\n",
      "? everyone 0.14608173\n",
      "? , 0.4724007\n",
      "? \n",
      " 0.33923057\n",
      "? I -0.17093109\n",
      "? 'm -0.025267573\n",
      "? using 0.019607946\n",
      "? yt -0.15787925\n",
      "? to -0.08407705\n",
      "? run 0.0861451\n",
      "? the -0.1051755\n",
      "? Rockstar -0.04222009\n",
      "? Halo -0.051232897\n",
      "? Finder -0.12124964\n",
      "? . 0.45913053\n",
      "? Is -0.10650851\n",
      "? it 0.02067557\n",
      "? somehow -0.012753352\n",
      "? possible -0.063771926\n",
      "? to -0.043715347\n",
      "? output 0.13215216\n",
      "? the -0.18260278\n",
      "? particle 0.07096346\n",
      "? data 0.067374155\n",
      "? of 0.06550537\n",
      "? each -0.08654324\n",
      "? halo 0.106671974\n",
      "? ? 1.0\n",
      "? According 0.02468864\n",
      "? to 0.15173993\n",
      "? the 0.02470891\n",
      "? Rockstar -0.07585234\n",
      "? User -0.0044438415\n",
      "? 's 0.0060102968\n",
      "? Guide 0.016898988\n",
      "? this -0.013774075\n",
      "? is -0.08026768\n",
      "? doable -0.06616074\n",
      "? but 0.10009476\n",
      "? I -0.07543726\n",
      "? could -0.01209153\n",
      "? n't -0.080719635\n",
      "? figure 0.04011371\n",
      "? out 0.18727036\n",
      "? how 0.025893416\n",
      "? in -0.036419544\n",
      "? yt -0.089471914\n",
      "? . 0.5391788\n",
      "? Thanks -0.084023245\n",
      "? in -0.009319135\n",
      "? advance -0.037886236\n",
      "? for 0.2557233\n",
      "? your 0.05054802\n",
      "? support -0.04594513\n",
      "? ! 0.6238686\n",
      "According Hi 0.07759787\n",
      "According everyone 0.011680606\n",
      "According , 0.03919615\n",
      "According \n",
      " -0.12629364\n",
      "According I -0.12819633\n",
      "According 'm -0.0165855\n",
      "According using 0.645737\n",
      "According yt -0.037718948\n",
      "According to 0.15887827\n",
      "According run 0.008251625\n",
      "According the 0.021663137\n",
      "According Rockstar 0.07979578\n",
      "According Halo 0.017895345\n",
      "According Finder -0.09984278\n",
      "According . -0.11850517\n",
      "According Is 0.27282178\n",
      "According it 0.024548931\n",
      "According somehow 0.3000083\n",
      "According possible 0.234071\n",
      "According to 0.105038844\n",
      "According output 0.121755235\n",
      "According the -0.07856332\n",
      "According particle -0.006231167\n",
      "According data -0.024149261\n",
      "According of 0.18716998\n",
      "According each -0.15057713\n",
      "According halo -0.068045065\n",
      "According ? 0.02468864\n",
      "According According 1.0\n",
      "According to 0.2531866\n",
      "According the 0.042663544\n",
      "According Rockstar 0.08579065\n",
      "According User -0.08171353\n",
      "According 's 0.03216893\n",
      "According Guide -0.09408394\n",
      "According this -0.05635079\n",
      "According is 0.04956142\n",
      "According doable -0.12220848\n",
      "According but -0.10962342\n",
      "According I -0.09799604\n",
      "According could -0.023372576\n",
      "According n't 0.04671822\n",
      "According figure 0.047828242\n",
      "According out -0.037203893\n",
      "According how 0.078383565\n",
      "According in 0.10840529\n",
      "According yt -0.16862166\n",
      "According . -0.107093304\n",
      "According Thanks 0.14086334\n",
      "According in 0.2113472\n",
      "According advance 0.23463167\n",
      "According for 0.3590549\n",
      "According your 0.08722653\n",
      "According support -0.12938105\n",
      "According ! 0.017523246\n",
      "to Hi 0.043419085\n",
      "to everyone 0.07447013\n",
      "to , -0.07740964\n",
      "to \n",
      " -0.093866594\n",
      "to I -0.0030580128\n",
      "to 'm -0.03759129\n",
      "to using 0.10226652\n",
      "to yt 0.2678223\n",
      "to to 1.0\n",
      "to run 0.13605937\n",
      "to the 0.07650483\n",
      "to Rockstar -0.030377502\n",
      "to Halo 0.17556685\n",
      "to Finder -0.06471595\n",
      "to . -0.018153507\n",
      "to Is 0.20643702\n",
      "to it 0.0044000805\n",
      "to somehow 0.0154123325\n",
      "to possible 0.13359553\n",
      "to to 1.0\n",
      "to output 0.06993556\n",
      "to the -0.04635867\n",
      "to particle 0.09466346\n",
      "to data 0.037385978\n",
      "to of 0.47461262\n",
      "to each -0.081058964\n",
      "to halo 0.092476934\n",
      "to ? 0.15173993\n",
      "to According 0.2531866\n",
      "to to 1.0\n",
      "to the 0.12846579\n",
      "to Rockstar -0.055221662\n",
      "to User 0.06442037\n",
      "to 's 0.10365828\n",
      "to Guide 0.077160716\n",
      "to this -0.106798984\n",
      "to is 0.00522274\n",
      "to doable -0.046517767\n",
      "to but 0.090312056\n",
      "to I 0.06239065\n",
      "to could -0.05168084\n",
      "to n't -0.04769439\n",
      "to figure -0.08059852\n",
      "to out 0.07681805\n",
      "to how 0.08205119\n",
      "to in 0.37831128\n",
      "to yt 0.13520823\n",
      "to . -0.1107612\n",
      "to Thanks 0.064324014\n",
      "to in 0.48802248\n",
      "to advance 0.032895043\n",
      "to for 0.56109077\n",
      "to your 0.14660972\n",
      "to support -0.07494461\n",
      "to ! -0.0359422\n",
      "the Hi -0.05278793\n",
      "the everyone 0.19948292\n",
      "the , -0.13619632\n",
      "the \n",
      " -0.011273731\n",
      "the I 0.02237848\n",
      "the 'm -0.010372122\n",
      "the using 0.07374004\n",
      "the yt -0.015854087\n",
      "the to 0.036872555\n",
      "the run 0.006804403\n",
      "the the 1.0\n",
      "the Rockstar 0.17851937\n",
      "the Halo 0.042352192\n",
      "the Finder 0.0009723498\n",
      "the . 0.06349347\n",
      "the Is -0.16944236\n",
      "the it 0.12133333\n",
      "the somehow -0.07528896\n",
      "the possible -0.08242155\n",
      "the to 0.066865735\n",
      "the output 0.07088504\n",
      "the the 1.0\n",
      "the particle 0.13842753\n",
      "the data -0.06731491\n",
      "the of -0.017156808\n",
      "the each 0.58236307\n",
      "the halo 0.14243181\n",
      "the ? 0.02470891\n",
      "the According 0.042663544\n",
      "the to 0.12846579\n",
      "the the 1.0\n",
      "the Rockstar 0.16550244\n",
      "the User -0.12681398\n",
      "the 's 0.08627405\n",
      "the Guide -0.11736892\n",
      "the this 0.4860723\n",
      "the is -0.16347647\n",
      "the doable 0.1369192\n",
      "the but 0.10358103\n",
      "the I -0.01562734\n",
      "the could -0.15729651\n",
      "the n't -0.024812775\n",
      "the figure 0.13485587\n",
      "the out 0.19616686\n",
      "the how -0.1682779\n",
      "the in 0.016842\n",
      "the yt 0.063884586\n",
      "the . 0.011464027\n",
      "the Thanks 0.00058312924\n",
      "the in 0.021637794\n",
      "the advance 0.18232709\n",
      "the for 0.07752022\n",
      "the your 0.2595377\n",
      "the support 0.17213617\n",
      "the ! 0.04621339\n",
      "Rockstar Hi -0.12526739\n",
      "Rockstar everyone 0.014099446\n",
      "Rockstar , -0.059447847\n",
      "Rockstar \n",
      " -0.1391312\n",
      "Rockstar I -0.07608498\n",
      "Rockstar 'm -0.22432244\n",
      "Rockstar using 0.041675135\n",
      "Rockstar yt 0.04929925\n",
      "Rockstar to -0.21000984\n",
      "Rockstar run 0.11733546\n",
      "Rockstar the 0.26276764\n",
      "Rockstar Rockstar 1.0\n",
      "Rockstar Halo 0.5096655\n",
      "Rockstar Finder 0.46337524\n",
      "Rockstar . 0.23526704\n",
      "Rockstar Is -0.056891885\n",
      "Rockstar it -0.19271216\n",
      "Rockstar somehow -0.09796393\n",
      "Rockstar possible 0.0007855175\n",
      "Rockstar to -0.07735791\n",
      "Rockstar output 0.122743376\n",
      "Rockstar the 0.19914648\n",
      "Rockstar particle 0.3101408\n",
      "Rockstar data 0.051007003\n",
      "Rockstar of -0.069769956\n",
      "Rockstar each 0.018924303\n",
      "Rockstar halo 0.13535783\n",
      "Rockstar ? -0.07585234\n",
      "Rockstar According 0.08579065\n",
      "Rockstar to -0.055221662\n",
      "Rockstar the 0.16550244\n",
      "Rockstar Rockstar 1.0\n",
      "Rockstar User 0.4669787\n",
      "Rockstar 's 0.1467174\n",
      "Rockstar Guide 0.40988538\n",
      "Rockstar this 0.030118093\n",
      "Rockstar is -0.035133343\n",
      "Rockstar doable 0.08361957\n",
      "Rockstar but -0.081966884\n",
      "Rockstar I -0.10486702\n",
      "Rockstar could -0.09588411\n",
      "Rockstar n't -0.10571424\n",
      "Rockstar figure 0.018192664\n",
      "Rockstar out -0.09190575\n",
      "Rockstar how 0.011635369\n",
      "Rockstar in -0.009182566\n",
      "Rockstar yt 0.072417185\n",
      "Rockstar . -0.041844286\n",
      "Rockstar Thanks -0.1399497\n",
      "Rockstar in -0.17127536\n",
      "Rockstar advance 0.12851897\n",
      "Rockstar for -0.21349734\n",
      "Rockstar your 0.18675376\n",
      "Rockstar support 0.23606114\n",
      "Rockstar ! -0.018537143\n",
      "User Hi 0.027806975\n",
      "User everyone 0.25891036\n",
      "User , -0.057446495\n",
      "User \n",
      " -0.045758598\n",
      "User I 0.15186216\n",
      "User 'm -0.15941754\n",
      "User using -0.012871384\n",
      "User yt 0.046476044\n",
      "User to -0.086429246\n",
      "User run 0.16013128\n",
      "User the 0.026483657\n",
      "User Rockstar 0.42844254\n",
      "User Halo 0.542774\n",
      "User Finder 0.4625388\n",
      "User . 0.3193336\n",
      "User Is -0.06525545\n",
      "User it -0.14573547\n",
      "User somehow -0.1020392\n",
      "User possible 0.0045279213\n",
      "User to -0.14790468\n",
      "User output 0.088798255\n",
      "User the -0.026512329\n",
      "User particle 0.27710193\n",
      "User data 0.13462913\n",
      "User of 0.0595424\n",
      "User each -0.18606101\n",
      "User halo 0.24244772\n",
      "User ? -0.0044438415\n",
      "User According -0.08171353\n",
      "User to 0.06442037\n",
      "User the -0.12681398\n",
      "User Rockstar 0.4669787\n",
      "User User 1.0\n",
      "User 's 0.2230503\n",
      "User Guide 0.52911276\n",
      "User this 0.02770623\n",
      "User is 0.07215813\n",
      "User doable 0.025064588\n",
      "User but -0.07607803\n",
      "User I 0.05767167\n",
      "User could 0.030393645\n",
      "User n't 0.041965764\n",
      "User figure 0.11538557\n",
      "User out -0.030613236\n",
      "User how 0.044041403\n",
      "User in 0.11047234\n",
      "User yt 0.08121725\n",
      "User . 0.03750283\n",
      "User Thanks -0.08127514\n",
      "User in -0.021307776\n",
      "User advance 0.08522288\n",
      "User for -0.07989366\n",
      "User your -0.006296432\n",
      "User support 0.20489958\n",
      "User ! -0.106060244\n",
      "'s Hi -0.11499486\n",
      "'s everyone 0.10066008\n",
      "'s , 0.026969321\n",
      "'s \n",
      " -0.26694527\n",
      "'s I 0.06450714\n",
      "'s 'm 0.08304575\n",
      "'s using 0.18111208\n",
      "'s yt 0.07281221\n",
      "'s to -0.043949537\n",
      "'s run 0.12732849\n",
      "'s the 0.15365063\n",
      "'s Rockstar 0.20227253\n",
      "'s Halo 0.21401754\n",
      "'s Finder 0.16961434\n",
      "'s . 0.13238545\n",
      "'s Is 0.03731652\n",
      "'s it 0.18931647\n",
      "'s somehow 0.099809214\n",
      "'s possible 0.008607433\n",
      "'s to -0.058544144\n",
      "'s output 0.105359554\n",
      "'s the 0.104192525\n",
      "'s particle 0.13526975\n",
      "'s data 0.10898526\n",
      "'s of 0.042598836\n",
      "'s each -0.006576972\n",
      "'s halo 0.06705607\n",
      "'s ? 0.0060102968\n",
      "'s According 0.03216893\n",
      "'s to 0.10365828\n",
      "'s the 0.08627405\n",
      "'s Rockstar 0.1467174\n",
      "'s User 0.2230503\n",
      "'s 's 1.0\n",
      "'s Guide 0.0674446\n",
      "'s this 0.14786719\n",
      "'s is 0.14525472\n",
      "'s doable -0.02272614\n",
      "'s but -0.0781769\n",
      "'s I -0.028433004\n",
      "'s could -0.088489965\n",
      "'s n't 0.028292548\n",
      "'s figure 0.08651839\n",
      "'s out 0.06331945\n",
      "'s how -0.1167579\n",
      "'s in 0.02370227\n",
      "'s yt 0.08759573\n",
      "'s . -0.1990582\n",
      "'s Thanks -0.15886176\n",
      "'s in 0.05538434\n",
      "'s advance 0.15334179\n",
      "'s for 0.01078029\n",
      "'s your 0.045117952\n",
      "'s support 0.040737998\n",
      "'s ! 0.04484212\n",
      "Guide Hi 0.13340518\n",
      "Guide everyone 0.21810208\n",
      "Guide , 0.043101933\n",
      "Guide \n",
      " -0.004773321\n",
      "Guide I 0.106838115\n",
      "Guide 'm -0.18615584\n",
      "Guide using 0.015719067\n",
      "Guide yt 0.024250224\n",
      "Guide to -0.14244793\n",
      "Guide run 0.3220655\n",
      "Guide the 0.023708427\n",
      "Guide Rockstar 0.36845666\n",
      "Guide Halo 0.37206903\n",
      "Guide Finder 0.29227856\n",
      "Guide . 0.0669352\n",
      "Guide Is -0.20114046\n",
      "Guide it -0.08808726\n",
      "Guide somehow -0.019196136\n",
      "Guide possible 0.034875546\n",
      "Guide to -0.16534562\n",
      "Guide output 0.22165719\n",
      "Guide the 0.047563918\n",
      "Guide particle 0.18314521\n",
      "Guide data 0.17225534\n",
      "Guide of 0.1435809\n",
      "Guide each -0.039469063\n",
      "Guide halo 0.1205808\n",
      "Guide ? 0.016898988\n",
      "Guide According -0.09408394\n",
      "Guide to 0.077160716\n",
      "Guide the -0.11736892\n",
      "Guide Rockstar 0.40988538\n",
      "Guide User 0.52911276\n",
      "Guide 's 0.0674446\n",
      "Guide Guide 1.0\n",
      "Guide this 0.056999933\n",
      "Guide is -0.06693902\n",
      "Guide doable 0.16209757\n",
      "Guide but 0.13041298\n",
      "Guide I 0.022819573\n",
      "Guide could -0.058505647\n",
      "Guide n't 0.07411657\n",
      "Guide figure 0.14172979\n",
      "Guide out 0.057739817\n",
      "Guide how 0.16925631\n",
      "Guide in -0.01943975\n",
      "Guide yt 0.07207971\n",
      "Guide . 0.032390714\n",
      "Guide Thanks -0.043906678\n",
      "Guide in -0.08372972\n",
      "Guide advance 0.1767973\n",
      "Guide for -0.069428146\n",
      "Guide your -0.111586824\n",
      "Guide support 0.20371753\n",
      "Guide ! -0.027811993\n",
      "this Hi -0.06019851\n",
      "this everyone 0.21346182\n",
      "this , -0.07401814\n",
      "this \n",
      " 0.0042792885\n",
      "this I 0.22198439\n",
      "this 'm 0.04180482\n",
      "this using -0.012093432\n",
      "this yt -0.011204084\n",
      "this to -0.061340466\n",
      "this run -0.006399283\n",
      "this the 0.43557504\n",
      "this Rockstar 0.027006442\n",
      "this Halo 0.039884783\n",
      "this Finder 0.068213925\n",
      "this . 0.060826186\n",
      "this Is -0.26323748\n",
      "this it 0.26529962\n",
      "this somehow 0.031445876\n",
      "this possible 0.0064453944\n",
      "this to -0.10664879\n",
      "this output -0.025200222\n",
      "this the 0.38907346\n",
      "this particle 0.20963502\n",
      "this data 0.005282925\n",
      "this of -0.043391727\n",
      "this each 0.48755175\n",
      "this halo 0.22864294\n",
      "this ? -0.013774075\n",
      "this According -0.05635079\n",
      "this to -0.106798984\n",
      "this the 0.4860723\n",
      "this Rockstar 0.030118093\n",
      "this User 0.02770623\n",
      "this 's 0.14786719\n",
      "this Guide 0.056999933\n",
      "this this 1.0\n",
      "this is -0.17831123\n",
      "this doable 0.15805171\n",
      "this but 0.15897189\n",
      "this I 0.17927797\n",
      "this could -0.004702268\n",
      "this n't 0.124145515\n",
      "this figure 0.11751569\n",
      "this out 0.08266176\n",
      "this how -0.18929848\n",
      "this in -0.0083556445\n",
      "this yt 0.02258721\n",
      "this . 0.020718185\n",
      "this Thanks 0.058729902\n",
      "this in -0.05751761\n",
      "this advance 0.3087538\n",
      "this for -0.11504486\n",
      "this your 0.0049379175\n",
      "this support 0.23311248\n",
      "this ! 0.014591115\n",
      "is Hi 0.22837877\n",
      "is everyone 0.0010299301\n",
      "is , -0.056215506\n",
      "is \n",
      " 0.02414896\n",
      "is I -0.03612251\n",
      "is 'm 0.27715588\n",
      "is using -0.0031973007\n",
      "is yt 0.13508606\n",
      "is to 0.0894818\n",
      "is run -0.009016437\n",
      "is the -0.113762304\n",
      "is Rockstar -0.04870413\n",
      "is Halo 0.06353398\n",
      "is Finder 0.059353195\n",
      "is . 0.0005447097\n",
      "is Is 0.6230915\n",
      "is it 0.051451404\n",
      "is somehow 0.21620373\n",
      "is possible -0.06975916\n",
      "is to 0.026388498\n",
      "is output 0.009678711\n",
      "is the -0.05613849\n",
      "is particle 0.14135975\n",
      "is data 0.15308906\n",
      "is of 0.0020557013\n",
      "is each -0.16573285\n",
      "is halo 0.1637306\n",
      "is ? -0.08026768\n",
      "is According 0.04956142\n",
      "is to 0.00522274\n",
      "is the -0.16347647\n",
      "is Rockstar -0.035133343\n",
      "is User 0.07215813\n",
      "is 's 0.14525472\n",
      "is Guide -0.06693902\n",
      "is this -0.17831123\n",
      "is is 1.0\n",
      "is doable 0.014709245\n",
      "is but 0.2522848\n",
      "is I -0.034737714\n",
      "is could 0.21595338\n",
      "is n't 0.11218124\n",
      "is figure 0.047718685\n",
      "is out 0.06874489\n",
      "is how -0.06076647\n",
      "is in 0.07947309\n",
      "is yt 0.09806183\n",
      "is . -0.107426085\n",
      "is Thanks -0.04377354\n",
      "is in 0.13140014\n",
      "is advance 0.049086448\n",
      "is for 0.14066988\n",
      "is your -0.18294322\n",
      "is support -0.05409505\n",
      "is ! -0.113568485\n",
      "doable Hi 0.12119339\n",
      "doable everyone 0.15151332\n",
      "doable , 0.029413832\n",
      "doable \n",
      " 0.02820831\n",
      "doable I 0.1537336\n",
      "doable 'm 0.08493891\n",
      "doable using 0.19692673\n",
      "doable yt -0.05805588\n",
      "doable to -0.07820919\n",
      "doable run 0.2059651\n",
      "doable the 0.15521285\n",
      "doable Rockstar 0.03101355\n",
      "doable Halo 0.018762004\n",
      "doable Finder -0.043880045\n",
      "doable . 0.011936579\n",
      "doable Is -0.07364798\n",
      "doable it 0.22195399\n",
      "doable somehow 0.14700569\n",
      "doable possible 0.31968272\n",
      "doable to -0.1400799\n",
      "doable output 0.17688337\n",
      "doable the 0.20757215\n",
      "doable particle 0.3778746\n",
      "doable data -0.0823356\n",
      "doable of -0.10231545\n",
      "doable each 0.19608854\n",
      "doable halo 0.1051275\n",
      "doable ? -0.06616074\n",
      "doable According -0.12220848\n",
      "doable to -0.046517767\n",
      "doable the 0.1369192\n",
      "doable Rockstar 0.08361957\n",
      "doable User 0.025064588\n",
      "doable 's -0.02272614\n",
      "doable Guide 0.16209757\n",
      "doable this 0.15805171\n",
      "doable is 0.014709245\n",
      "doable doable 1.0\n",
      "doable but 0.14486629\n",
      "doable I 0.16083638\n",
      "doable could 0.05598931\n",
      "doable n't -0.010948976\n",
      "doable figure 0.14387211\n",
      "doable out -0.05487015\n",
      "doable how -0.16333589\n",
      "doable in 0.16107437\n",
      "doable yt 0.032410678\n",
      "doable . 0.016346287\n",
      "doable Thanks 0.06902857\n",
      "doable in -0.03324044\n",
      "doable advance 0.15205419\n",
      "doable for -0.046269666\n",
      "doable your 0.06848503\n",
      "doable support 0.16492268\n",
      "doable ! -0.045445245\n",
      "but Hi 0.24568978\n",
      "but everyone -0.09335777\n",
      "but , 0.025278673\n",
      "but \n",
      " 0.3323853\n",
      "but I 0.13575214\n",
      "but 'm 0.12491707\n",
      "but using -0.013347847\n",
      "but yt 0.094009854\n",
      "but to 0.040619835\n",
      "but run -0.014491892\n",
      "but the 0.016530478\n",
      "but Rockstar -0.13173614\n",
      "but Halo -0.0065571344\n",
      "but Finder -0.13810109\n",
      "but . 0.09880979\n",
      "but Is 0.189091\n",
      "but it -0.019734042\n",
      "but somehow 0.04602209\n",
      "but possible -0.09523223\n",
      "but to 0.0049328026\n",
      "but output -0.1783708\n",
      "but the -0.015200244\n",
      "but particle 0.060628258\n",
      "but data 0.08977919\n",
      "but of 0.2423468\n",
      "but each 0.21544038\n",
      "but halo 0.029044695\n",
      "but ? 0.10009476\n",
      "but According -0.10962342\n",
      "but to 0.090312056\n",
      "but the 0.10358103\n",
      "but Rockstar -0.081966884\n",
      "but User -0.07607803\n",
      "but 's -0.0781769\n",
      "but Guide 0.13041298\n",
      "but this 0.15897189\n",
      "but is 0.2522848\n",
      "but doable 0.14486629\n",
      "but but 1.0\n",
      "but I 0.19878313\n",
      "but could -0.0014306047\n",
      "but n't 0.12807544\n",
      "but figure -0.13988106\n",
      "but out -0.003418538\n",
      "but how 0.14919634\n",
      "but in 0.13492239\n",
      "but yt 0.2022844\n",
      "but . 0.21235749\n",
      "but Thanks 0.10655788\n",
      "but in 0.11161043\n",
      "but advance 0.03312994\n",
      "but for 0.2743752\n",
      "but your -0.09198726\n",
      "but support 0.05156164\n",
      "but ! 0.21664123\n",
      "I Hi 0.15807469\n",
      "I everyone 0.18859659\n",
      "I , -0.14130746\n",
      "I \n",
      " -0.01225748\n",
      "I I 1.0\n",
      "I 'm 0.08097468\n",
      "I using 0.050075628\n",
      "I yt 0.2509748\n",
      "I to -0.037966672\n",
      "I run 0.033109326\n",
      "I the -0.064828426\n",
      "I Rockstar -0.050623063\n",
      "I Halo -0.09732211\n",
      "I Finder -0.018801523\n",
      "I . -0.14595714\n",
      "I Is 0.16343494\n",
      "I it 0.43858194\n",
      "I somehow -0.13066109\n",
      "I possible 0.0803756\n",
      "I to -0.10245125\n",
      "I output 0.032755867\n",
      "I the 0.015701955\n",
      "I particle 0.10760016\n",
      "I data 0.068021335\n",
      "I of 0.08422053\n",
      "I each 0.08946361\n",
      "I halo 0.058038555\n",
      "I ? -0.07543726\n",
      "I According -0.09799604\n",
      "I to 0.06239065\n",
      "I the -0.01562734\n",
      "I Rockstar -0.10486702\n",
      "I User 0.05767167\n",
      "I 's -0.028433004\n",
      "I Guide 0.022819573\n",
      "I this 0.17927797\n",
      "I is -0.034737714\n",
      "I doable 0.16083638\n",
      "I but 0.19878313\n",
      "I I 1.0\n",
      "I could 0.016656982\n",
      "I n't 0.011283242\n",
      "I figure 0.007708784\n",
      "I out -0.1105527\n",
      "I how -0.11370618\n",
      "I in 0.10909369\n",
      "I yt 0.26254675\n",
      "I . 0.020312682\n",
      "I Thanks 0.12431484\n",
      "I in -0.012066359\n",
      "I advance 0.13498051\n",
      "I for 0.014130833\n",
      "I your 0.2541185\n",
      "I support 0.05552653\n",
      "I ! -0.017963966\n",
      "could Hi 0.31173414\n",
      "could everyone 0.1972463\n",
      "could , -0.01897995\n",
      "could \n",
      " 0.03915839\n",
      "could I 0.014841805\n",
      "could 'm 0.49244896\n",
      "could using -0.052855514\n",
      "could yt -0.15401481\n",
      "could to 0.24823198\n",
      "could run 0.13086513\n",
      "could the -0.1332097\n",
      "could Rockstar -0.031853277\n",
      "could Halo -0.0091885235\n",
      "could Finder -0.065800525\n",
      "could . -0.024150712\n",
      "could Is 0.108498834\n",
      "could it -0.032118734\n",
      "could somehow 0.14211333\n",
      "could possible 0.03850741\n",
      "could to 0.23174599\n",
      "could output 0.084520735\n",
      "could the -0.13886581\n",
      "could particle -0.058279265\n",
      "could data -0.05153165\n",
      "could of -0.03962203\n",
      "could each -0.07320853\n",
      "could halo -0.045268435\n",
      "could ? -0.01209153\n",
      "could According -0.023372576\n",
      "could to -0.05168084\n",
      "could the -0.15729651\n",
      "could Rockstar -0.09588411\n",
      "could User 0.030393645\n",
      "could 's -0.088489965\n",
      "could Guide -0.058505647\n",
      "could this -0.004702268\n",
      "could is 0.21595338\n",
      "could doable 0.05598931\n",
      "could but -0.0014306047\n",
      "could I 0.016656982\n",
      "could could 1.0\n",
      "could n't 0.33226386\n",
      "could figure 0.1837211\n",
      "could out -0.032290623\n",
      "could how -0.16028175\n",
      "could in 0.016450752\n",
      "could yt -0.25208762\n",
      "could . -0.020740194\n",
      "could Thanks -0.06390103\n",
      "could in 0.058599096\n",
      "could advance -0.009130621\n",
      "could for -0.026138535\n",
      "could your -0.07237443\n",
      "could support 0.051093653\n",
      "could ! -0.047211003\n",
      "n't Hi 0.2279573\n",
      "n't everyone 0.24737084\n",
      "n't , -0.06283637\n",
      "n't \n",
      " -0.12895338\n",
      "n't I 0.10454577\n",
      "n't 'm 0.12802567\n",
      "n't using 0.0137119545\n",
      "n't yt 0.04038918\n",
      "n't to 0.31313977\n",
      "n't run 0.07613054\n",
      "n't the -0.040108774\n",
      "n't Rockstar -0.021795118\n",
      "n't Halo -0.01836821\n",
      "n't Finder -0.08597771\n",
      "n't . -0.15662464\n",
      "n't Is 0.09943246\n",
      "n't it 0.24182206\n",
      "n't somehow 0.3515904\n",
      "n't possible 0.06361824\n",
      "n't to 0.12790039\n",
      "n't output 0.18219295\n",
      "n't the 0.04735156\n",
      "n't particle -0.07677289\n",
      "n't data 0.0078020506\n",
      "n't of -0.010889377\n",
      "n't each 0.097630255\n",
      "n't halo -0.11455005\n",
      "n't ? -0.080719635\n",
      "n't According 0.04671822\n",
      "n't to -0.04769439\n",
      "n't the -0.024812775\n",
      "n't Rockstar -0.10571424\n",
      "n't User 0.041965764\n",
      "n't 's 0.028292548\n",
      "n't Guide 0.07411657\n",
      "n't this 0.124145515\n",
      "n't is 0.11218124\n",
      "n't doable -0.010948976\n",
      "n't but 0.12807544\n",
      "n't I 0.011283242\n",
      "n't could 0.33226386\n",
      "n't n't 1.0\n",
      "n't figure 0.13551798\n",
      "n't out 0.13967286\n",
      "n't how 0.18570586\n",
      "n't in 0.030750606\n",
      "n't yt 0.048937816\n",
      "n't . -0.1354626\n",
      "n't Thanks -0.008556107\n",
      "n't in -0.016496869\n",
      "n't advance 0.066141725\n",
      "n't for -0.122664355\n",
      "n't your -0.11517965\n",
      "n't support -0.001973767\n",
      "n't ! -0.11054668\n",
      "figure Hi 0.08111673\n",
      "figure everyone 0.34122095\n",
      "figure , 0.15372881\n",
      "figure \n",
      " 0.030054122\n",
      "figure I 0.02072231\n",
      "figure 'm 0.27017048\n",
      "figure using 0.114905864\n",
      "figure yt -0.049000084\n",
      "figure to 0.13570216\n",
      "figure run 0.5939319\n",
      "figure the 0.13926038\n",
      "figure Rockstar 0.08080608\n",
      "figure Halo -0.15575029\n",
      "figure Finder 0.106061764\n",
      "figure . 0.12537894\n",
      "figure Is 0.007106225\n",
      "figure it 0.08864164\n",
      "figure somehow 0.018862886\n",
      "figure possible 0.0033471412\n",
      "figure to 0.007557381\n",
      "figure output 0.66999924\n",
      "figure the 0.12623407\n",
      "figure particle 0.35977444\n",
      "figure data 0.014231263\n",
      "figure of -0.10056594\n",
      "figure each 0.06659406\n",
      "figure halo 0.08918373\n",
      "figure ? 0.04011371\n",
      "figure According 0.047828242\n",
      "figure to -0.08059852\n",
      "figure the 0.13485587\n",
      "figure Rockstar 0.018192664\n",
      "figure User 0.11538557\n",
      "figure 's 0.08651839\n",
      "figure Guide 0.14172979\n",
      "figure this 0.11751569\n",
      "figure is 0.047718685\n",
      "figure doable 0.14387211\n",
      "figure but -0.13988106\n",
      "figure I 0.007708784\n",
      "figure could 0.1837211\n",
      "figure n't 0.13551798\n",
      "figure figure 1.0\n",
      "figure out 0.12657367\n",
      "figure how -0.10121765\n",
      "figure in -0.18404065\n",
      "figure yt -0.04445739\n",
      "figure . 0.12504862\n",
      "figure Thanks -0.22006057\n",
      "figure in -0.14620128\n",
      "figure advance 0.15782201\n",
      "figure for -0.06280303\n",
      "figure your 0.006092437\n",
      "figure support 0.31053463\n",
      "figure ! 0.10298411\n",
      "out Hi -0.04382779\n",
      "out everyone 0.15227097\n",
      "out , 0.07923468\n",
      "out \n",
      " -0.04295294\n",
      "out I -0.09812246\n",
      "out 'm -0.033314813\n",
      "out using -0.04352716\n",
      "out yt 0.014597924\n",
      "out to 0.023397285\n",
      "out run 0.12865019\n",
      "out the 0.19933185\n",
      "out Rockstar 0.041016474\n",
      "out Halo -0.05605699\n",
      "out Finder -0.037703842\n",
      "out . 0.19181606\n",
      "out Is -0.016379839\n",
      "out it 0.008641338\n",
      "out somehow 0.19122943\n",
      "out possible 0.13867223\n",
      "out to -0.065104894\n",
      "out output 0.118245475\n",
      "out the 0.15013298\n",
      "out particle -0.03255387\n",
      "out data 0.05411644\n",
      "out of 0.016786195\n",
      "out each 0.0028671413\n",
      "out halo 0.18440455\n",
      "out ? 0.18727036\n",
      "out According -0.037203893\n",
      "out to 0.07681805\n",
      "out the 0.19616686\n",
      "out Rockstar -0.09190575\n",
      "out User -0.030613236\n",
      "out 's 0.06331945\n",
      "out Guide 0.057739817\n",
      "out this 0.08266176\n",
      "out is 0.06874489\n",
      "out doable -0.05487015\n",
      "out but -0.003418538\n",
      "out I -0.1105527\n",
      "out could -0.032290623\n",
      "out n't 0.13967286\n",
      "out figure 0.12657367\n",
      "out out 1.0\n",
      "out how 0.1850544\n",
      "out in 0.2833507\n",
      "out yt 0.03990352\n",
      "out . 0.031433847\n",
      "out Thanks -0.16566424\n",
      "out in 0.07673267\n",
      "out advance 0.11897557\n",
      "out for 0.14544906\n",
      "out your -0.10904693\n",
      "out support 0.11434392\n",
      "out ! 0.15129274\n",
      "how Hi 0.07235979\n",
      "how everyone -0.039744757\n",
      "how , 0.08333305\n",
      "how \n",
      " 0.10373024\n",
      "how I -0.018570364\n",
      "how 'm -0.119237214\n",
      "how using -0.03554337\n",
      "how yt 0.14050995\n",
      "how to 0.010346979\n",
      "how run -0.05930145\n",
      "how the -0.15515706\n",
      "how Rockstar 0.007883923\n",
      "how Halo 0.05908702\n",
      "how Finder -0.0815637\n",
      "how . 0.031469353\n",
      "how Is 0.043337617\n",
      "how it -0.19737291\n",
      "how somehow 0.21505533\n",
      "how possible 0.023736954\n",
      "how to 0.036792025\n",
      "how output -0.055936303\n",
      "how the -0.034289464\n",
      "how particle -0.09921537\n",
      "how data 0.09073041\n",
      "how of 0.09295072\n",
      "how each -0.04407756\n",
      "how halo -0.006342878\n",
      "how ? 0.025893416\n",
      "how According 0.078383565\n",
      "how to 0.08205119\n",
      "how the -0.1682779\n",
      "how Rockstar 0.011635369\n",
      "how User 0.044041403\n",
      "how 's -0.1167579\n",
      "how Guide 0.16925631\n",
      "how this -0.18929848\n",
      "how is -0.06076647\n",
      "how doable -0.16333589\n",
      "how but 0.14919634\n",
      "how I -0.11370618\n",
      "how could -0.16028175\n",
      "how n't 0.18570586\n",
      "how figure -0.10121765\n",
      "how out 0.1850544\n",
      "how how 1.0\n",
      "how in 0.25410253\n",
      "how yt 0.1887118\n",
      "how . 0.0033302945\n",
      "how Thanks -0.10440248\n",
      "how in 0.11276923\n",
      "how advance 0.06683761\n",
      "how for 0.036393747\n",
      "how your -0.05517647\n",
      "how support -0.0107738655\n",
      "how ! 0.03520113\n",
      "in Hi 0.19859283\n",
      "in everyone 0.10796753\n",
      "in , -0.09484523\n",
      "in \n",
      " -0.067018956\n",
      "in I 0.13580233\n",
      "in 'm -0.08384223\n",
      "in using 0.12536435\n",
      "in yt 0.21289557\n",
      "in to 0.019183608\n",
      "in run -0.07932019\n",
      "in the 0.060739093\n",
      "in Rockstar 0.05440898\n",
      "in Halo 0.1612368\n",
      "in Finder -0.00360172\n",
      "in . -0.045441855\n",
      "in Is 0.15821132\n",
      "in it -0.03427149\n",
      "in somehow 0.2876073\n",
      "in possible 0.22202983\n",
      "in to -0.029869908\n",
      "in output -0.22124948\n",
      "in the 0.058818918\n",
      "in particle 0.049312424\n",
      "in data 0.015092228\n",
      "in of 0.29202363\n",
      "in each -0.009775211\n",
      "in halo 0.06399074\n",
      "in ? -0.036419544\n",
      "in According 0.10840529\n",
      "in to 0.37831128\n",
      "in the 0.016842\n",
      "in Rockstar -0.009182566\n",
      "in User 0.11047234\n",
      "in 's 0.02370227\n",
      "in Guide -0.01943975\n",
      "in this -0.0083556445\n",
      "in is 0.07947309\n",
      "in doable 0.16107437\n",
      "in but 0.13492239\n",
      "in I 0.10909369\n",
      "in could 0.016450752\n",
      "in n't 0.030750606\n",
      "in figure -0.18404065\n",
      "in out 0.2833507\n",
      "in how 0.25410253\n",
      "in in 1.0\n",
      "in yt 0.16330978\n",
      "in . -0.13377729\n",
      "in Thanks 0.03403923\n",
      "in in 1.0\n",
      "in advance 0.07873078\n",
      "in for 0.38647977\n",
      "in your 0.0031912213\n",
      "in support 0.0648211\n",
      "in ! -0.1351173\n",
      "yt Hi 0.08762924\n",
      "yt everyone 0.077059984\n",
      "yt , -0.101757556\n",
      "yt \n",
      " 0.013197038\n",
      "yt I 0.19352359\n",
      "yt 'm -0.014558908\n",
      "yt using -0.054004114\n",
      "yt yt 1.0\n",
      "yt to 0.16104509\n",
      "yt run -0.06519329\n",
      "yt the 0.03404641\n",
      "yt Rockstar 0.06922394\n",
      "yt Halo 0.0092675835\n",
      "yt Finder 0.15982367\n",
      "yt . 0.052151427\n",
      "yt Is 0.12896928\n",
      "yt it 0.25627556\n",
      "yt somehow -0.05675933\n",
      "yt possible 0.010213123\n",
      "yt to 0.031461198\n",
      "yt output -0.086070985\n",
      "yt the 0.10962311\n",
      "yt particle 0.08280672\n",
      "yt data 0.23831664\n",
      "yt of 0.044282183\n",
      "yt each 0.1071525\n",
      "yt halo 0.09594962\n",
      "yt ? -0.089471914\n",
      "yt According -0.16862166\n",
      "yt to 0.13520823\n",
      "yt the 0.063884586\n",
      "yt Rockstar 0.072417185\n",
      "yt User 0.08121725\n",
      "yt 's 0.08759573\n",
      "yt Guide 0.07207971\n",
      "yt this 0.02258721\n",
      "yt is 0.09806183\n",
      "yt doable 0.032410678\n",
      "yt but 0.2022844\n",
      "yt I 0.26254675\n",
      "yt could -0.25208762\n",
      "yt n't 0.048937816\n",
      "yt figure -0.04445739\n",
      "yt out 0.03990352\n",
      "yt how 0.1887118\n",
      "yt in 0.16330978\n",
      "yt yt 1.0\n",
      "yt . 0.18845989\n",
      "yt Thanks 0.26905975\n",
      "yt in 0.02512731\n",
      "yt advance 0.073630996\n",
      "yt for -0.042692423\n",
      "yt your 0.30844083\n",
      "yt support 0.24313483\n",
      "yt ! -0.044309128\n",
      ". Hi -0.1341996\n",
      ". everyone -0.085296266\n",
      ". , 0.38273433\n",
      ". \n",
      " 0.44662184\n",
      ". I -0.063700095\n",
      ". 'm -0.055872824\n",
      ". using -0.06931585\n",
      ". yt -0.011428421\n",
      ". to -0.031584878\n",
      ". run 0.18799222\n",
      ". the -0.084831856\n",
      ". Rockstar -0.004425938\n",
      ". Halo -0.105963066\n",
      ". Finder -0.07131491\n",
      ". . 1.0\n",
      ". Is -0.18026812\n",
      ". it -0.10971745\n",
      ". somehow -0.19022655\n",
      ". possible -0.15436675\n",
      ". to 0.023298187\n",
      ". output 0.1270131\n",
      ". the -0.10659661\n",
      ". particle 0.08287049\n",
      ". data 0.17365837\n",
      ". of 0.0067066923\n",
      ". each 0.047667842\n",
      ". halo 0.08299859\n",
      ". ? 0.5391788\n",
      ". According -0.107093304\n",
      ". to -0.1107612\n",
      ". the 0.011464027\n",
      ". Rockstar -0.041844286\n",
      ". User 0.03750283\n",
      ". 's -0.1990582\n",
      ". Guide 0.032390714\n",
      ". this 0.020718185\n",
      ". is -0.107426085\n",
      ". doable 0.016346287\n",
      ". but 0.21235749\n",
      ". I 0.020312682\n",
      ". could -0.020740194\n",
      ". n't -0.1354626\n",
      ". figure 0.12504862\n",
      ". out 0.031433847\n",
      ". how 0.0033302945\n",
      ". in -0.13377729\n",
      ". yt 0.18845989\n",
      ". . 1.0\n",
      ". Thanks 0.18162626\n",
      ". in -0.08017768\n",
      ". advance -0.077221945\n",
      ". for 0.03339954\n",
      ". your 0.043380294\n",
      ". support 0.13855618\n",
      ". ! 0.5843759\n",
      "Thanks Hi 0.26144847\n",
      "Thanks everyone -0.08648325\n",
      "Thanks , -0.075700134\n",
      "Thanks \n",
      " 0.0853085\n",
      "Thanks I 0.08044374\n",
      "Thanks 'm -0.02910461\n",
      "Thanks using 0.03269754\n",
      "Thanks yt 0.29598102\n",
      "Thanks to -0.024553124\n",
      "Thanks run -0.08284697\n",
      "Thanks the -0.015187249\n",
      "Thanks Rockstar -0.13288647\n",
      "Thanks Halo 0.012826049\n",
      "Thanks Finder 0.044440173\n",
      "Thanks . -0.22257498\n",
      "Thanks Is 0.08921477\n",
      "Thanks it 0.028499262\n",
      "Thanks somehow 0.0024256539\n",
      "Thanks possible -0.04071031\n",
      "Thanks to -0.0710926\n",
      "Thanks output -0.18077327\n",
      "Thanks the -0.19854034\n",
      "Thanks particle -0.09756437\n",
      "Thanks data 0.24375492\n",
      "Thanks of 0.114243366\n",
      "Thanks each 0.01821319\n",
      "Thanks halo 0.10038113\n",
      "Thanks ? -0.084023245\n",
      "Thanks According 0.14086334\n",
      "Thanks to 0.064324014\n",
      "Thanks the 0.00058312924\n",
      "Thanks Rockstar -0.1399497\n",
      "Thanks User -0.08127514\n",
      "Thanks 's -0.15886176\n",
      "Thanks Guide -0.043906678\n",
      "Thanks this 0.058729902\n",
      "Thanks is -0.04377354\n",
      "Thanks doable 0.06902857\n",
      "Thanks but 0.10655788\n",
      "Thanks I 0.12431484\n",
      "Thanks could -0.06390103\n",
      "Thanks n't -0.008556107\n",
      "Thanks figure -0.22006057\n",
      "Thanks out -0.16566424\n",
      "Thanks how -0.10440248\n",
      "Thanks in 0.03403923\n",
      "Thanks yt 0.26905975\n",
      "Thanks . 0.18162626\n",
      "Thanks Thanks 1.0\n",
      "Thanks in 0.03878822\n",
      "Thanks advance 0.025007097\n",
      "Thanks for 0.083436444\n",
      "Thanks your 0.002836325\n",
      "Thanks support 0.025681412\n",
      "Thanks ! -0.13745\n",
      "in Hi 0.18180202\n",
      "in everyone 0.13392821\n",
      "in , 0.03116122\n",
      "in \n",
      " 0.023947151\n",
      "in I -0.048901986\n",
      "in 'm 0.09071505\n",
      "in using 0.08277201\n",
      "in yt 0.14802985\n",
      "in to 0.068339944\n",
      "in run -0.053743813\n",
      "in the 0.041372847\n",
      "in Rockstar -0.18187015\n",
      "in Halo -0.06609577\n",
      "in Finder -0.08068714\n",
      "in . -0.1555129\n",
      "in Is 0.20326507\n",
      "in it -0.072115175\n",
      "in somehow 0.084836185\n",
      "in possible 0.17624831\n",
      "in to 0.20484072\n",
      "in output -0.17168172\n",
      "in the -0.07129996\n",
      "in particle -0.14005204\n",
      "in data 0.018843018\n",
      "in of 0.4775281\n",
      "in each 0.015759995\n",
      "in halo -0.076900445\n",
      "in ? -0.009319135\n",
      "in According 0.2113472\n",
      "in to 0.48802248\n",
      "in the 0.021637794\n",
      "in Rockstar -0.17127536\n",
      "in User -0.021307776\n",
      "in 's 0.05538434\n",
      "in Guide -0.08372972\n",
      "in this -0.05751761\n",
      "in is 0.13140014\n",
      "in doable -0.03324044\n",
      "in but 0.11161043\n",
      "in I -0.012066359\n",
      "in could 0.058599096\n",
      "in n't -0.016496869\n",
      "in figure -0.14620128\n",
      "in out 0.07673267\n",
      "in how 0.11276923\n",
      "in in 1.0\n",
      "in yt 0.02512731\n",
      "in . -0.08017768\n",
      "in Thanks 0.03878822\n",
      "in in 1.0\n",
      "in advance 0.07728125\n",
      "in for 0.5504899\n",
      "in your -0.080004714\n",
      "in support -0.22865832\n",
      "in ! -0.020455185\n",
      "advance Hi 0.06684664\n",
      "advance everyone 0.4335847\n",
      "advance , 0.044000342\n",
      "advance \n",
      " -0.069637604\n",
      "advance I 0.18924372\n",
      "advance 'm 0.06427351\n",
      "advance using 0.30079395\n",
      "advance yt 0.07154192\n",
      "advance to -0.031013971\n",
      "advance run 0.028526168\n",
      "advance the 0.2162737\n",
      "advance Rockstar 0.13529246\n",
      "advance Halo -0.04201268\n",
      "advance Finder 0.08087063\n",
      "advance . 0.045980137\n",
      "advance Is 0.044996463\n",
      "advance it 0.12488499\n",
      "advance somehow 0.089878656\n",
      "advance possible 0.04229015\n",
      "advance to -0.07316365\n",
      "advance output 0.21518347\n",
      "advance the 0.123630516\n",
      "advance particle 0.3468109\n",
      "advance data 0.19663858\n",
      "advance of 0.011162984\n",
      "advance each 0.18523087\n",
      "advance halo 0.24771968\n",
      "advance ? -0.037886236\n",
      "advance According 0.23463167\n",
      "advance to 0.032895043\n",
      "advance the 0.18232709\n",
      "advance Rockstar 0.12851897\n",
      "advance User 0.08522288\n",
      "advance 's 0.15334179\n",
      "advance Guide 0.1767973\n",
      "advance this 0.3087538\n",
      "advance is 0.049086448\n",
      "advance doable 0.15205419\n",
      "advance but 0.03312994\n",
      "advance I 0.13498051\n",
      "advance could -0.009130621\n",
      "advance n't 0.066141725\n",
      "advance figure 0.15782201\n",
      "advance out 0.11897557\n",
      "advance how 0.06683761\n",
      "advance in 0.07873078\n",
      "advance yt 0.073630996\n",
      "advance . -0.077221945\n",
      "advance Thanks 0.025007097\n",
      "advance in 0.07728125\n",
      "advance advance 1.0\n",
      "advance for 0.03171626\n",
      "advance your 0.06967287\n",
      "advance support 0.37604335\n",
      "advance ! -0.01709508\n",
      "for Hi 0.08780641\n",
      "for everyone 0.057326086\n",
      "for , 0.18593101\n",
      "for \n",
      " 0.08991105\n",
      "for I -0.09198397\n",
      "for 'm 0.048743\n",
      "for using 0.17146626\n",
      "for yt 0.091223866\n",
      "for to 0.17393795\n",
      "for run 0.033065133\n",
      "for the 0.012480866\n",
      "for Rockstar -0.15041766\n",
      "for Halo 0.034106977\n",
      "for Finder -0.19187692\n",
      "for . 0.017448498\n",
      "for Is 0.29261988\n",
      "for it -0.12460923\n",
      "for somehow 0.030575382\n",
      "for possible 0.16528064\n",
      "for to 0.20874995\n",
      "for output 0.012280922\n",
      "for the -0.064804554\n",
      "for particle -0.0357185\n",
      "for data -0.009910587\n",
      "for of 0.4857952\n",
      "for each 0.013259723\n",
      "for halo 0.06577955\n",
      "for ? 0.2557233\n",
      "for According 0.3590549\n",
      "for to 0.56109077\n",
      "for the 0.07752022\n",
      "for Rockstar -0.21349734\n",
      "for User -0.07989366\n",
      "for 's 0.01078029\n",
      "for Guide -0.069428146\n",
      "for this -0.11504486\n",
      "for is 0.14066988\n",
      "for doable -0.046269666\n",
      "for but 0.2743752\n",
      "for I 0.014130833\n",
      "for could -0.026138535\n",
      "for n't -0.122664355\n",
      "for figure -0.06280303\n",
      "for out 0.14544906\n",
      "for how 0.036393747\n",
      "for in 0.38647977\n",
      "for yt -0.042692423\n",
      "for . 0.03339954\n",
      "for Thanks 0.083436444\n",
      "for in 0.5504899\n",
      "for advance 0.03171626\n",
      "for for 1.0\n",
      "for your -0.038792413\n",
      "for support -0.21839474\n",
      "for ! 0.18629718\n",
      "your Hi -0.030821275\n",
      "your everyone 0.014232022\n",
      "your , -0.20122357\n",
      "your \n",
      " -0.13890532\n",
      "your I 0.1405704\n",
      "your 'm -0.03861253\n",
      "your using 0.16144356\n",
      "your yt 0.20610662\n",
      "your to 0.002571621\n",
      "your run -0.030290782\n",
      "your the 0.12355241\n",
      "your Rockstar 0.22025731\n",
      "your Halo -0.040498257\n",
      "your Finder -0.048082516\n",
      "your . 0.007289701\n",
      "your Is -0.050694443\n",
      "your it 0.12894712\n",
      "your somehow -0.038063455\n",
      "your possible 0.09800035\n",
      "your to 0.14381692\n",
      "your output 0.10969101\n",
      "your the 0.26874754\n",
      "your particle 0.13017987\n",
      "your data 0.11100432\n",
      "your of 0.042999435\n",
      "your each 0.14118399\n",
      "your halo 0.005054288\n",
      "your ? 0.05054802\n",
      "your According 0.08722653\n",
      "your to 0.14660972\n",
      "your the 0.2595377\n",
      "your Rockstar 0.18675376\n",
      "your User -0.006296432\n",
      "your 's 0.045117952\n",
      "your Guide -0.111586824\n",
      "your this 0.0049379175\n",
      "your is -0.18294322\n",
      "your doable 0.06848503\n",
      "your but -0.09198726\n",
      "your I 0.2541185\n",
      "your could -0.07237443\n",
      "your n't -0.11517965\n",
      "your figure 0.006092437\n",
      "your out -0.10904693\n",
      "your how -0.05517647\n",
      "your in 0.0031912213\n",
      "your yt 0.30844083\n",
      "your . 0.043380294\n",
      "your Thanks 0.002836325\n",
      "your in -0.080004714\n",
      "your advance 0.06967287\n",
      "your for -0.038792413\n",
      "your your 1.0\n",
      "your support -0.042934384\n",
      "your ! 0.008716882\n",
      "support Hi -0.20409453\n",
      "support everyone 0.3083148\n",
      "support , 0.10340355\n",
      "support \n",
      " -0.09543171\n",
      "support I 0.09128368\n",
      "support 'm 0.05293569\n",
      "support using 0.0364688\n",
      "support yt -0.0071781618\n",
      "support to 0.062396117\n",
      "support run 0.1721825\n",
      "support the 0.22322023\n",
      "support Rockstar 0.27753422\n",
      "support Halo 0.13409917\n",
      "support Finder 0.34895846\n",
      "support . 0.115888655\n",
      "support Is -0.23585175\n",
      "support it 0.019268168\n",
      "support somehow -0.057133935\n",
      "support possible -0.024780866\n",
      "support to -0.112805866\n",
      "support output 0.23892343\n",
      "support the 0.15924889\n",
      "support particle 0.4836165\n",
      "support data 0.32190624\n",
      "support of -0.06815974\n",
      "support each 0.20187037\n",
      "support halo 0.33053726\n",
      "support ? -0.04594513\n",
      "support According -0.12938105\n",
      "support to -0.07494461\n",
      "support the 0.17213617\n",
      "support Rockstar 0.23606114\n",
      "support User 0.20489958\n",
      "support 's 0.040737998\n",
      "support Guide 0.20371753\n",
      "support this 0.23311248\n",
      "support is -0.05409505\n",
      "support doable 0.16492268\n",
      "support but 0.05156164\n",
      "support I 0.05552653\n",
      "support could 0.051093653\n",
      "support n't -0.001973767\n",
      "support figure 0.31053463\n",
      "support out 0.11434392\n",
      "support how -0.0107738655\n",
      "support in 0.0648211\n",
      "support yt 0.24313483\n",
      "support . 0.13855618\n",
      "support Thanks 0.025681412\n",
      "support in -0.22865832\n",
      "support advance 0.37604335\n",
      "support for -0.21839474\n",
      "support your -0.042934384\n",
      "support support 1.0\n",
      "support ! 0.097059265\n",
      "! Hi -0.25907907\n",
      "! everyone -0.074575186\n",
      "! , 0.4811996\n",
      "! \n",
      " 0.31698865\n",
      "! I -0.066929094\n",
      "! 'm -0.010697375\n",
      "! using 0.038336366\n",
      "! yt -0.19187434\n",
      "! to -0.055484314\n",
      "! run 0.12950997\n",
      "! the -0.05786373\n",
      "! Rockstar 0.07086655\n",
      "! Halo -0.18942472\n",
      "! Finder -0.11136301\n",
      "! . 0.54608315\n",
      "! Is -0.14175566\n",
      "! it -0.11508507\n",
      "! somehow -0.026333857\n",
      "! possible -0.022037962\n",
      "! to 0.058788188\n",
      "! output 0.14299308\n",
      "! the -0.0561242\n",
      "! particle 0.03790695\n",
      "! data 0.13984138\n",
      "! of 0.056811783\n",
      "! each 0.025847495\n",
      "! halo 0.03781329\n",
      "! ? 0.6238686\n",
      "! According 0.017523246\n",
      "! to -0.0359422\n",
      "! the 0.04621339\n",
      "! Rockstar -0.018537143\n",
      "! User -0.106060244\n",
      "! 's 0.04484212\n",
      "! Guide -0.027811993\n",
      "! this 0.014591115\n",
      "! is -0.113568485\n",
      "! doable -0.045445245\n",
      "! but 0.21664123\n",
      "! I -0.017963966\n",
      "! could -0.047211003\n",
      "! n't -0.11054668\n",
      "! figure 0.10298411\n",
      "! out 0.15129274\n",
      "! how 0.03520113\n",
      "! in -0.1351173\n",
      "! yt -0.044309128\n",
      "! . 0.5843759\n",
      "! Thanks -0.13745\n",
      "! in -0.020455185\n",
      "! advance -0.01709508\n",
      "! for 0.18629718\n",
      "! your 0.008716882\n",
      "! support 0.097059265\n",
      "! ! 1.0\n"
     ]
    }
   ],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
